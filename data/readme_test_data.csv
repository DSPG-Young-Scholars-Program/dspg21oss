,slug,readme_text,batch,as_of,status
0,Vinetos/emacs,,batch1,16:59:00,Done
1,joshblack/carbon-experimental,"carbon-experimental

All work shown here is experimental, unstable, and not intended to be used.

",batch2,8:09:16,Done
2,ledermann/docker-rails,"Docker-Rails


Simple Rails 6.1 application to demonstrate using Docker for production deployment. The application is a very simple kind of CMS (content management system) allowing to manage posts. Beside the boring CRUD functionality it has some non-default features.
This project aims to build a lean Docker image for use in production. Therefore it's based on the official Alpine Ruby image, uses multi-stage building and some optimizations that I described in my blog. This results in an image size of ~80MB.
Features

Auto refresh via ActionCable: If a displayed post gets changed by another user/instance, it refreshes automatically using the publish/subscribe pattern
Full text search via Elasticsearch and the Searchkick gem to find post content (with suggestions)
Autocompletion with autocompleter
Editing HTML content with the WYSIWYG JavaScript editor Trix
Uploading images directly to S3 with the Shrine gem and jQuery-File-Upload
Background jobs with ActiveJob and the Sidekiq gem (to handle full text indexing, image processing and ActionCable broadcasting)
Cron scheduling with Sidekiq-Cron to handle daily data updates from Wikipedia
Permalinks using the FriendlyId gem
Infinitive scrolling (using the Kaminari gem and some JavaScript)
User authentication with the Clearance gem
Sending HTML e-mails with Premailer and the Really Simple Responsive HTML Email Template
Admin dashboards with Blazer gem
JavaScript with Stimulus
Bundle JavaScript libraries with Yarn

Why?
This project demonstrates my way of building Rails applications. The techniques used to build the app should not be considered as ""best practice"", maybe there are better ways to build. Any feedback would be appreciated.
Multi container architecture
There is an example docker-compose.production.yml. The whole stack is divided into multiple different containers:

app: Main part. It contains the Rails code to handle web requests (by using the Puma gem). See the Dockerfile for details. The image is based on the Alpine variant of the official Ruby image and uses multi-stage building.
worker: Background processing. It contains the same Rails code, but only runs Sidekiq
db: PostgreSQL database
elasticsearch: Full text search engine
redis: In-memory key/value store (used by Sidekiq, ActionCable and for caching)
backup: Regularly backups the database as a dump via CRON to an Amazon S3 bucket

Check it out!
To start up the application in your local Docker environment:
git clone https://github.com/ledermann/docker-rails.git
cd docker-rails
docker-compose build
docker-compose up
Wait some minutes while the database will be prepared by fetching articles from Wikipedia. Then,
navigate your browser to http://[DOCKER_HOST]:[DOCKER_PORT].
Sign in to the admin account:

Username: admin@example.org
Password: secret

Enjoy!
Tests / CI
On every push, the test suite (including RuboCop checks) is performed via GitHub Actions. If successful, a production image is built and pushed to a private GitLab Docker Registry.
Production deployment
The Docker image build includes precompiled assets only (no node_modules and no sources). The spec folder is removed and the Alpine packages for Node and Yarn are not installed.
The stack is ready to host with traefik or nginx proxy and letsencrypt-nginx-proxy-companion.
Demo
A demo installation is set up on https://docker-rails.georg-ledermann.de.
",batch2,8:09:16,Done
3,jackjack821/pytest,"











The pytest framework makes it easy to write small tests, yet
scales to support complex functional testing for applications and libraries.
An example of a simple test:
# content of test_sample.py
def inc(x):
    return x + 1

def test_answer():
    assert inc(3) == 5
To execute it:
$ pytest
============================= test session starts =============================
collected 1 items

test_sample.py F

================================== FAILURES ===================================
_________________________________ test_answer _________________________________

    def test_answer():
>       assert inc(3) == 5
E       assert 4 == 5
E        +  where 4 = inc(3)

test_sample.py:5: AssertionError
========================== 1 failed in 0.04 seconds ===========================

Due to pytest's detailed assertion introspection, only plain assert statements are used. See getting-started for more examples.

Features

Detailed info on failing assert statements (no need to remember self.assert* names);
Auto-discovery
of test modules and functions;
Modular fixtures for
managing small or parametrized long-lived test resources;
Can run unittest (or trial),
nose test suites out of the box;
Python2.6+, Python3.3+, PyPy-2.3, Jython-2.5 (untested);
Rich plugin architecture, with over 315+ external plugins and thriving community;


Documentation
For full documentation, including installation, tutorials and PDF documents, please see http://docs.pytest.org.

Bugs/Requests
Please use the GitHub issue tracker to submit bugs or request features.

Changelog
Consult the Changelog page for fixes and enhancements of each version.

License
Copyright Holger Krekel and others, 2004-2017.
Distributed under the terms of the MIT license, pytest is free and open source software.
",batch2,8:09:15,Done
4,DamienIrving/ocean-analysis,"README
This repository contains the code I use on a daily basis
for the analysis and visualisation of climate data.
Every time I publish a paper,
I share corresponding log files on Figshare (see my profile page
here)
that show how the code in this repository was used to generate the key results.
The rationale for this approach to reproducible research is outlined
here.
Table of contents



Directory
Contents




data_processing
Scripts used to process/analyse climate data


development
IPython notebooks and other rough notes used in the initial development phase of a new script or workflow


downloads
Scripts and other notes relating to downloading and pre-processing various datasets


modules
Commonly used code that is imported/used by various data processing and visualisation scripts


visualisation
Scripts used to visualise data



Note that the workflows follow the data reference syntax outlined in data_reference_syntax.md.
",batch2,8:09:16,Done
5,qqbis/tutorial,"A/B Street
Ever been stuck in traffic on a bus, wondering why is there legal street parking
instead of a dedicated bus lane? A/B Street is a game exploring how small
changes to a city affect the movement of drivers, cyclists, transit users, and
pedestrians.

Play on
Windows,
Mac,
Linux,
or read all instructions (new releases every Sunday)
build from source (new changes daily)

Show, don't tell
Alpha release trailer
Find a problem:

Make some changes:

Measure the effects:

Documentation

How A/B Street works
Case studies
Technical

Developer guide
Map model
Traffic simulation
Running A/B Street in a new city
UX design


Presentations

April 2020 Rust meetup:
recording,
slides
Feb 2020 traffic sim
Oct 2019 Traffic sim and current challenges
Oct 2019 Map construction


Project

Roadmap
Motivations
History



Roadmap and contributing
See the roadmap for current work, including ways to help. If
you want to bring this to your city or if you're skilled in design, traffic
simulation, data visualization, or civic/government outreach, please contact
Dustin Carlino at dabreegster@gmail.com. Follow
r/abstreet for weekly updates or
@CarlinoDustin for occasional videos of
recent progress.
Project mission
If you fix some traffic problem while playing A/B Street, my ultimate goal is
for your changes to become a real proposal for adjusting Seattle's
infrastructure. A/B Street is of course a game, using a simplified approach to
traffic modeling, so city governments still have to evaluate proposals using
their existing methods. A/B Street is intended as a conversation starter and
tool to communicate ideas with interactive visualizations.
Why not leave city planning to professionals? People are local experts on the
small slice of the city they interact with daily -- the one left turn lane that
always backs up or a certain set of poorly timed walk signals.
Laura Adler
writes:

""Only with simple, accessible simulation programs can citizens become active
generators of their own urban visions, not just passive recipients of options
laid out by government officials.""

Existing urban planning software is either proprietary or hard to use. A/B
Street strives to be highly accessible, by being a fun, engaging game. See
here for more guiding principles.
Credits
Core team:

Dustin Carlino (dabreegster@gmail.com)
Yuwen Li (UX)

Others:

All
contributors
Logo by Ryan Pierson
Graphic design advice from Starcat Games,
Daniel Huffman,
Brian Prince
Lightning-fast pathfinding thanks to
fast_paths by Andreas Barth
(easbar.mail@posteo.net)
Hackathon drop-ins from Democracy Lab events
CUGOS and Julian Michael
have been great sounding boards for ideas since the beginning
In-game character faces adapted from
Anokhee Jandhyala
Pandemic modeling by Orestis Malaspinas (orestis.malaspinas@hesge.ch)
Game design advice from Christopher Klein
Lots of help with rendering SVG and fonts from
RazrFalcon and
nical

Data:

Special thanks to all OpenStreetMap
contributors!
King County GIS
Seattle Open Data
Puget Sound Regional Council

",batch2,8:09:16,Done
6,depexo/AI-voice-assistant,"   
 


Mycroft
Mycroft is a hackable open source voice assistant.
Table of Contents

Table of Contents
Getting Started
Running Mycroft
Using Mycroft

Home Device and Account Manager
Skills


Behind the scenes

Pairing Information
Configuration
Using Mycroft Without Home
API Key Services
Using Mycroft behind a proxy

Using Mycroft behind a proxy without authentication
Using Mycroft behind an authenticated proxy




Getting Involved
Links

Getting Started
First, get the code on your system!  The simplest method is via git (git installation instructions):

cd ~/
git clone https://github.com/MycroftAI/mycroft-core.git
cd mycroft-core
bash dev_setup.sh

This script sets up dependencies and a virtualenv.  If running in an environment besides Ubuntu/Debian, Arch or Fedora you may need to manually install packages as instructed by dev_setup.sh.
NOTE: The default branch for this repository is 'dev', which should be considered a work-in-progress. If you want to clone a more stable version, switch over to the 'master' branch.
Running Mycroft
Mycroft provides start-mycroft.sh to perform common tasks. This script uses a virtualenv created by dev_setup.sh.  Assuming you installed mycroft-core in your home directory run:

cd ~/mycroft-core
./start-mycroft.sh debug

The ""debug"" command will start the background services (microphone listener, skill, messagebus, and audio subsystems) as well as bringing up a text-based Command Line Interface (CLI) you can use to interact with Mycroft and see the contents of the various logs. Alternatively you can run ./start-mycroft.sh all to begin the services without the command line interface.  Later you can bring up the CLI using ./start-mycroft.sh cli.
The background services can be stopped as a group with:

./stop-mycroft.sh

Using Mycroft
Home Device and Account Manager
Mycroft AI, Inc. maintains a device and account management system known as Mycroft Home. Developers may sign up at: https://home.mycroft.ai
By default, mycroft-core  is configured to use Home. By saying ""Hey Mycroft, pair my device"" (or any other request verbal request) you will be informed that your device needs to be paired. Mycroft will speak a 6-digit code which you can enter into the pairing page within the Mycroft Home site.
Once paired, your unit will use Mycroft API keys for services such as Speech-to-Text (STT), weather and various other skills.
Skills
Mycroft is nothing without skills.  There are a handful of default skills that are downloaded automatically to your /opt/mycroft/skills directory, but most need to be installed explicitly.  See the Skill Repo to discover skills made by others.  And please share your own interesting work!
Behind the scenes
Pairing Information
Pairing information generated by registering with Home is stored in:
~/.mycroft/identity/identity2.json <-- DO NOT SHARE THIS WITH OTHERS!
Configuration
Mycroft configuration consists of 4 possible locations:

mycroft-core/mycroft/configuration/mycroft.conf(Defaults)
Mycroft Home (Remote)
/etc/mycroft/mycroft.conf(Machine)
$HOME/.mycroft/mycroft.conf(User)

When the configuration loader starts, it looks in these locations in this order, and loads ALL configurations. Keys that exist in multiple configuration files will be overridden by the last file to contain the value. This process results in a minimal amount being written for a specific device and user, without modifying default distribution files.
Using Mycroft Without Home
If you do not wish to use the Mycroft Home service, before starting Mycroft for the first time, create $HOME/.mycroft/mycroft.conf with the following contents:
{
  ""skills"": {
    ""blacklisted_skills"": [
      ""mycroft-configuration.mycroftai"",
      ""mycroft-pairing.mycroftai""
    ]
  }
}

Mycroft will then be unable to perform speech-to-text conversion, so you'll need to set that up as well, using one of the STT engines Mycroft supports.
You may insert your own API keys into the configuration files listed above in Configuration.  For example, to insert the API key for the Weather skill, create a new JSON key in the configuration file like so:
{
  // other configuration settings...
  //
  ""WeatherSkill"": {
    ""api_key"": ""<insert your API key here>""
  }
}

API Key Services
These are the keys currently used in Mycroft Core:

STT API, Google STT, Google Cloud Speech
Weather Skill API, OpenWeatherMap
Wolfram-Alpha Skill

Using Mycroft behind a proxy
Many schools, universities and workplaces run a proxy on their network. If you need to type in a username and password to access the external internet, then you are likely behind a proxy.
If you plan to use Mycroft behind a proxy, then you will need to do an additional configuration step.
NOTE: In order to complete this step, you will need to know the hostname and port for the proxy server. Your network administrator will be able to provide these details. Your network administrator may want information on what type of traffic Mycroft will be using. We use https traffic on port 443, primarily for accessing ReST-based APIs.
Using Mycroft behind a proxy without authentication
If you are using Mycroft behind a proxy without authentication, add the following environment variables, changing the proxy_hostname.com and proxy_port for the values for your network. These commands are executed from the Linux command line interface (CLI).
$ export http_proxy=http://proxy_hostname.com:proxy_port
$ export https_port=http://proxy_hostname.com:proxy_port
$ export no_proxy=""localhost,127.0.0.1,localaddress,.localdomain.com,0.0.0.0,::1""
Using Mycroft behind an authenticated proxy
If  you are behind a proxy which requires authentication, add the following environment variables, changing the proxy_hostname.com and proxy_port for the values for your network. These commands are executed from the Linux command line interface (CLI).
$ export http_proxy=http://user:password@proxy_hostname.com:proxy_port
$ export https_port=http://user:password@proxy_hostname.com:proxy_port
$ export no_proxy=""localhost,127.0.0.1,localaddress,.localdomain.com,0.0.0.0,::1""
Getting Involved
This is an open source project and we would love your help. We have prepared a contributing guide to help you get started.
If this is your first PR or you're not sure where to get started,
say hi in Mycroft Chat and a team member would be happy to mentor you.
Join the Mycroft Forum for questions and answers.
Links

Creating a Skill
Documentation
Skill Writer API Docs
Release Notes
Mycroft Chat
Mycroft Forum
Mycroft Blog

",batch1,16:57:57,Done
7,manjulaRathnayaka/app-cloud,"Welcome to WSO2 Integration Cloud
Integration Cloud enables you to host applications written in multiple languages on multiple runtimes such as WSO2 Application Server, MSF4J, WSO2 Integration Server etc.
##High level features

application/service hosting
custom URL for application endpoints
runtime logs of deployed applications
configurable application scaling factors
application revision management
dashboards(application and runtime statistics)
support of env variables
tagging application(stage=Dev, version=2.0.0 etc)
mysql database support
container specifications to create docker containers
tier implementation

##Getting Help
Use dev@wso2.org mailing list to ask questions, get support. You can subscribe to wso2 mailing list from site 
##License
WSO2 Inc. licenses this source under the Apache License,
Version 2.0 (the ""License""); you may not use this file except
in compliance with the License. You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing,software distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the
specific language governing permissions and limitations under the License.
",batch2,8:09:16,Done
8,ramblehead/emacs,,batch1,16:59:00,Done
9,gcampax/mutter,,batch2,8:09:16,Done
10,RanvierMUD/ranviermud,"
ranvier
Node.js-based MUD engine
Ranvier is a MUD game engine whose goal is to be a simple but powerful way to build whatever MUD you want with special care given to extensibility. The core code strives to be completely unopinionated toward any specific style of game while using the bundle system to build the game you want without having to dig through the engine's code.
Special Features

Robust bundle system: Nearly every aspect of the game can be modified without changing the core and allows for easy
packaging and sharing of commands/areas/items/npcs/channels/behaviors
Unopinionated network layer: easily swap out telnet for any network layer you like. No need to gut the whole codebase
just to support a different transport type, just drop in a file.
Customizable data layer: You are not tied to saving in any particular database or file storage sytem
Optional coordinate based room system allowing for the flexibilty of a standard MUD world with the easy mappability of
a strict 3D world.
Scripting for all entities in the game for any event along with behaviors to create shared, composable scripts
Skill system with passive/active skills
Effects e.g., buffs/debuffs
Quest system allowing for starting/progress/completion from any event in the game
Communication channels with custom audiences

Documentation
Ranvier prides itself on having thorough documentation which is available on our website: ranviermud.com
Slack
We have a Slack channel you can use to ask questions, suggest features, or just keep up to date with the project: https://ranviermud.slack.com
Get an invite
Requirements

Node.js >= v10.12.0

Demo
Point your favorite client or telnet to ranviermud.com port 4000. This demo server is wiped and updated from the master branch every hour.
",batch2,8:10:17,Done
11,CallistoNetwork/CallistoEtherWallet,"




 
 
MyEtherWallet is a doorway to the Ethereum blockchain, allowing users to manage their own funds without a centralized platform. -Stephen, #MEWForce

Philosophy

Empower the people: Give people the ability to interact with the Ethereum blockchain easily, without having to run a full node.
Make it easy & free: Everyone should be able to create a wallet and send Ether & Tokens without additional cost.
People are the Priority: People are the most important & their experience trumps all else. If monetization worsens the experience, we don't do it. (e.g. ads)
A learning experience, too: We want to educate about Ethereum, security, privacy, the importance of controlling your own keys, how the blockchain works, and how Ethereum and blockchain technologies enable a better world.
If it can be hacked, it will be hacked: Never save, store, or transmit secret info, like passwords or keys.
Offline / Client-Side: User should be able to run locally and offline without issue.
Private: No tracking!!! No emails. No ads. No demographics. We don't even know how many wallets have been generated, let alone who / what / where you are.
Open source & auditable

MEW Around the Web

Website: https://www.myetherwallet.com/
CX: https://chrome.google.com/webstore/detail/myetherwallet-cx/nlbmnnijcnlegkjjpcfjclmcfggfefdm
Anti-phish CX
FB: https://www.facebook.com/MyEtherWallet/
Twitter: https://twitter.com/myetherwallet
Medium: https://medium.com/@myetherwallet
Github MEW Repo: https://github.com/MyEtherWallet/MyEtherWallet
Github MEW Org: https://github.com/MyEtherWallet
Github Latest Releases: https://github.com/MyEtherWallet/MyEtherWallet/releases/latest
Github Anti-phish CX: https://github.com/409H/EtherAddressLookup
MEW ETH Donation Address: 0xDECAF9CD2367cdbb726E904cD6397eDFcAe6068D (mewtopia.eth)
MEW BTC Donation Address: 1DECAF2uSpFTP4L1fAHR8GCLrPqdwdLse9

Bug / Feature Request
If you find a bug, or want a new feature added, please submit it on the Github Issues
Getting started

Open terminal
Clone the repo: git clone git@github.com:MyEtherWallet/MyEtherWallet.git
run npm i to install node packages.
run npm run build. You can also use the offline version by opening the index file from the dist folder with your preferred browser
start npm start
If npm start fails and above the error message it states 'new update found' then the package.json version of the indicated packages needs to be updated to match the versions shown in the notice.
App should be running in https://localhost:8080

Developers

Open terminal
Clone the repo: git clone git@github.com:MyEtherWallet/MyEtherWallet.git
run git checkout develop
run npm i to install node packages.
run npm run build. You can also use the offline version by opening the index file from the dist folder with your preferred browser
start npm run dev
App should be running in https://localhost:8080

EACCESS issue can be resolved by running: sudo chown -R $(whoami) ~/.npm
For other issues, try the steps shown here: https://github.com/MyEtherWallet/MyEtherWallet/issues/1182#issuecomment-506342875 by @tomwalton78
",batch2,8:09:15,Done
12,jonasoreland/runnerup,"RunnerUp


Track your sport activities with RunnerUp using the GPS in your Android phone.
Features

See detailed stats around your pace, distance and time
Get stats and progress with built-in highly configurable audio cues
Run free runs with target pace or target heart rate zone
Easily configure and run effective interval workouts modeled after Garmin
Automatic upload to various external applications such as Strava and Runalyze. Some also support download and feed updates (see here for details).
Share your favorite workouts with friends (using email)
Heart rate monitor: Bluetooth SMART (BLE) and ANT+ (as well as PolarWearLink and Zephyr)
Configure and use heart rate zones
Phone internal sensors like step sensor and barometer.
WearOS app
Pebble support

Release

Play Store Join beta group for early access.
GitHub All releases, including alpha releases.
F-Droid This version does not contain some features due to licensing, see the F-Droid description.

The current 2.x release support Android 4.0 and later. The Play release also supports Android 2.2, 2.3, but due to lack of test devices, RunnerUp cannot support Android 2.x. The ""Froyo"" version do not include graphs, maps and elevation GeoId correction.
User information
Please read the wiki, especially the Manual
Contributing
Patches, forks, pull requests, suggestions or harsh flame is welcome!
Please read the wiki.
Translations

Interested in helping to translate RunnerUp? Contribute on Transifex.
License
Unless otherwise stated, the code for this project is under GNU GPL v3. See LICENSE for more information.
Components licensed differently are listed on the CREDITS page.
Donations
If your already donate to UNHCR, UNICEF and/or other important things, you might donate using paypal .
",batch2,8:10:17,Done
13,Shifter78/shitno,"LenoxBot

LenoxBot is a Discord bot that offers many cool new features to your Discord server!






Table of Content

Installation
Support
License

Installation
You can find the whole instructions to download and how to configure the bot in our documentation here.
Support
Twitter
Discord Server
Documentation
License
MIT
",batch2,8:09:15,Done
14,AtScaleInc/Impala,,batch2,8:10:17,Done
15,WPN-XM/registry,"Software Registries of the WPN-XM Server Stack

",batch2,8:10:18,Done
16,dltsys/dashj," 

Welcome to bitcoinj
The bitcoinj library is a Java implementation of the Bitcoin protocol, which allows it to maintain a wallet and send/receive transactions without needing a local copy of Bitcoin Core. It comes with full documentation and some example apps showing how to use it.
Technologies

Java 6 for the core modules, Java 8 for everything else
Maven 3+ - for building the project
Orchid - for secure communications over TOR
Google Protocol Buffers - for use with serialization and hardware communications

Getting started
To get started, it is best to have the latest JDK and Maven installed. The HEAD of the master branch contains the latest development code and various production releases are provided on feature branches.
Building from the command line
To perform a full build use
mvn clean package

You can also run
mvn site:site

to generate a website with useful information like JavaDocs.
The outputs are under the target directory.
Building from an IDE
Alternatively, just import the project using your IDE. IntelliJ has Maven integration built-in and has a free Community Edition. Simply use File | Import Project and locate the pom.xml in the root of the cloned project source tree.
Example applications
These are found in the examples module.
Forwarding service
This will download the block chain and eventually print a Bitcoin address that it has generated.
If you send coins to that address, it will forward them on to the address you specified.
  cd examples
  mvn exec:java -Dexec.mainClass=org.bitcoinj.examples.ForwardingService -Dexec.args=""<insert a bitcoin address here>""

Note that this example app does not use checkpointing, so the initial chain sync will be pretty slow. You can make an app that starts up and does the initial sync much faster by including a checkpoints file; see the documentation for
more info on this technique.
Where next?
Now you are ready to follow the tutorial.
",batch2,8:09:16,Done
17,Hydractify/forgottenserver,"



























forgottenserver
Repository for maintaing our custom open tibia server using forgottenserver.
What you need

MySQL
A computer

Getting started
This server has been tested/used in Linux, we cannot guarantee it will work in MacOS or Windows, however, it might.
Compiling
The steps to compile the server can be found here: https://github.com/otland/forgottenserver/wiki/Compiling
MySQL
After you built tfs, you can proceed to work with MySql. You can use whatever user and database name you want, our recommendation is to have forgottenserver.
After you set up the user and granted permission to the database, you can run the schema.sql located in the root of the project. Here is a handy way to do it in Linux: sudo mysql -u root -p forgottenserver < ./schema.sql
Configuration
Copy the config.lua.dist and name it config.lua, there you want to change the mysqlUser to whatever you named the MySql user, mysqlPass for it's password (if there is any) and change mysqlDatabase to whatever you named the database.
You also want to make sure that you have decompressed the map.rar file that comes inside data/world, otherwise the server will not find a map. If you have not cloned with the submodule flag, you can get the map files in orts/world.
Once you are done with that, all you have to do is run the tfs that you compiled! Have fun. :)
Documentation
Use this link as reference for everything related to the open tibia server:

forgottenserver

You will be able to find the world that we are using as well as the server we are basing ourselves off of in these links:

orts/world
orts/server

",batch2,8:10:17,Done
18,amlatyrngom/SQLIR,"The LLVM Compiler Infrastructure
This directory and its sub-directories contain source code for LLVM,
a toolkit for the construction of highly optimized compilers,
optimizers, and run-time environments.
The README briefly describes how to get started with building LLVM.
For more information on how to contribute to the LLVM project, please
take a look at the
Contributing to LLVM guide.
Getting Started with the LLVM System
Taken from https://llvm.org/docs/GettingStarted.html.
Overview
Welcome to the LLVM project!
The LLVM project has multiple components. The core of the project is
itself called ""LLVM"". This contains all of the tools, libraries, and header
files needed to process intermediate representations and converts it into
object files.  Tools include an assembler, disassembler, bitcode analyzer, and
bitcode optimizer.  It also contains basic regression tests.
C-like languages use the Clang front end.  This
component compiles C, C++, Objective C, and Objective C++ code into LLVM bitcode
-- and from there into object files, using LLVM.
Other components include:
the libc++ C++ standard library,
the LLD linker, and more.
Getting the Source Code and Building LLVM
The LLVM Getting Started documentation may be out of date.  The Clang
Getting Started page might have more
accurate information.
This is an example work-flow and configuration to get and build the LLVM source:


Checkout LLVM (including related sub-projects like Clang):


git clone https://github.com/llvm/llvm-project.git


Or, on windows, git clone --config core.autocrlf=false  https://github.com/llvm/llvm-project.git




Configure and build LLVM and Clang:


cd llvm-project


mkdir build


cd build


cmake -G <generator> [options] ../llvm
Some common build system generators are:

Ninja --- for generating Ninja
build files. Most llvm developers use Ninja.
Unix Makefiles --- for generating make-compatible parallel makefiles.
Visual Studio --- for generating Visual Studio projects and
solutions.
Xcode --- for generating Xcode projects.

Some Common options:


-DLLVM_ENABLE_PROJECTS='...' --- semicolon-separated list of the LLVM
sub-projects you'd like to additionally build. Can include any of: clang,
clang-tools-extra, libcxx, libcxxabi, libunwind, lldb, compiler-rt, lld,
polly, or debuginfo-tests.
For example, to build LLVM, Clang, libcxx, and libcxxabi, use
-DLLVM_ENABLE_PROJECTS=""clang;libcxx;libcxxabi"".


-DCMAKE_INSTALL_PREFIX=directory --- Specify for directory the full
path name of where you want the LLVM tools and libraries to be installed
(default /usr/local).


-DCMAKE_BUILD_TYPE=type --- Valid options for type are Debug,
Release, RelWithDebInfo, and MinSizeRel. Default is Debug.


-DLLVM_ENABLE_ASSERTIONS=On --- Compile with assertion checks enabled
(default is Yes for Debug builds, No for all other build types).




cmake --build . [-- [options] <target>] or your build system specified above
directly.


The default target (i.e. ninja or make) will build all of LLVM.


The check-all target (i.e. ninja check-all) will run the
regression tests to ensure everything is in working order.


CMake will generate targets for each tool and library, and most
LLVM sub-projects generate their own check-<project> target.


Running a serial build will be slow.  To improve speed, try running a
parallel build.  That's done by default in Ninja; for make, use the option
-j NNN, where NNN is the number of parallel jobs, e.g. the number of
CPUs you have.




For more information see CMake




Consult the
Getting Started with LLVM
page for detailed information on configuring and compiling LLVM. You can visit
Directory Layout
to learn about the layout of the source code tree.
",batch1,16:57:57,Done
19,daskeyboard/daskeyboard.io,"Das Keyboard Q Documentation Website
This is the source files repo for https://www.daskeyboard.io.


Issues, bugs, and requests
We welcome contributions and feedback.
Please file a request in our
issue tracker
and we'll take a look.
Dev env installation
A TLDR version follows:

Ensure you have Ruby installed; you need version 2.2.2 or later:

ruby --version


Ensure you have Bundler installed; if not install with:

gem install bundler -v 2.0.1


Install all dependencies:

bundle install



If you see this error:
ERROR: Failed to build gem native extension
then you'll need change your ruby version by using (with X.X version asked in error message):
sudo apt-get install rubyX.X-dev
If you see nokogiri installation error, make sure you got required dependencices:
    sudo apt-get install libxslt-dev libxml2-dev
View Site in dev mode
 bundle exec jekyll serve
or
jekyll serve -w --force_polling
Testing
rake checklinks

IMPORTANT
Need to run the website in another process

Some form of broken links prevention is done automatically by rake checklinks
on every commit (through tool/travis.sh). But this will not see any Firebase
redirects (rake checklinks does not run the Firebase server) and it will not
check incoming links.
Before we can move the more complete
automated linkcheck solution
from dartlang.org, we recommend manually running the following.


First time setup:
pub global activate linkcheck
npm install -g superstatic


Start the localhost Firebase server:
superstatic --port 3474


Run the link checker:
linkcheck :3474


Even better, to check that old URLs are correctly redirected:
linkcheck :3474 --input tool/sitemap.txt

Automatic deployment
Merge your work on branch deploy, and push it.

You need to make sure than the tests pass.

We have set up Travis to deploy on commit on the git branch deploy.
Manual deployment
Generate static site:
bundle exec jekyll build # build goes to ./_site

Deploy to Firebase hosting
firebase deploy -p ./_site

Adding next/previous page links
If you have a document that spans multiple pages, you can add next and previous
page links to make navigating these pages easier. It involves adding some information
to the front matter of each page, and including some HTML.
---
layout: tutorial
title: ""Constraints""

permalink: /tutorials/layout/constraints.html
prev-page: /tutorials/layout/properties.html
prev-page-title: ""Container Properties""
next-page: /tutorials/layout/create.html
next-page-title: ""Create a Layout""
---

{% include prev-next-nav.html %}

{:toc}

<!-- PAGE CONTENT -->

{% include prev-next-nav.html %}
Omit the ""prev-page"" info for the first page, and the ""next-page"" info for the
last page.
Syntax highlighting
The website uses prism.js for syntax
highlighting. This section covers how to use syntax highlighting, and
how to update our syntax highlighter for new languages.
Supported languages
This website can syntax highlight the following languages:

shell
dart
html
css
javascript
java
objectivec
swift
go
php
python
ruby

Using syntax highlighting
The easiest way to syntax highlight a block of code is to wrap
it with triple backticks followed by the language.
Here's an example:
class SomeCode {
  String name;
}
See the list of supported languages above for what to use
following the first triple backticks.
Adding more languages for syntax highlighting
The  website uses a custom build of prism, which
includes only the languages the website requires. To improve
load times and user experience, we do not support every
language that prism supports.
To add a new language for syntax highlighting, you will need
to generate a new copy of the prism.js file.
Follow these steps to generate a new copy of prism.js:

Open js/prism.js
Copy the URL in the comment of the first line of the file
Paste it into a browser window/tab
Add the new language that you wish to syntax highlight
DO NOT change the other plugins, languages, or settings
Download the generated JavaScript, and use it to replace js/prism.js
Download the generated CSS, and use it to replace _sass/_prism.scss

Including a region of a file
You can include a specific range of lines from a file:
{% include includelines filename=PATH start=INT count=INT %}
PATH must be inside of _include. If you are including source code,
place that code into _include/code to follow our convention.
",batch2,8:10:17,Done
20,davidswelt/aquamacs-emacs-pre2015,,batch1,16:57:57,Done
21,clarete/emacs,,batch1,16:59:00,Done
22,comitterman/testrepo,"testrepo
test
",batch1,16:57:59,Done
23,lawleagle/emacs-nobinds,"emacs-nobinds -- Emacs Without Default Keybindings
Emacs is known for it's default stupid keybindings, so this project removes them all (or almost, very few still got through).
emacs-nobinds is based on emacs-26, changes done to emacs-26 to bring this version to life can be viewed in this PR (which will never be closed): https://github.com/lawleagle/emacs-nobinds/pull/1
",batch1,16:59:01,Done
24,nexjhealth/x264,,batch2,8:10:17,Done
25,Mel1818/mycroft-changes,"   
 


Mycroft
Mycroft is a hackable open source voice assistant.
Table of Contents

Table of Contents
Getting Started
Running Mycroft
Using Mycroft

Home Device and Account Manager
Skills


Behind the scenes

Pairing Information
Configuration
Using Mycroft Without Home
API Key Services
Using Mycroft behind a proxy

Using Mycroft behind a proxy without authentication
Using Mycroft behind an authenticated proxy




Getting Involved
Links

Getting Started
First, get the code on your system!  The simplest method is via git (git installation instructions):

cd ~/
git clone https://github.com/MycroftAI/mycroft-core.git
cd mycroft-core
bash dev_setup.sh

This script sets up dependencies and a virtualenv.  If running in an environment besides Ubuntu/Debian, Arch or Fedora you may need to manually install packages as instructed by dev_setup.sh.
NOTE: The default branch for this repository is 'dev', which should be considered a work-in-progress. If you want to clone a more stable version, switch over to the 'master' branch.
Running Mycroft
Mycroft provides start-mycroft.sh to perform common tasks. This script uses a virtualenv created by dev_setup.sh.  Assuming you installed mycroft-core in your home directory run:

cd ~/mycroft-core
./start-mycroft.sh debug

The ""debug"" command will start the background services (microphone listener, skill, messagebus, and audio subsystems) as well as bringing up a text-based Command Line Interface (CLI) you can use to interact with Mycroft and see the contents of the various logs. Alternatively you can run ./start-mycroft.sh all to begin the services without the command line interface.  Later you can bring up the CLI using ./start-mycroft.sh cli.
The background services can be stopped as a group with:

./stop-mycroft.sh

Using Mycroft
Home Device and Account Manager
Mycroft AI, Inc. maintains a device and account management system known as Mycroft Home. Developers may sign up at: https://home.mycroft.ai
By default, mycroft-core  is configured to use Home. By saying ""Hey Mycroft, pair my device"" (or any other request verbal request) you will be informed that your device needs to be paired. Mycroft will speak a 6-digit code which you can entered into the pairing page within the Mycroft Home site.
Once paired, your unit will use Mycroft API keys for services such as Speech-to-Text (STT), weather and various other skills.
Skills
Mycroft is nothing without skills.  There are a handful of default skills that are downloaded automatically to your /opt/mycroft/skills directory, but most need to be installed explicitly.  See the Skill Repo to discover skills made by others.  And please share your own interesting work!
Behind the scenes
Pairing Information
Pairing information generated by registering with Home is stored in:
~/.mycroft/identity/identity2.json <-- DO NOT SHARE THIS WITH OTHERS!
Configuration
Mycroft configuration consists of 4 possible locations:

mycroft-core/mycroft/configuration/mycroft.conf(Defaults)
Mycroft Home (Remote)
/etc/mycroft/mycroft.conf(Machine)
$HOME/.mycroft/mycroft.conf(User)

When the configuration loader starts, it looks in these locations in this order, and loads ALL configurations. Keys that exist in multiple configuration files will be overridden by the last file to contain the value. This process results in a minimal amount being written for a specific device and user, without modifying default distribution files.
Using Mycroft Without Home
If you do not wish to use the Mycroft Home service, you may insert your own API keys into the configuration files listed below in configuration.
The place to insert the API key looks like the following:
[WeatherSkill]
api_key = """"
Put a relevant key inside the quotes and mycroft-core should begin to use the key immediately.
API Key Services
These are the keys currently used in Mycroft Core:

STT API, Google STT, Google Cloud Speech
Weather Skill API, OpenWeatherMap
Wolfram-Alpha Skill

Using Mycroft behind a proxy
Many schools, universities and workplaces run a proxy on their network. If you need to type in a username and password to access the external internet, then you are likely behind a proxy.
If you plan to use Mycroft behind a proxy, then you will need to do an additional configuration step.
NOTE: In order to complete this step, you will need to know the hostname and port for the proxy server. Your network administrator will be able to provide these details. Your network administrator may want information on what type of traffic Mycroft will be using. We use https traffic on port 443, primarily for accessing ReST-based APIs.
Using Mycroft behind a proxy without authentication
If you are using Mycroft behind a proxy without authentication, add the following environment variables, changing the proxy_hostname.com and proxy_port for the values for your network. These commands are executed from the Linux command line interface (CLI).
$ export http_proxy=http://proxy_hostname.com:proxy_port
$ export https_port=http://proxy_hostname.com:proxy_port
$ export no_proxy=""localhost,127.0.0.1,localaddress,.localdomain.com,0.0.0.0,::1""
Using Mycroft behind an authenticated proxy
If  you are behind a proxy which requires authentication, add the following environment variables, changing the proxy_hostname.com and proxy_port for the values for your network. These commands are executed from the Linux command line interface (CLI).
$ export http_proxy=http://user:password@proxy_hostname.com:proxy_port
$ export https_port=http://user:password@proxy_hostname.com:proxy_port
$ export no_proxy=""localhost,127.0.0.1,localaddress,.localdomain.com,0.0.0.0,::1""
Getting Involved
This is an open source project and we would love your help. We have prepared a contributing guide to help you get started.
If this is your first PR or you're not sure where to get started,
say hi in Mycroft Chat and a team member would be happy to mentor you.
Join the Mycroft Forum for questions and answers.
Links

Creating a Skill
Documentation
Skill Writer API Docs
Release Notes
Mycroft Chat
Mycroft Forum
Mycroft Blog

",batch1,16:57:57,Done
26,oaplatform/oap,"Open Application Platform
",batch2,8:10:17,Done
27,jopohl/urh,"





The Universal Radio Hacker (URH) is a complete suite for wireless protocol investigation with native support for many common Software Defined Radios.
URH allows easy demodulation of signals combined with an automatic detection of modulation parameters making it a breeze to identify the bits and bytes that fly over the air.
As data often gets encoded before transmission, URH offers customizable decodings to crack even sophisticated encodings like CC1101 data whitening.
When it comes to protocol reverse-engineering, URH is helpful in two ways. You can either manually assign protocol fields and message types or let URH automatically infer protocol fields with a rule-based intelligence.
Finally, URH entails a fuzzing component aimed at stateless protocols and a simulation environment for stateful attacks.
Getting started
In order to get started

view the installation instructions on this page,
download the official userguide (PDF),
watch the demonstration videos (YouTube),
check out the wiki for more information such as supported devices or
read some articles about URH for inspiration.

If you like URH, please ⭐ this repository and join our Slack channel. We appreciate your support!
Citing URH
We encourage researchers working with URH to cite this WOOT'18 paper or directly use the following BibTeX entry.

 URH BibTeX entry for your research paper 
@inproceedings {220562,
author = {Johannes Pohl and Andreas Noack},
title = {Universal Radio Hacker: A Suite for Analyzing and Attacking Stateful Wireless Protocols},
booktitle = {12th {USENIX} Workshop on Offensive Technologies ({WOOT} 18)},
year = {2018},
address = {Baltimore, MD},
url = {https://www.usenix.org/conference/woot18/presentation/pohl},
publisher = {{USENIX} Association},
}

Installation
URH runs on Windows, Linux and macOS. Click on your operating system below to view installation instructions.

Windows
On Windows, URH can be installed with its Installer. No further dependencies are required.
If you get an error about missing api-ms-win-crt-runtime-l1-1-0.dll, run Windows Update or directly install KB2999226.


Linux

Generic Installation with pip (recommended)
URH is available on PyPi so you can install it with
# IMPORTANT: Make sure your pip is up to date
sudo python3 -m pip install --upgrade pip  # Update your pip installation
sudo python3 -m pip install urh            # Install URH
This is the recommended way to install URH on Linux because it comes with all native extensions precompiled.
In order to access your SDR as non-root user, install the according udev rules. You can find them in the wiki.


Install via Package Manager
URH is included in the repositories of many linux distributions such as Arch Linux, Gentoo, Fedora, openSUSE or NixOS. There is also a package for FreeBSD.  If available, simply use your package manager to install URH.
Note: For native support, you must install the according -dev package(s) of your SDR(s) such as hackrf-dev before installing URH.


Snap
URH is available as a snap: https://snapcraft.io/urh


Docker Image
The official URH docker image is available here. It has all native backends included and ready to operate.



macOS

Using DMG
It is recommended to use at least macOS 10.14 when using the DMG available here.


With pip

Install Python 3 for Mac OS X.
If you experience issues with preinstalled Python, make sure you update to a recent version using the given link.
(Optional) Install desired native libs e.g. brew install librtlsdr for
corresponding native device support.
In a terminal, type: pip3 install urh.
Type urh in a terminal to get it started.




Update your installation
If you installed URH via pip you can keep it up to date with python3 -m pip install --upgrade urh.


Running from source

Without installation
To execute the Universal Radio Hacker without installation, just run:
git clone https://github.com/jopohl/urh/
cd urh/src/urh
./main.py
Note, before first usage the C++ extensions will be built.


Installing from source
To install URH from source you need to have python-setuptools installed. You can get them with python3 -m pip install setuptools.
Once the setuptools are installed execute:
git clone https://github.com/jopohl/urh/
cd urh
python setup.py install
And start the application by typing urh in a terminal.


Articles
Hacking stuff with URH

Hacking Burger Pagers
Reverse-engineer and Clone a Remote Control
Reverse-engineering Weather Station RF Signals
Reverse-engineering Wireless Blinds
Attacking Logitech Wireless Presenters (German Article)
Attacking Wireless Keyboards
Reverse-engineering a 433MHz Remote-controlled Power Socket for use with Arduino

General presentations and tutorials on URH

Hackaday Article
RTL-SDR.com Article
Short Tutorial on URH with LimeSDR Mini
Brute-forcing a RF Device: a Step-by-step Guide
Hacking wireless sockets like a NOOB

External decodings
See wiki for a list of external decodings provided by our community! Thanks for that!
Screenshots
Get the data out of raw signals

Keep an overview even on complex protocols

Record and send signals

",batch2,8:09:16,Done
28,ODEX-TOS/packages,"











packages

    This repo contains a lot of packages forked from upstream as some need to be modified
    
Explore the docs »


Report Bug
    ·
    Request Feature


Table of Contents

About the Project

Built With


Getting Started

Prerequisites
Installation


Usage
Repo
Roadmap
Contributing
License
Contact
Acknowledgements

About The Project
Built By

F0xedb

For more information, please refer to the Documentation
Roadmap
See the open issues for a list of proposed features (and known issues).
Contributing
Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are greatly appreciated.

Fork the Project
Create your Feature Branch (git checkout -b feature/AmazingFeature)
Commit your Changes (git commit -m 'Add some AmazingFeature')
Push to the Branch (git push origin feature/AmazingFeature)
Open a Pull Request

License
Distributed under the MIT License. See LICENSE for more information.
Contact
Tom Meyers - tom@odex.be
Project Link: https://github.com/ODEX-TOS/packages
Acknowledgements

F0xedb
TOS Homepage
Arch Linux

",batch1,16:57:58,Done
29,adamlaska/osmos-cosmos-sdk,"Cosmos SDK









The Cosmos-SDK is a framework for building blockchain applications in Golang.
It is being used to build Gaia, the first implementation of the Cosmos Hub.
WARNING: The SDK has mostly stabilized, but we are still making some
breaking changes.
Note: Requires Go 1.13+
Quick Start
To learn how the SDK works from a high-level perspective, go to the SDK Intro.
If you want to get started quickly and learn how to build on top of the SDK, please follow the SDK Application Tutorial. You can also fork the tutorial's repo to get started building your own Cosmos SDK application.
For more, please go to the Cosmos SDK Docs
Cosmos Hub Mainnet
The Cosmos Hub application, gaia, has moved to its own repository. Go there to join the Cosmos Hub mainnet and more.
Disambiguation
This Cosmos-SDK project is not related to the React-Cosmos project (yet). Many thanks to Evan Coury and Ovidiu (@skidding) for this Github organization name. As per our agreement, this disambiguation notice will stay here.
",batch2,8:09:15,Done
30,cymbiotica/ROM-mud,"RanvierMUD Tiny is a barebones starter kit for people who ""know what they're doing"" and just want to have a telnet
connection, login, command parsing, and movement. There is almost nothing here which may be preferable for those that
know what they want and don't want to spend time tearing everything down before they can build it back up.
This setup includes account creation, multiplayer players per account, a very basic command parser that understands
room exits and exact command entry.  That is to say the player needs to type 'look' if they want to look, not 'l'.
This gives you freedom to do whatever kind of handling you want without any code to tear down.
There are exactly 2 commands included: look and quit and there is one starter area with 3 rooms to demonstrate that
movement actually works.
The basic command parser does not support skills or channels, if you're using this bundle you know what you're doing and
can decide how you want those to be parsed.  The input events for the login/character creation flow are nearly identical
to bundle-example-input-events but pared down to the bare minimum. There are no character classes. There is only one
default attribute: health.
Installation
git clone --recursive https://github.com/ranviermud/tiny
cd tiny
npm install
git submodule foreach npm install

",batch2,8:10:17,Done
31,thiagoblima/math-board-converter,"Math Converter
Welcome to the e-learning Math Board Converter app, created for educational purposes letting the students quickly convert math's tasks.
Complementary tasks are going to be added here for the students, easily to manage through the student's ID.
Intro
MEAN Stack Application:

MongoDB v3.4
Expressjs v4.16.2
Angular v1.x
NodeJS v8.9.1 LTS
TypeScript v2.6.2 - back-end

Architecture
The server-side counts with a REST API based architecture, using JWT user authorization for granting access on the client-side, each user receives a JWT TOKEN for a session that may expire for inactivity or in case user logging out.
The client-side stores the JWT TOKEN on the local storage of your browser, an auth-service is provieded on its end.
Getting Started


Create a fork of this repository


Make sure you're using a version of Nodejs >= v7.2.1.


Make sure you've got ruby x64 architectureinstalled.




Mac:

Make sure you have brew installed: brew link
brew install ruby
gem install sass



Linux:

sudo apt-get install ruby-ful
gem install sass



Windows:

Make sure you have ruby installed: ruby installer link
gem install sass





Testing ruby
$ ruby -v
ruby 2.2.3p173 (2015-08-18 revision 51636) [x64-mingw32]



Testing sass
$ sass -v
Sass 3.4.21 (Selective Steve)



Installing back-end / server-side dependencies

./

$ npm install



Installing front-end / client-side dependencies

./public/

$ npm install 



Starting Nodejs server

./

$ node app 
Math Server Started on port: http://localhost:3412



References
Technologies you are going to see in this project:

Front-end


HTML5
CSS3
Bootstrap
Java Script ES5
AngularJS
GruntJS
SASS
Compass


Back-end


NodeJS
Express
MongoDB

This software is licensed under the terms of the MIT license.
",batch2,8:10:17,Done
32,berigei/kactus2dev,"                            README : Kactus2
       Copyright (c) 2012-2017 Tampere University of Technology
                        http://funbase.cs.tut.fi

Summary
Kactus2 is a toolset for designing embedded products, especially FPGA-based
MP-SoCs. The aim is easier IP reusabilility and integration for both hardware and
software. The tool is based on IEEE 1685-2014 ""IP-XACT"" standard.
Windows installer and tar-package for Linux are available at https://sourceforge.net/projects/kactus2/
What you can do with Kactus2
Package IPs for reuse and exchange

Import your existing IPs as IP-XACT components
Create new IP-XACT components and generate their HDL module headers
Reuse IP-XACT files from any standard compatible vendor
Reuse the IPs in your designs and connect them with wires and busses

Create HW designs with hierarchy

Create multilevel hierarchies, where a design has multiple sub-designs
Configure component instances in designs, including the sub-designs
Use generator plugins to create HDL with wiring and parameterization

Integrate HW and SW

Use memory designer to preview memory maps and address spaces in your hierarchy
Package software to IP-XACT components and map them to hardware
Generate makefiles that build executables with rules defined in IP-XACT components

What you cannot do with Kactus2

Behavioral logic: Neither Kactus2 nor IP-XACT handles module implementations
Synthesis or simulation: These require tools that are specificly created for the purpose

Examples
Example IPs are available in GitHub at https://github.com/kactus2/ipxactexamplelib
Tutorials
Video tutorials are available in Youtube at https://www.youtube.com/user/Kactus2Tutorial
Community Guidelines
Issues should be reported at Kactus2 project management site: https://kactus2.cs.tut.fi
Otherwise support is provided by email: kactus2@cs.tut.fi
When contributing to Kactus2, you should see the wiki at the project management site.
It will guide on essentials like building with Visual Studio and plugin development.
Contributions to Kactus2 core, the IP-XACTmodels library, and the plugins provided by
TUT will not make to a release unless we accept them and you agree to transfer the
copyrights of the changes to TUT.
When creating a plugin, you may keep it closed or open source, and the copyright stays with you.
Contributors
Antti Kamppi, Joni-Matti Määttä, Lauri Matilainen, Timo D. Hämäläinen,
Mikko Teuho, Juho Järvinen, Esko Pekkarinen, Janne Virtanen
Kactus2 is linked with Qt 5.2.0, copyright Digia Plc. (LGPL)
Licencing
This software is licensed under the GPL2 General Public License.
Kactus2 is also available for dual licensing. Please contact kactus2@cs.tut.fi
to purchase a commercial license.
Icons used in Kactus2
Oxygen Icons by Oxygen Team
http://www.oxygen-icons.org/ (LGPL)
Farm Fresh Icons by Fatcow Web Hosting
http://www.fatcow.com/free-icons (CC Attribution 3.0)
realistiK Reloaded by Pavel InFeRnODeMoN
http://kde-look.org/content/show.php/realistiK+Reloaded?content=52362 (GPL)
Dark Glass by Alessandro Rei
http://kde-look.org/content/show.php/Dark-Glass+reviewed?content=67902 (GPL)
Human o2 by Oliver Scholtz
http://schollidesign.deviantart.com/art/Human-O2-Iconset-105344123
(Free for non-commercial use)
Office Icons by Custom Icon Design
http://www.customicondesign.com/ (Free for non-commercial use)
Snowish by Alexander Moore
http://gnome-look.org/content/show.php?content=32599 (GPL)
VistaICO Toolbar Icons by VistaICO.com
http://www.vistaico.com/ (CC Attribution 3.0 Unported)
Nuvola by David Vignoni
http://www.icon-king.com/ (LGPL)
Crystal Project by Everaldo Coelho
http://www.everaldo.com/
Aha-soft Icons
http://www.aha-soft.com/
Fugue Icons
http://p.yusukekamiyamane.com/ (CC Attribution 3.0 Unported)
Absque Icons
http://macthemes.net/forum/viewtopic.php?id=16796237 (Free for commercial use, requires backlink)
PixeloPhilia 2 by Omercetin
http://omercetin.deviantart.com/ (CC Attribution 3.0 Unported)
Dellipack by Dellustrations
http://logoexpedite.com/ (Free for commercial use)
Hypic by Shlyapnikova
http://shlyapnikova.deviantart.com/ (CC Attribution 3.0 Unported)
Must Have by Visual Pharm
http://www.visualpharm.com/ (CC Attribution-No Derivative Works 3.0 Unported)
Basic Set by Pixel Mixer
http://www.pixel-mixer.com/ (Free for commercial use)
",batch2,8:10:18,Done
33,novartema/Chart.js,"

    Simple yet flexible JavaScript charting for designers & developers








Documentation

Introduction
Getting Started
General
Configuration
Charts
Axes
Developers
Popular Extensions
Samples

Contributing
Instructions on building and testing Chart.js can be found in the documentation. Before submitting an issue or a pull request, please take a moment to look over the contributing guidelines first. For support, please post questions on Stack Overflow with the chartjs tag.
License
Chart.js is available under the MIT license.
",batch2,8:10:18,Done
34,ergoemacs/ergoemacs-mode,"


Ergoemacs Keybindings
Xah Lee, David Capello, and Matthew Fidler
Library Information
ErgoEmacs keybindings improve GNU Emacs for people who did not grew
up with Emacs. User interface is based on common modern software
interface familiar to most people today, such as using 【Ctrl+C】 key
for Copy,【Ctrl+Z】 for undo, 【Ctrl+O】 for Open file, and also
bundles many Emacs Lisp functions that are not in GNU Emacs by default.
(setq ergoemacs-theme nil)
(setq ergoemacs-keyboard-layout ""us"")
(require 'ergoemacs-mode)
(ergoemacs-mode 1)

Ergoemacs-mode makes use of make-composed-keymap and therefore is
only comptabile with emacs 24.1+
More information is found at http://ergoemacs.github.io
",batch2,8:10:17,Done
35,Mummut/wot-xvm,,batch2,8:10:18,Done
36,dperezde/little-penguin,"README
This README would normally document whatever steps are necessary to get your application up and running.
What is this repository for?
Keep track of dev of Eudyptula challenge

Quick summary
Version
Learn Markdown

How do I get set up?

Summary of set up
Configuration
Dependencies
Database configuration
How to run tests
Deployment instructions

Contribution guidelines

Writing tests
Code review
Other guidelines

Who do I talk to?

Repo owner or admin
Other community or team contact

",batch1,16:57:58,Done
37,hiroakit/emacs,,batch1,16:58:59,Done
38,nagyistoce/netzob,"Netzob : Inferring Communication Protocols

About Netzob

Functional Description
Netzob is an opensource tool for reverse engineering, traffic generation
and fuzzing of communication protocols. This tool allows to infer the message format (vocabulary)
and the state machine (grammar) of a protocol through passive and active processes.
Its objective is to bring state of art academic researches to the operational field,
by leveraging bio-informatic and grammatical inferring algorithms in a semi-automatic manner.
Netzob is suitable for reversing network protocols, structured files and system and
process flows (IPC and communication with drivers and devices).
Dedicated modules are provided to capture and import data in multiple contexts (network, file and process data acquisition).
Once inferred, a protocol model can afterward be exported to third party tools (Peach, Scapy, Wireshark, etc.)
or used in the traffic generation engine, to allow simulation of realistic and controllable communication endpoints and flows.
Netzob handles different types of protocols: text protocols (like HTTP and IRC), delimiter-based protocols,
fixed fields protocols (like IP and TCP) and variable-length fields protocols (like TLV-based protocols).

Technical Description
Netzob's source code is mostly made of Python (90%) with some specific
extensions in C (6%). It includes a graphical interface based on GTK3.
The tool is made of a core (officially maintained) and of bunch of
plugins (exporters, importers, ...). Some plugins are provided by the team while others are
created and managed directly by users.

More Information


Website:http://www.netzob.org

Email:contact@netzob.org

Mailing list:Two lists are available, use the SYMPA web interface to register.

IRC:You can hang-out with us on Freenode's IRC channel #netzob @ freenode.org.

Wiki:Discuss strategy on Netzob's wiki

Twitter:Follow Netzob's official accounts (@Netzob)




Get Started with Netzob

Install it
There are two main ways of installing Netzob. The first one is based on
per-OS installers while the other one is more 'pythonic'.
We recommend the per-OS installers for 'normal' users while
testers, developers and python experts might prefer the 'pythonic' way.

Per-OS Installers:
Please follow the specification documentations for each supported platform:


Debian/Ubuntu:Installation documentation on Debian (wiki)

Gentoo:Installation documentation on Gentoo (wiki)




Pythonic Installer:
As a 'classic' python project, Netzob is provided with its
setup.py. This file defines what and how to install the project on a
python hosting OS.
This file depends on setuptools which like few other modules cannot be
automatically installed. The reason why, you have to manually install the
following bunch of prerequisites before initiating Netzob's install process.

python
python-dev
python-impacket
libxml2-dev
libxslt-dev
python-setuptools
python-gi
gir1.2-gtk-3.0, gir1.2-glib-2.0, gir1.2-gdkpixbuf-2.0, gir1.2-pango-1.0
libgtk-3-0
graphviz

We also highly recommend to install the following additional dependencies:

python-babel (for the translations)
python-sphinx (for the documentation)

Once the required dependencies are installed, you can test Netzob in
developer mode:
python setup.py build
python setup.py develop --user

Otherwise, if you want to install Netzob on the system:
python setup.py build
python setup.py develop
python setup.py install


Start it
Once installed, running Netzob is as simple as executing the provided script:
$ ./netzob

This script is in Python's path if you've installed Netzob, otherwise
(in developer mode), it's located in the top distribution directory.

Miscellaneous
Configuration requirements for Network and PCAP input:
*Note: Capturing data from network interfaces often requires admin privileges. Before we provide a cleaner and secure way (see issue 425 on the bugtracker for updated information - https://dev.netzob.org/issues/425), a possible HACK is to provide additionnal capabilities to the python binary.* ::


$ sudo setcap cap_net_raw=ep /usr/bin/python2.XX
Configuration requirements for IPC input on Ubuntu:
$ sudo bash -c ""echo 0 > /proc/sys/kernel/yama/ptrace_scope""


Documentation
The folder doc/documentation contains all the documentation of Netzob.
The user manual can be generated based on RST sources located in folder
doc/documentation/source with the following commands:
$ sphinx-apidoc -o doc/documentation/source/developer_guide/API/ src/netzob/
$ sphinx-build -b html doc/documentation/source/ doc/documentation/build/


Contributing
There are multiple ways to help-us.

Defects and Features  Requests
Help-us by reporting bugs and requesting features using the Bug Tracker.

Translation
Netzob has support for translation.
Currently English and French languages are supported. New languages are welcome.

Join the Development Team
To participate in the development, you need to get the latest version,
modify it and submit your changes.
These operations are detailed on Netzob's wiki through the following
pages:

Accessing and using Git Repositories for Netzob development
First steps for a new developer

You're interested in joining, please contact-us !

Authors, Contributors and Sponsors
See the top distribution file AUTHORS.txt for the detailed and updated list
of authors, contributors and sponsors.

License
This software is licensed under the GPLv3 License. See the COPYING.txt file
in the top distribution directory for the full license text.

Extra


Zoby, the official mascot of Netzob.

",batch2,8:10:17,Done
39,vquentin/scinote-dev,"SciNote

About
SciNote is an open source electronic lab notebook (ELN) that helps you manage your laboratory work and stores all your experimental data in one place. SciNote is specifically designed for life science students, researchers, lab technicians and group leaders.
Build & run
See Wiki/Build & run.
Testing
See Wiki/Testing.
Contributing
See Wiki/Contributing & Collaboration.
License
SciNote is developed and maintained by BioSistemika USA, LLC, under Mozilla Public License Version 2.0.
See LICENSE-3RD-PARTY.txt for licenses of included third-party libraries.
",batch2,8:09:15,Done
40,EugeneYilia/EugeneSignIn,"EugeneLiu3022
",batch1,16:57:57,Done
41,LivelyKernel/lively.next,"lively.next
This is the repository of the lively.next project.
Requirements
Please note Currently the Lively server runs best on MacOS, Linux or the Windows Linux subsystem. Getting it going on pure Windows is possible but will require additional tweaks.
Make sure you have the following software installed.

node.js version 8 or later.
git

Installation and Setup

Clone this repository and run the install.sh script. This will install the necessary dependencies and sync the Lively Partsbin with lively-next.org. Please note that this process will take a few minutes.
Run the start.sh script.
Lively will now be running on your local computer at http://localhost:9011.

Docker Image
A docker image exists for this to try it out in the environment of your choice.

Download chrome.json and take note of where it is saved
Run the docker command as follows (replacing the seccomp section with the location above where the file was saved): docker run -d --restart=unless-stopped --init --security-opt seccomp=/path/to/chrome.json --name lively-next -p 127.0.0.1:9011:9011 engagelively/lively-next:alpha4.5.0
Once completely started, navigate to http://localhost:9011 

Documentation
Some hints and documentation can be found in the project wiki.
License
This project is MIT licensed.
",batch2,8:09:15,Done
42,macdaddy12561/fhir-server,"hapi-fhir
HAPI FHIR - Java API for HL7 FHIR Clients and Servers





Complete project documentation is available here:
http://jamesagnew.github.io/hapi-fhir/
A demonstration of this project is available here:
http://fhirtest.uhn.ca/
This project is Open Source, licensed under the Apache Software License 2.0.
",batch2,8:10:17,Done
43,openstack/taskflow,"Team and repository tags



TaskFlow


A library to do [jobs, tasks, flows] in a highly available, easy to understand
and declarative manner (and more!) to be used with OpenStack and other
projects.

Free software: Apache license
Documentation: https://docs.openstack.org/taskflow/latest/
Source: https://opendev.org/openstack/taskflow
Bugs: https://bugs.launchpad.net/taskflow/
Release notes: https://docs.openstack.org/releasenotes/taskflow/


Join us

https://launchpad.net/taskflow


Testing and requirements

Requirements
Because this project has many optional (pluggable) parts like persistence
backends and engines, we decided to split our requirements into two
parts: - things that are absolutely required (you can't use the project
without them) are put into requirements.txt. The requirements
that are required by some optional part of this project (you can use the
project without them) are put into our test-requirements.txt file (so
that we can still test the optional functionality works as expected). If
you want to use the feature in question (eventlet or the worker based engine
that uses kombu or the sqlalchemy persistence backend or jobboards which
have an implementation built using kazoo ...), you should add
that requirement(s) to your project or environment.

Tox.ini
Our tox.ini file describes several test environments that allow to test
TaskFlow with different python versions and sets of requirements installed.
Please refer to the tox documentation to understand how to make these test
environments work for you.

Developer documentation
We also have sphinx documentation in docs/source.
To build it, run:
$ python setup.py build_sphinx

",batch2,8:10:18,Done
44,Homebrew/linuxbrew-core,"Linuxbrew Core
Core formulae for the Homebrew package manager.
Homebrew/discussions (forum)
How do I install these formulae?
Just brew install <formula>. This is the default tap for Homebrew and is installed by default.
More Documentation, Troubleshooting, Contributing, Security, Community, Donations, License and Sponsors
See these sections in Homebrew/brew's README.
",batch1,16:57:58,Done
45,astronautlevel2/Luma3DS,"astronautlevel2.github.io
A small site and script made to automatically build the latest Luma3DS nightly every night and add it to a website
TODO:

Find another way to store builds before I run out of github space Done!
Implement a way to only build new commits to save space Done!
Make it look nicer

",batch2,8:09:16,Done
46,navidpati/git-github.com-substratum-substratum,"
substratum development branch 
A new theme manager based off Sony's Overlay Manager Service and Resource Runtime Overlay. This is the official repository of the substratum theme engine.
interfacer
interfacer is Substratum's background service that runs functions on a system level to trigger direct connections to native Android libraries, including Package Manager, Overlay Manager Service, SystemUI and Activity Manager. This alleviates the stress on the main app and allows for Substratum to run more smoothly.
https://github.com/substratum/interfacer
pull requests
Pull requests to dev would be highly appreciated and accepted!
team code review
To submit a code patch towards any platform applications, please direct them all to our Gerrit server!
https://substratum.review
",batch2,8:09:16,Done
47,Alphadelta14/jenkins-job-builder,"README
Jenkins Job Builder takes simple descriptions of Jenkins jobs in YAML or JSON
format and uses them to configure Jenkins. You can keep your job descriptions in
human readable text format in a version control system to make changes and
auditing easier. It also has a flexible template system, so creating many
similarly configured jobs is easy.
To install:
$ pip install --user jenkins-job-builder

Online documentation:

http://docs.openstack.org/infra/jenkins-job-builder/


Developers
Bug report:

https://storyboard.openstack.org/#!/project/723

Repository:

https://git.openstack.org/cgit/openstack-infra/jenkins-job-builder

Cloning:
git clone https://git.openstack.org/openstack-infra/jenkins-job-builder

A virtual environment is recommended for development.  For example, Jenkins
Job Builder may be installed from the top level directory:
$ virtualenv .venv
$ source .venv/bin/activate
$ pip install -r test-requirements.txt -e .

Patches are submitted via Gerrit at:

https://review.openstack.org/

Please do not submit GitHub pull requests, they will be automatically closed.
Mailing list:

https://groups.google.com/forum/#!forum/jenkins-job-builder

IRC:

#openstack-jjb on Freenode

More details on how you can contribute is available on our wiki at:

http://docs.openstack.org/infra/manual/developers.html


Writing a patch
We ask that all code submissions be pep8 and pyflakes clean.  The
easiest way to do that is to run tox before submitting code for
review in Gerrit.  It will run pep8 and pyflakes in the same
manner as the automated test suite that will run on proposed
patchsets.
When creating new YAML components, please observe the following style
conventions:

All YAML identifiers (including component names and arguments)
should be lower-case and multiple word identifiers should use
hyphens.  E.g., ""build-trigger"".
The Python functions that implement components should have the same
name as the YAML keyword, but should use underscores instead of
hyphens. E.g., ""build_trigger"".

This consistency will help users avoid simple mistakes when writing
YAML, as well as developers when matching YAML components to Python
implementation.

Unit Tests
Unit tests have been included and are in the tests folder. Many unit
tests samples are included as examples in our documentation to ensure that
examples are kept current with existing behaviour. To run the unit tests,
execute the command:
tox -e py34,py27


Note: View tox.ini to run tests on other versions of Python,
generating the documentation and additionally for any special notes
on running the test to validate documentation external URLs from behind
proxies.


Installing without setup.py
For YAML support, you will need libyaml installed.
Mac OS X:
$ brew install libyaml

Then install the required python packages using pip:
$ sudo pip install PyYAML python-jenkins

",batch2,8:09:16,Done
48,juancho85/opencast,"Opencast
Open Source Lecture Capture & Video Management for Education
Opencast is a free, open-source platform to support the management of
educational audio and video content. Institutions can use Opencast to
produce lecture recordings, manage existing video, serve designated
distribution channels, and provide user interfaces to engage students with
educational videos.
Installation
Installation instructions can be found locally at
docs/guides/admin/docs

or on our documentation server at:

Opencast Documentation

Community
More information about the community can be found at:

Opencast Website
Mailing lists, IRC, …
Issue Tracker

",batch2,8:09:15,Done
49,ttruongatl/fullcalendar,"FullCalendar 
A full-sized drag & drop event calendar (jQuery plugin).

Project website and demos
Documentation
Support
Contributing
Changelog
License

",batch2,8:10:17,Done
50,nelsonsbrian/testMud,"
ranvier
Node.js-based MUD engine
Ranvier is a MUD game engine whose goal is to be a simple but powerful way to build whatever MUD you want with special care given to extensibility. The core code strives to be completely unopinionated toward any specific style of game while using the bundle system to build the game you want without having to dig through the engine's code.
Special Features

Robust bundle system: Nearly every aspect of the game can be modified without changing the core and allows for easy
packaging and sharing of commands/areas/items/npcs/channels/behaviors
Unopinionated network layer: easily swap out telnet for any network layer you like. No need to gut the whole codebase
just to support a different transport type, just drop in a file.
Customizable data layer: You are not tied to saving in any particular database or file storage sytem
Optional coordinate based room system allowing for the flexibilty of a standard MUD world with the easy mappability of
a strict 3D world.
Scripting for all entities in the game for any event along with behaviors to create shared, composable scripts
Skill system with passive/active skills
Effects e.g., buffs/debuffs
Quest system allowing for starting/progress/completion from any event in the game
Communication channels with custom audiences

Documentation
Ranvier prides itself on having thorough documentation which is available on our website: ranviermud.com
Slack
We have a Slack channel you can use to ask questions, suggest features, or just keep up to date with the project: https://ranviermud.slack.com
Get an invite
Requirements

Node.js >= v10.12.0

Demo
Point your favorite client or telnet to ranviermud.com port 4000. This demo server is wiped and updated from the master branch every hour.
",batch2,8:10:18,Done
51,kitnil/nixpkgs,"






Nixpkgs is a collection of over
40,000 software packages that can be installed with the
Nix package manager. It also implements
NixOS, a purely-functional Linux distribution.
Manuals

NixOS Manual - how to install, configure, and maintain a purely-functional Linux distribution
Nixpkgs Manual - contributing to Nixpkgs and using programming-language-specific Nix expressions
Nix Package Manager Manual - how to write Nix expressions (programs), and how to use Nix command line tools

Community

Discourse Forum
IRC - #nixos on freenode.net
NixOS Weekly
Community-maintained wiki
Community-maintained list of ways to get in touch (Discord, Matrix, Telegram, other IRC channels, etc.)

Other Project Repositories
The sources of all official Nix-related projects are in the NixOS
organization on GitHub. Here are some of
the main ones:

Nix - the purely functional package manager
NixOps - the tool to remotely deploy NixOS machines
Nix RFCs - the formal process for making substantial changes to the community
NixOS homepage - the NixOS.org website
hydra - our continuous integration system
NixOS Artwork - NixOS artwork

Continuous Integration and Distribution
Nixpkgs and NixOS are built and tested by our continuous integration
system, Hydra.

Continuous package builds for unstable/master
Continuous package builds for the NixOS 20.03 release
Tests for unstable/master
Tests for the NixOS 20.03 release

Artifacts successfully built with Hydra are published to cache at
https://cache.nixos.org/. When successful build and test criteria are
met, the Nixpkgs expressions are distributed via Nix
channels.
Contributing
Nixpkgs is among the most active projects on GitHub. While thousands
of open issues and pull requests might seem a lot at first, it helps
consider it in the context of the scope of the project. Nixpkgs
describes how to build over 40,000 pieces of software and implements a
Linux distribution. The GitHub Insights
page gives a sense of the project activity.
Community contributions are always welcome through GitHub Issues and
Pull Requests. When pull requests are made, our tooling automation bot,
OfBorg will perform various checks
to help ensure expression quality.
The Nixpkgs maintainers are people who have assigned themselves to
maintain specific individual packages. We encourage people who care
about a package to assign themselves as a maintainer. When a pull
request is made against a package, OfBorg will notify the appropriate
maintainer(s). The Nixpkgs committers are people who have been given
permission to merge.
Most contributions are based on and merged into these branches:

master is the main branch where all small contributions go
staging is branched from master, changes that have a big impact on
Hydra builds go to this branch
staging-next is branched from staging and only fixes to stabilize
and security fixes with a big impact on Hydra builds should be
contributed to this branch. This branch is merged into master when
deemed of sufficiently high quality

For more information about contributing to the project, please visit
the contributing page.
Donations
The infrastructure for NixOS and related projects is maintained by a
nonprofit organization, the NixOS
Foundation. To ensure the
continuity and expansion of the NixOS infrastructure, we are looking
for donations to our organization.
You can donate to the NixOS foundation by using Open Collective:

License
Nixpkgs is licensed under the MIT License.
Note: MIT license does not apply to the packages built by Nixpkgs,
merely to the files in this repository (the Nix expressions, build
scripts, NixOS modules, etc.). It also might not apply to patches
included in Nixpkgs, which may be derivative works of the packages to
which they apply. The aforementioned artifacts are all covered by the
licenses of the respective packages.
",batch1,16:57:58,Done
52,yumingfei/virt-manager,,batch2,8:10:17,Done
53,mjshuff23/myMud,"
ranvier
Node.js-based MUD engine
Ranvier is a MUD game engine whose goal is to be a simple but powerful way to build whatever MUD you want with special care given to extensibility. The core code strives to be completely unopinionated toward any specific style of game while using the bundle system to build the game you want without having to dig through the engine's code.
Special Features

Robust bundle system: Nearly every aspect of the game can be modified without changing the core and allows for easy
packaging and sharing of commands/areas/items/npcs/channels/behaviors
Unopinionated network layer: easily swap out telnet for any network layer you like. No need to gut the whole codebase
just to support a different transport type, just drop in a file.
Customizable data layer: You are not tied to saving in any particular database or file storage sytem
Optional coordinate based room system allowing for the flexibilty of a standard MUD world with the easy mappability of
a strict 3D world.
Scripting for all entities in the game for any event along with behaviors to create shared, composable scripts
Skill system with passive/active skills
Effects e.g., buffs/debuffs
Quest system allowing for starting/progress/completion from any event in the game
Communication channels with custom audiences

Documentation
Ranvier prides itself on having thorough documentation which is available on our website: ranviermud.com
Slack
We have a Slack channel you can use to ask questions, suggest features, or just keep up to date with the project: https://ranviermud.slack.com
Get an invite
Requirements

Node.js >= v10.12.0

Demo
Point your favorite client or telnet to ranviermud.com port 4000. This demo server is wiped and updated from the master branch every hour.
",batch2,8:10:17,Done
54,pjones/emacs,,batch1,16:58:59,Done
55,skzitz/emacs,,batch1,16:59:01,Done
56,qingqibing/libcxx,,batch2,8:09:15,Done
57,NixOS/nixpkgs-channels,"DEPRECATED
Use NixOS/nixpkgs instead of NixOS/nixpkgs-channels going forward.
For more information see https://github.com/NixOS/nixpkgs/issues/99257
",batch1,16:57:57,Done
58,MariaDB/server,"Code status:

 travis-ci.org (10.5 branch)
 ci.appveyor.com

MariaDB: The open source relational database
MariaDB was designed as a drop-in replacement of MySQL(R) with more
features, new storage engines, fewer bugs, and better performance.
MariaDB is brought to you by the MariaDB Foundation and the MariaDB Corporation.
Please read the CREDITS file for details about the MariaDB Foundation,
and who is developing MariaDB.
MariaDB is developed by many of the original developers of MySQL who
now work for the MariaDB Corporation, the MariaDB Foundation and by
many people in the community.
MySQL, which is the base of MariaDB, is a product and trademark of Oracle
Corporation, Inc. For a list of developers and other contributors,
see the Credits appendix.  You can also run 'SHOW authors' to get a
list of active contributors.
A description of the MariaDB project and a manual can be found at:
https://mariadb.org
https://mariadb.com/kb/en/
https://mariadb.com/kb/en/mariadb-vs-mysql-features/
https://mariadb.com/kb/en/mariadb-versus-mysql-compatibility/
https://mariadb.com/kb/en/new-and-old-releases/
Help
More help is available from the Maria Discuss mailing list
https://launchpad.net/~maria-discuss, MariaDB's Zulip
instance, https://mariadb.zulipchat.com/
and the #maria IRC channel on Freenode.
Live QA for beginner contributors
MariaDB has a dedicated time each week when we answer new contributor questions live on Zulip and IRC.
From 8:00 to 10:00 UTC on Mondays, and 10:00 to 12:00 UTC on Thursdays,
anyone can ask any questions they’d like, and a live developer will be available to assist.
New contributors can ask questions any time, but we will provide immediate feedback during that interval.
Licensing

NOTE:
MariaDB is specifically available only under version 2 of the GNU
General Public License (GPLv2). (I.e. Without the ""any later version""
clause.) This is inherited from MySQL. Please see the README file in
the MySQL distribution for more information.
License information can be found in the COPYING file. Third party
license information can be found in the THIRDPARTY file.

Bug Reports
Bug and/or error reports regarding MariaDB should be submitted at:
https://jira.mariadb.org
For reporting security vulnerabilities see:
https://mariadb.org/about/security-policy/
The code for MariaDB, including all revision history, can be found at:
https://github.com/MariaDB/server

",batch1,16:57:58,Done
59,ddcc/llvm-project,"The LLVM Compiler Infrastructure
This directory and its sub-directories contain source code for LLVM,
a toolkit for the construction of highly optimized compilers,
optimizers, and run-time environments.
The README briefly describes how to get started with building LLVM.
For more information on how to contribute to the LLVM project, please
take a look at the
Contributing to LLVM guide.
Getting Started with the LLVM System
Taken from https://llvm.org/docs/GettingStarted.html.
Overview
Welcome to the LLVM project!
The LLVM project has multiple components. The core of the project is
itself called ""LLVM"". This contains all of the tools, libraries, and header
files needed to process intermediate representations and converts it into
object files.  Tools include an assembler, disassembler, bitcode analyzer, and
bitcode optimizer.  It also contains basic regression tests.
C-like languages use the Clang front end.  This
component compiles C, C++, Objective-C, and Objective-C++ code into LLVM bitcode
-- and from there into object files, using LLVM.
Other components include:
the libc++ C++ standard library,
the LLD linker, and more.
Getting the Source Code and Building LLVM
The LLVM Getting Started documentation may be out of date.  The Clang
Getting Started page might have more
accurate information.
This is an example work-flow and configuration to get and build the LLVM source:


Checkout LLVM (including related sub-projects like Clang):


git clone https://github.com/llvm/llvm-project.git


Or, on windows, git clone --config core.autocrlf=false  https://github.com/llvm/llvm-project.git




Configure and build LLVM and Clang:


cd llvm-project


mkdir build


cd build


cmake -G <generator> [options] ../llvm
Some common build system generators are:

Ninja --- for generating Ninja
build files. Most llvm developers use Ninja.
Unix Makefiles --- for generating make-compatible parallel makefiles.
Visual Studio --- for generating Visual Studio projects and
solutions.
Xcode --- for generating Xcode projects.

Some Common options:


-DLLVM_ENABLE_PROJECTS='...' --- semicolon-separated list of the LLVM
sub-projects you'd like to additionally build. Can include any of: clang,
clang-tools-extra, libcxx, libcxxabi, libunwind, lldb, compiler-rt, lld,
polly, or debuginfo-tests.
For example, to build LLVM, Clang, libcxx, and libcxxabi, use
-DLLVM_ENABLE_PROJECTS=""clang;libcxx;libcxxabi"".


-DCMAKE_INSTALL_PREFIX=directory --- Specify for directory the full
path name of where you want the LLVM tools and libraries to be installed
(default /usr/local).


-DCMAKE_BUILD_TYPE=type --- Valid options for type are Debug,
Release, RelWithDebInfo, and MinSizeRel. Default is Debug.


-DLLVM_ENABLE_ASSERTIONS=On --- Compile with assertion checks enabled
(default is Yes for Debug builds, No for all other build types).




cmake --build . [-- [options] <target>] or your build system specified above
directly.


The default target (i.e. ninja or make) will build all of LLVM.


The check-all target (i.e. ninja check-all) will run the
regression tests to ensure everything is in working order.


CMake will generate targets for each tool and library, and most
LLVM sub-projects generate their own check-<project> target.


Running a serial build will be slow.  To improve speed, try running a
parallel build.  That's done by default in Ninja; for make, use the option
-j NNN, where NNN is the number of parallel jobs, e.g. the number of
CPUs you have.




For more information see CMake




Consult the
Getting Started with LLVM
page for detailed information on configuring and compiling LLVM. You can visit
Directory Layout
to learn about the layout of the source code tree.
",batch1,16:57:58,Done
60,chisuhua/llvm,,batch1,16:57:58,Done
61,clear39/avcode-libx264,"avcode-libx264
源码路径：git remote set-url origin git://git.videolan.org/x264.git
libx264
git clone git://git.videolan.org/x264.git libx264
网站：http://www.videolan.org/developers/x264.html
",batch2,8:09:16,Done
62,alanszp/test,"




















TypeORM is an ORM
that can run in NodeJS, Browser, Cordova, PhoneGap, Ionic, React Native and Electron platforms
and can be used with TypeScript and JavaScript (ES5, ES6, ES7).
Its goal is to always support the latest JavaScript features and provide additional features
that help you to develop any kind of application that uses databases - from
small applications with a few tables to large scale enterprise applications
with multiple databases.
TypeORM supports both Active Record and Data Mapper patterns,
unlike all other JavaScript ORMs currently in existance,
which means you can write high quality, loosely coupled, scalable,
maintainable applications the most productive way.
TypeORM is highly influenced by other ORMs, such as Hibernate,
Doctrine and Entity Framework.
Some TypeORM features:

supports both DataMapper and ActiveRecord (your choice)
entities and columns
database-specific column types
entity manager
repositories and custom repositories
clean object relational model
associations (relations)
eager and lazy relations
uni-directional, bi-directional and self-referenced relations
supports multiple inheritance patterns
cascades
indices
transactions
migrations and automatic migrations generation
connection pooling
replication
using multiple database connections
working with multiple databases types
cross-database and cross-schema queries
elegant-syntax, flexible and powerful QueryBuilder
left and inner joins
proper pagination for queries using joins
query caching
streaming raw results
logging
listeners and subscribers (hooks)
supports closure table pattern
schema declaration in models or separate configuration files
connection configuration in json / xml / yml / env formats
supports MySQL / MariaDB / Postgres / SQLite / Microsoft SQL Server / Oracle / sql.js
supports MongoDB NoSQL database
works in NodeJS / Browser / Ionic / Cordova / React Native / Electron platforms
TypeScript and JavaScript support
produced code is performant, flexible, clean and maintainable
follows all possible best practices
CLI

And more...
With TypeORM your models look like this:
import {Entity, PrimaryGeneratedColumn, Column} from ""typeorm"";

@Entity()
export class User {

    @PrimaryGeneratedColumn()
    id: number;

    @Column()
    firstName: string;

    @Column()
    lastName: string;

    @Column()
    age: number;

}
And your domain logic looks like this:
const user = new User();
user.firstName = ""Timber"";
user.lastName = ""Saw"";
user.age = 25;
await repository.save(user);

const allUsers = await repository.find();
const firstUser = await repository.findOne(1); // find by id
const timber = await repository.findOne({ firstName: ""Timber"", lastName: ""Saw"" });

await repository.remove(timber);
Alternatively, if you prefer to use the ActiveRecord implementation, you can use it as well:
import {Entity, PrimaryGeneratedColumn, Column, BaseEntity} from ""typeorm"";

@Entity()
export class User extends BaseEntity {

    @PrimaryGeneratedColumn()
    id: number;

    @Column()
    firstName: string;

    @Column()
    lastName: string;

    @Column()
    age: number;

}
And your domain logic will look this way:
const user = new User();
user.firstName = ""Timber"";
user.lastName = ""Saw"";
user.age = 25;
await user.save();

const allUsers = await User.find();
const firstUser = await User.findOne(1);
const timber = await User.findOne({ firstName: ""Timber"", lastName: ""Saw"" });

await timber.remove();
Installation


Install the npm package:
npm install typeorm --save


You need to install reflect-metadata shim:
npm install reflect-metadata --save
and import it somewhere in the global place of your app (for example in app.ts):
import ""reflect-metadata"";


You may need to install node typings:
npm install @types/node --save


Install a database driver:


for MySQL or MariaDB
npm install mysql --save (you can install mysql2 instead as well)


for PostgreSQL
npm install pg --save


for SQLite
npm install sqlite3 --save


for Microsoft SQL Server
npm install mssql --save


for sql.js
npm install sql.js --save


for Oracle (experimental)
npm install oracledb --save


Install only one of them, depending on which database you use.
To make the Oracle driver work, you need to follow the installation instructions from
their site.
Oracle support is experimental at the moment and isn't bug-free.
Expect to see more stable Oracle support in the near future.


TypeScript configuration
Also, make sure you are using TypeScript compiler version 2.3 or greater,
and you have enabled the following settings in tsconfig.json:
""emitDecoratorMetadata"": true,
""experimentalDecorators"": true,
You may also need to enable es6 in the lib section of compiler options, or install es6-shim from @types.
Quick Start
The quickest way to get started with TypeORM is to use its CLI commands to generate a starter project.
Quick start works only if you are using TypeORM in a NodeJS application.
If you are using other platforms, proceed to the step-by-step guide.
First, install TypeORM globally:
npm install typeorm -g

Then go to the directory where you want to create a new project and run the command:
typeorm init --name MyProject --database mysql

Where name is the name of your project and database is the database you'll use.
Database can be one of the following values: mysql, mariadb, postgres, sqlite, mssql, oracle, mongodb, cordova, react-native.
This command will generate a new project in the MyProject directory with the following files:
MyProject
├── src              // place of your TypeScript code
│   ├── entity       // place where your entities (database models) are stored
│   │   └── User.ts  // sample entity
│   ├── migration    // place where your migrations are stored
│   └── index.ts     // start point of your application
├── .gitignore       // standard gitignore file
├── ormconfig.json   // ORM and database connection configuration
├── package.json     // node module dependencies
├── README.md        // simple readme file
└── tsconfig.json    // TypeScript compiler options


You can also run typeorm init on an existing node project, but be careful - it may override some files you already have.

The next step is to install new project dependencies:
cd MyProject
npm install

While installation is in progress, edit the ormconfig.json file and put your own database connection configuration options in there:
{
   ""type"": ""mysql"",
   ""host"": ""localhost"",
   ""port"": 3306,
   ""username"": ""test"",
   ""password"": ""test"",
   ""database"": ""test"",
   ""synchronize"": true,
   ""logging"": false,
   ""entities"": [
      ""src/entity/**/*.ts""
   ],
   ""migrations"": [
      ""src/migration/**/*.ts""
   ],
   ""subscribers"": [
      ""src/subscriber/**/*.ts""
   ]
}
Particularly, most of the time you'll only need to configure
host, username, password, database and maybe port options.
Once you finish with configuration and all node modules are installed, you can run your application:
npm start

That's it, your application should successfully run and insert a new user into the database.
You can continue to work with this project and integrate other modules you need and start
creating more entities.

You can generate an even more advanced project with express installed by running
typeorm init --name MyProject --database mysql --express command.

Step-by-Step Guide
What are you expecting from ORM?
First of all, you are expecting it will create database tables for you
and find / insert / update / delete your data without the pain of
having to write lots of hardly maintainable SQL queries.
This guide will show you how to setup TypeORM from scratch and make it do what you are expecting from an ORM.
Create a model
Working with a database starts from creating tables.
How do you tell TypeORM to create a database table?
The answer is - through the models.
Your models in your app are your database tables.
For example, you have a Photo model:
export class Photo {
    id: number;
    name: string;
    description: string;
    filename: string;
    views: number;
}
And you want to store photos in your database.
To store things in the database, first you need a database table,
and database tables are created from your models.
Not all models, but only those you define as entities.
Create an entity
Entity is your model decorated by an @Entity decorator.
A database table will be created for such models.
You work with entities everywhere with TypeORM.
You can load/insert/update/remove and perform other operations with them.
Let's make our Photo model as an entity:
import {Entity} from ""typeorm"";

@Entity()
export class Photo {
    id: number;
    name: string;
    description: string;
    filename: string;
    views: number;
    isPublished: boolean;
}
Now, a database table will be created for the Photo entity and we'll be able to work with it anywhere in our app.
We have created a database table, however what table can exist without columns?
Let's create a few columns in our database table.
Adding table columns
To add database columns, you simply need to decorate an entity's properties you want to make into a column
with a @Column decorator.
import {Entity, Column} from ""typeorm"";

@Entity()
export class Photo {

    @Column()
    id: number;

    @Column()
    name: string;

    @Column()
    description: string;

    @Column()
    filename: string;

    @Column()
    views: number;

    @Column()
    isPublished: boolean;
}
Now id, name, description, filename, views and isPublished columns will be added to the photo table.
Column types in the database are inferred from the property types you used, e.g.
number will be converted into integer, string into varchar, boolean into bool, etc.
But you can use any column type your database supports by implicitly specifying a column type into the @Column decorator.
We generated a database table with columns, but there is one thing left.
Each database table must have a column with a primary key.
Creating a primary column
Each entity must have at least one primary key column.
This is a requirement and you can't avoid it.
To make a column a primary key, you need to use @PrimaryColumn decorator.
import {Entity, Column, PrimaryColumn} from ""typeorm"";

@Entity()
export class Photo {

    @PrimaryColumn()
    id: number;

    @Column()
    name: string;

    @Column()
    description: string;

    @Column()
    filename: string;

    @Column()
    views: number;

    @Column()
    isPublished: boolean;
}
Creating an auto generated column
Now, let's say you want your id column to be auto-generated (this is known as auto-increment / sequence / serial / generated identity column).
To do that, you need to change the @PrimaryColumn decorator to a @PrimaryGeneratedColumn decorator:
import {Entity, Column, PrimaryGeneratedColumn} from ""typeorm"";

@Entity()
export class Photo {

    @PrimaryGeneratedColumn()
    id: number;

    @Column()
    name: string;

    @Column()
    description: string;

    @Column()
    filename: string;

    @Column()
    views: number;

    @Column()
    isPublished: boolean;
}
Column data types
Next, let's fix our data types. By default, string is mapped to a varchar(255)-like type (depending on the database type).
Number is mapped to a integer-like type (depending on the database type).
We don't want all our columns to be limited varchars or integers.
Let's setup correct data types:
import {Entity, Column, PrimaryGeneratedColumn} from ""typeorm"";

@Entity()
export class Photo {

    @PrimaryGeneratedColumn()
    id: number;

    @Column({
        length: 100
    })
    name: string;

    @Column(""text"")
    description: string;

    @Column()
    filename: string;

    @Column(""double"")
    views: number;

    @Column()
    isPublished: boolean;
}
Column types are database-specific.
You can set any column type your database supports.
More information on supported column types can be found here.
Creating a connection to the database
Now, when our entity is created, let's create an index.ts (or app.ts whatever you call it) file and set up our connection there:
import ""reflect-metadata"";
import {createConnection} from ""typeorm"";
import {Photo} from ""./entity/Photo"";

createConnection({
    type: ""mysql"",
    host: ""localhost"",
    port: 3306,
    username: ""root"",
    password: ""admin"",
    database: ""test"",
    entities: [
        Photo
    ],
    synchronize: true,
    logging: false
}).then(connection => {
    // here you can start to work with your entities
}).catch(error => console.log(error));
We are using MySQL in this example, but you can use any other supported database.
To use another database, simply change the type in the options to the database type you are using:
mysql, mariadb, postgres, sqlite, mssql, oracle, cordova, react-native or mongodb.
Also make sure to use your own host, port, username, password and database settings.
We added our Photo entity to the list of entities for this connection.
Each entity you are using in your connection must be listed there.
Setting synchronize makes sure your entities will be synced with the database, every time you run the application.
Loading all entities from the directory
Later, when we create more entities we need to add them to the entities in our configuration.
This is not very convenient, so instead we can set up the whole directory, from where all entities will be connected and used in our connection:
import {createConnection} from ""typeorm"";

createConnection({
    type: ""mysql"",
    host: ""localhost"",
    port: 3306,
    username: ""root"",
    password: ""admin"",
    database: ""test"",
    entities: [
        __dirname + ""/entity/*.js""
    ],
    synchronize: true,
}).then(connection => {
    // here you can start to work with your entities
}).catch(error => console.log(error));
But be careful with this approach.
If you are using ts-node then you need to specify paths to .ts files instead.
If you are using outDir then you'll need to specify paths to .js files inside outDir directory.
If you are using outDir and when you remove or rename your entities make sure to clear outDir directory
and re-compile your project again, because when you remove your source .ts files their compiled .js versions
aren't removed from output directory and still are loaded by TypeORM because they present in outDir directory.
Running the application
Now if you run your index.ts, a connection with database will be initialized and a database table for your photos will be created.
+-------------+--------------+----------------------------+
|                         photo                           |
+-------------+--------------+----------------------------+
| id          | int(11)      | PRIMARY KEY AUTO_INCREMENT |
| name        | varchar(500) |                            |
| description | text         |                            |
| filename    | varchar(255) |                            |
| views       | int(11)      |                            |
| isPublished | boolean      |                            |
+-------------+--------------+----------------------------+
Creating and inserting a photo into the database
Now let's create a new photo to save it in the database:
import {createConnection} from ""typeorm"";
import {Photo} from ""./entity/Photo"";

createConnection(/*...*/).then(connection => {

    let photo = new Photo();
    photo.name = ""Me and Bears"";
    photo.description = ""I am near polar bears"";
    photo.filename = ""photo-with-bears.jpg"";
    photo.views = 1;
    photo.isPublished = true;

    return connection.manager
            .save(photo)
            .then(photo => {
                console.log(""Photo has been saved. Photo id is"", photo.id);
            });

}).catch(error => console.log(error));
Once your entity is saved it will get a newly generated id.
save method returns an instance of the same object you pass to it.
It's not a new copy of the object, it modifies its ""id"" and returns it.
Using async/await syntax
Let's take advantage of the latest ES7 features and use async/await syntax instead:
import {createConnection} from ""typeorm"";
import {Photo} from ""./entity/Photo"";

createConnection(/*...*/).then(async connection => {

    let photo = new Photo();
    photo.name = ""Me and Bears"";
    photo.description = ""I am near polar bears"";
    photo.filename = ""photo-with-bears.jpg"";
    photo.views = 1;
    photo.isPublished = true;

    await connection.manager.save(photo);
    console.log(""Photo has been saved"");

}).catch(error => console.log(error));
Using Entity Manager
We just created a new photo and saved it in the database.
We used EntityManager to save it.
Using entity manager you can manipulate any entity in your app.
For example, let's load our saved entity:
import {createConnection} from ""typeorm"";
import {Photo} from ""./entity/Photo"";

createConnection(/*...*/).then(async connection => {

    /*...*/
    let savedPhotos = await connection.manager.find(Photo);
    console.log(""All photos from the db: "", savedPhotos);

}).catch(error => console.log(error));
savedPhotos will be an array of Photo objects with the data loaded from the database.
Learn more about EntityManager here.
Using Repositories
Now let's refactor our code and use Repository instead of EntityManager.
Each entity has its own repository which handles all operations with its entity.
When you deal with entities a lot, Repositories are more convenient to use than EntityManagers:
import {createConnection} from ""typeorm"";
import {Photo} from ""./entity/Photo"";

createConnection(/*...*/).then(async connection => {

    let photo = new Photo();
    photo.name = ""Me and Bears"";
    photo.description = ""I am near polar bears"";
    photo.filename = ""photo-with-bears.jpg"";
    photo.views = 1;
    photo.isPublished = true;

    let photoRepository = connection.getRepository(Photo);

    await photoRepository.save(photo);
    console.log(""Photo has been saved"");

    let savedPhotos = await photoRepository.find();
    console.log(""All photos from the db: "", savedPhotos);

}).catch(error => console.log(error));
Learn more about Repository here.
Loading from the database
Let's try more load operations using the Repository:
import {createConnection} from ""typeorm"";
import {Photo} from ""./entity/Photo"";

createConnection(/*...*/).then(async connection => {

    /*...*/
    let allPhotos = await photoRepository.find();
    console.log(""All photos from the db: "", allPhotos);

    let firstPhoto = await photoRepository.findOne(1);
    console.log(""First photo from the db: "", firstPhoto);

    let meAndBearsPhoto = await photoRepository.findOne({ name: ""Me and Bears"" });
    console.log(""Me and Bears photo from the db: "", meAndBearsPhoto);

    let allViewedPhotos = await photoRepository.find({ views: 1 });
    console.log(""All viewed photos: "", allViewedPhotos);

    let allPublishedPhotos = await photoRepository.find({ isPublished: true });
    console.log(""All published photos: "", allPublishedPhotos);

    let [allPhotos, photosCount] = await photoRepository.findAndCount();
    console.log(""All photos: "", allPhotos);
    console.log(""Photos count: "", photosCount);

}).catch(error => console.log(error));
Updating in the database
Now let's load a single photo from the database, update it and save it:
import {createConnection} from ""typeorm"";
import {Photo} from ""./entity/Photo"";

createConnection(/*...*/).then(async connection => {

    /*...*/
    let photoToUpdate = await photoRepository.findOne(1);
    photoToUpdate.name = ""Me, my friends and polar bears"";
    await photoRepository.save(photoToUpdate);

}).catch(error => console.log(error));
Now photo with id = 1 will be updated in the database.
Removing from the database
Now let's remove our photo from the database:
import {createConnection} from ""typeorm"";
import {Photo} from ""./entity/Photo"";

createConnection(/*...*/).then(async connection => {

    /*...*/
    let photoToRemove = await photoRepository.findOne(1);
    await photoRepository.remove(photoToRemove);

}).catch(error => console.log(error));
Now photo with id = 1 will be removed from the database.
Creating a one-to-one relation
Let's create a one-to-one relation with another class.
Let's create a new class in PhotoMetadata.ts. This PhotoMetadata class is supposed to contain our photo's additional meta-information:
import {Entity, Column, PrimaryGeneratedColumn, OneToOne, JoinColumn} from ""typeorm"";
import {Photo} from ""./Photo"";

@Entity()
export class PhotoMetadata {

    @PrimaryGeneratedColumn()
    id: number;

    @Column(""int"")
    height: number;

    @Column(""int"")
    width: number;

    @Column()
    orientation: string;

    @Column()
    compressed: boolean;

    @Column()
    comment: string;

    @OneToOne(type => Photo)
    @JoinColumn()
    photo: Photo;
}
Here, we are using a new decorator called @OneToOne. It allows us to create a one-to-one relationship between two entities.
type => Photo is a function that returns the class of the entity with which we want to make our relationship.
We are forced to use a function that returns a class, instead of using the class directly, because of the language specifics.
We can also write it as () => Photo, but we use type => Photo as a convention to increase code readability.
The type variable itself does not contain anything.
We also add a @JoinColumn decorator, which indicates that this side of the relationship will own the relationship.
Relations can be unidirectional or bidirectional.
Only one side of relational can be owning.
Using @JoinColumn decorator is required on the owner side of the relationship.
If you run the app, you'll see a newly generated table, and it will contain a column with a foreign key for the photo relation:
+-------------+--------------+----------------------------+
|                     photo_metadata                      |
+-------------+--------------+----------------------------+
| id          | int(11)      | PRIMARY KEY AUTO_INCREMENT |
| height      | int(11)      |                            |
| width       | int(11)      |                            |
| comment     | varchar(255) |                            |
| compressed  | boolean      |                            |
| orientation | varchar(255) |                            |
| photoId     | int(11)      | FOREIGN KEY                |
+-------------+--------------+----------------------------+
Save a one-to-one relation
Now let's save a photo, its metadata and attach them to each other.
import {createConnection} from ""typeorm"";
import {Photo} from ""./entity/Photo"";
import {PhotoMetadata} from ""./entity/PhotoMetadata"";

createConnection(/*...*/).then(async connection => {

    // create a photo
    let photo = new Photo();
    photo.name = ""Me and Bears"";
    photo.description = ""I am near polar bears"";
    photo.filename = ""photo-with-bears.jpg"";
    photo.isPublished = true;

    // create a photo metadata
    let metadata = new PhotoMetadata();
    metadata.height = 640;
    metadata.width = 480;
    metadata.compressed = true;
    metadata.comment = ""cybershoot"";
    metadata.orientation = ""portait"";
    metadata.photo = photo; // this way we connect them

    // get entity repositories
    let photoRepository = connection.getRepository(Photo);
    let metadataRepository = connection.getRepository(PhotoMetadata);

    // first we should save a photo
    await photoRepository.save(photo);

    // photo is saved. Now we need to save a photo metadata
    await metadataRepository.save(metadata);

    // done
    console.log(""Metadata is saved, and relation between metadata and photo is created in the database too"");

}).catch(error => console.log(error));
Inverse side of the relationship
Relations can be unidirectional or bidirectional.
Currently, our relation between PhotoMetadata and Photo is unidirectional.
The owner of the relation is PhotoMetadata, and Photo doesn't know anything about PhotoMetadata.
This makes it complicated to access PhotoMetadata from the Photo side.
To fix this issue we should add an inverse relation, and make relations between PhotoMetadata and Photo bidirectional.
Let's modify our entities:
import {Entity, Column, PrimaryGeneratedColumn, OneToOne, JoinColumn} from ""typeorm"";
import {Photo} from ""./Photo"";

@Entity()
export class PhotoMetadata {

    /* ... other columns */

    @OneToOne(type => Photo, photo => photo.metadata)
    @JoinColumn()
    photo: Photo;
}
import {Entity, Column, PrimaryGeneratedColumn, OneToOne} from ""typeorm"";
import {PhotoMetadata} from ""./PhotoMetadata"";

@Entity()
export class Photo {

    /* ... other columns */

    @OneToOne(type => PhotoMetadata, photoMetadata => photoMetadata.photo)
    metadata: PhotoMetadata;
}
photo => photo.metadata is a function that returns the name of the inverse side of the relation.
Here we show that the metadata property of the Photo class is where we store PhotoMetadata in the Photo class.
Instead of passing a function that returns a property of the photo, you could alternatively simply pass a string to @OneToOne decorator, like ""metadata"".
But we used this function-typed approach to make our refactoring easier.
Note that we should use @JoinColumn decorator only on one side of a relation.
Whichever side you put this decorator on will be the owning side of the relationship.
The owning side of a relationship contains a column with a foreign key in the database.
Loading objects with their relations
Now let's load our photo and its photo metadata in a single query.
There are two ways to do it - using find* methods or using QueryBuilder functionality.
Let's use find* methods first.
find* methods allow you to specify an object with the FindOneOptions / FindManyOptions interface.
import {createConnection} from ""typeorm"";
import {Photo} from ""./entity/Photo"";
import {PhotoMetadata} from ""./entity/PhotoMetadata"";

createConnection(/*...*/).then(async connection => {

    /*...*/
    let photoRepository = connection.getRepository(Photo);
    let photos = await photoRepository.find({ relations: [""metadata""] });

}).catch(error => console.log(error));
Here, photos will contain an array of photos from the database, and each photo will contain its photo metadata.
Learn more about Find Options in this documentation.
Using find options is good and dead simple, but if you need a more complex query, you should use QueryBuilder instead.
QueryBuilder allows more complex queries to be used in an elegant way:
import {createConnection} from ""typeorm"";
import {Photo} from ""./entity/Photo"";
import {PhotoMetadata} from ""./entity/PhotoMetadata"";

createConnection(/*...*/).then(async connection => {

    /*...*/
    let photos = await connection
            .getRepository(Photo)
            .createQueryBuilder(""photo"")
            .innerJoinAndSelect(""photo.metadata"", ""metadata"")
            .getMany();


}).catch(error => console.log(error));
QueryBuilder allows creation and execution of SQL queries of almost any complexity.
When you work with QueryBuilder, think like you are creating an SQL query.
In this example, ""photo"" and ""metadata"" are aliases applied to selected photos.
You use aliases to access columns and properties of the selected data.
Using cascades to automatically save related objects
We can setup cascade options in our relations, in the cases when we want our related object to be saved whenever the other object is saved.
Let's change our photo's @OneToOne decorator a bit:
export class Photo {
    /// ... other columns

    @OneToOne(type => PhotoMetadata, metadata => metadata.photo, {
        cascade: true,
    })
    metadata: PhotoMetadata;
}
Using cascade allows us not to separately save photo and separately save metadata objects now.
Now we can simply save a photo object, and the metadata object will be saved automatically because of cascade options.
createConnection(options).then(async connection => {

    // create photo object
    let photo = new Photo();
    photo.name = ""Me and Bears"";
    photo.description = ""I am near polar bears"";
    photo.filename = ""photo-with-bears.jpg"";
    photo.isPublished = true;

    // create photo metadata object
    let metadata = new PhotoMetadata();
    metadata.height = 640;
    metadata.width = 480;
    metadata.compressed = true;
    metadata.comment = ""cybershoot"";
    metadata.orientation = ""portait"";
    
    photo.metadata = metadata; // this way we connect them

    // get repository
    let photoRepository = connection.getRepository(Photo);

    // saving a photo also save the metadata
    await photoRepository.save(photo);

    console.log(""Photo is saved, photo metadata is saved too."")

}).catch(error => console.log(error));
Creating a many-to-one / one-to-many relation
Let's create a many-to-one / one-to-many relation.
Let's say a photo has one author, and each author can have many photos.
First, let's create an Author class:
import {Entity, Column, PrimaryGeneratedColumn, OneToMany, JoinColumn} from ""typeorm"";
import {Photo} from ""./Photo"";

@Entity()
export class Author {

    @PrimaryGeneratedColumn()
    id: number;

    @Column()
    name: string;

    @OneToMany(type => Photo, photo => photo.author) // note: we will create author property in the Photo class below
    photos: Photo[];
}
Author contains an inverse side of a relation.
OneToMany is always an inverse side of relation, and it can't exist without ManyToOne on the other side of the relation.
Now let's add the owner side of the relation into the Photo entity:
import {Entity, Column, PrimaryGeneratedColumn, ManyToOne} from ""typeorm"";
import {PhotoMetadata} from ""./PhotoMetadata"";
import {Author} from ""./Author"";

@Entity()
export class Photo {

    /* ... other columns */

    @ManyToOne(type => Author, author => author.photos)
    author: Author;
}
In many-to-one / one-to-many relation, the owner side is always many-to-one.
It means that the class that uses @ManyToOne will store the id of the related object.
After you run the application, the ORM will create the author table:
+-------------+--------------+----------------------------+
|                          author                         |
+-------------+--------------+----------------------------+
| id          | int(11)      | PRIMARY KEY AUTO_INCREMENT |
| name        | varchar(255) |                            |
+-------------+--------------+----------------------------+
It will also modify the photo table, adding a new author column and creating a foreign key for it:
+-------------+--------------+----------------------------+
|                         photo                           |
+-------------+--------------+----------------------------+
| id          | int(11)      | PRIMARY KEY AUTO_INCREMENT |
| name        | varchar(255) |                            |
| description | varchar(255) |                            |
| filename    | varchar(255) |                            |
| isPublished | boolean      |                            |
| authorId    | int(11)      | FOREIGN KEY                |
+-------------+--------------+----------------------------+
Creating a many-to-many relation
Let's create a many-to-one / many-to-many relation.
Let's say a photo can be in many albums, and each album can contain many photos.
Let's create an Album class:
import {Entity, PrimaryGeneratedColumn, Column, ManyToMany, JoinTable} from ""typeorm"";

@Entity()
export class Album {

    @PrimaryGeneratedColumn()
    id: number;

    @Column()
    name: string;

    @ManyToMany(type => Photo, photo => photo.albums)
    @JoinTable()
    photos: Photo[];
}
@JoinTable is required to specify that this is the owner side of the relationship.
Now let's add the inverse side of our relation to the Photo class:
export class Photo {
    /// ... other columns

    @ManyToMany(type => Album, album => album.photos)
    albums: Album[];
}
After you run the application, the ORM will create a album_photos_photo_albums junction table:
+-------------+--------------+----------------------------+
|                album_photos_photo_albums                |
+-------------+--------------+----------------------------+
| album_id    | int(11)      | PRIMARY KEY FOREIGN KEY    |
| photo_id    | int(11)      | PRIMARY KEY FOREIGN KEY    |
+-------------+--------------+----------------------------+
Don't forget to register the Album class with your connection in the ORM:
const options: ConnectionOptions = {
    // ... other options
    entities: [Photo, PhotoMetadata, Author, Album]
};
Now let's insert albums and photos to our database:
let connection = await createConnection(options);

// create a few albums
let album1 = new Album();
album1.name = ""Bears"";
await connection.manager.save(album1);

let album2 = new Album();
album2.name = ""Me"";
await connection.manager.save(album2);

// create a few photos
let photo = new Photo();
photo.name = ""Me and Bears"";
photo.description = ""I am near polar bears"";
photo.filename = ""photo-with-bears.jpg"";
photo.albums = [album1, album2];
await connection.manager.save(photo);

// now our photo is saved and albums are attached to it
// now lets load them:
const loadedPhoto = await connection
    .getRepository(Photo)
    .findOne(1, { relations: [""albums""] });
loadedPhoto will be equal to:
{
    id: 1,
    name: ""Me and Bears"",
    description: ""I am near polar bears"",
    filename: ""photo-with-bears.jpg"",
    albums: [{
        id: 1,
        name: ""Bears""
    }, {
        id: 2,
        name: ""Me""
    }]
}
Using QueryBuilder
You can use QueryBuilder to build SQL queries of almost any complexity. For example, you can do this:
let photos = await connection
    .getRepository(Photo)
    .createQueryBuilder(""photo"") // first argument is an alias. Alias is what you are selecting - photos. You must specify it.
    .innerJoinAndSelect(""photo.metadata"", ""metadata"")
    .leftJoinAndSelect(""photo.albums"", ""album"")
    .where(""photo.isPublished = true"")
    .andWhere(""(photo.name = :photoName OR photo.name = :bearName)"")
    .orderBy(""photo.id"", ""DESC"")
    .skip(5)
    .take(10)
    .setParameters({ photoName: ""My"", bearName: ""Mishka"" })
    .getMany();
This query selects all published photos with ""My"" or ""Mishka"" names.
It will select results from position 5 (pagination offset),
and will select only 10 results (pagination limit).
The selection result will be ordered by id in descending order.
The photo's albums will be left-joined and their metadata will be inner joined.
You'll use the query builder in your application a lot.
Learn more about QueryBuilder here.
Samples
Take a look at the samples in sample for examples of usage.
There are a few repositories which you can clone and start with:

Example how to use TypeORM with TypeScript
Example how to use TypeORM with JavaScript
Example how to use TypeORM with JavaScript and Babel
Example how to use TypeORM with TypeScript and SystemJS in Browser
Example how to use Express and TypeORM
Example how to use Koa and TypeORM
Example how to use TypeORM with MongoDB
Example how to use TypeORM in a Cordova/PhoneGap app
Example how to use TypeORM with an Ionic app
Example how to use TypeORM with React Native
Example how to use TypeORM with Electron using JavaScript
Example how to use TypeORM with Electron using TypeScript

Extensions
There are several extensions that simplify working with TypeORM and integrating it with other modules:

TypeORM + GraphQL framework
TypeORM integration with TypeDI
TypeORM integration with routing-controllers
Models generation from existing database - typeorm-model-generator

Contributing 😰
Learn about contribution here and how to setup your development environment here.
This project exists thanks to all the people who contribute:

Backers 🙏
Thank you to all our backers! If you want to support a project and become a backer click here.

Sponsors 🤑
Support this project by becoming a sponsor. Your logo will show up here with a link to your website. Become a sponsor










",batch2,8:09:16,Done
63,BartElliott/cs188-case-study,,batch1,16:59:01,Done
64,NicolasPetton/emacs,,batch1,16:59:02,Done
65,addb-swstarlab/ParallelAOF,"This README is just a fast quick start document. You can find more detailed documentation at redis.io.
What is Redis?
Redis is often referred as a data structures server. What this means is that Redis provides access to mutable data structures via a set of commands, which are sent using a server-client model with TCP sockets and a simple protocol. So different processes can query and modify the same data structures in a shared way.
Data structures implemented into Redis have a few special properties:

Redis cares to store them on disk, even if they are always served and modified into the server memory. This means that Redis is fast, but that is also non-volatile.
Implementation of data structures stress on memory efficiency, so data structures inside Redis will likely use less memory compared to the same data structure modeled using an high level programming language.
Redis offers a number of features that are natural to find in a database, like replication, tunable levels of durability, cluster, high availability.

Another good example is to think of Redis as a more complex version of memcached, where the operations are not just SETs and GETs, but operations to work with complex data types like Lists, Sets, ordered data structures, and so forth.
If you want to know more, this is a list of selected starting points:

Introduction to Redis data types. http://redis.io/topics/data-types-intro
Try Redis directly inside your browser. http://try.redis.io
The full list of Redis commands. http://redis.io/commands
There is much more inside the Redis official documentation. http://redis.io/documentation

Building Redis
Redis can be compiled and used on Linux, OSX, OpenBSD, NetBSD, FreeBSD.
We support big endian and little endian architectures, and both 32 bit
and 64 bit systems.
It may compile on Solaris derived systems (for instance SmartOS) but our
support for this platform is best effort and Redis is not guaranteed to
work as well as in Linux, OSX, and *BSD there.
It is as simple as:
% make

You can run a 32 bit Redis binary using:
% make 32bit

After building Redis, it is a good idea to test it using:
% make test

Fixing build problems with dependencies or cached build options
Redis has some dependencies which are included into the deps directory.
make does not automatically rebuild dependencies even if something in
the source code of dependencies changes.
When you update the source code with git pull or when code inside the
dependencies tree is modified in any other way, make sure to use the following
command in order to really clean everything and rebuild from scratch:
make distclean

This will clean: jemalloc, lua, hiredis, linenoise.
Also if you force certain build options like 32bit target, no C compiler
optimizations (for debugging purposes), and other similar build time options,
those options are cached indefinitely until you issue a make distclean
command.
Fixing problems building 32 bit binaries
If after building Redis with a 32 bit target you need to rebuild it
with a 64 bit target, or the other way around, you need to perform a
make distclean in the root directory of the Redis distribution.
In case of build errors when trying to build a 32 bit binary of Redis, try
the following steps:

Install the packages libc6-dev-i386 (also try g++-multilib).
Try using the following command line instead of make 32bit:
make CFLAGS=""-m32 -march=native"" LDFLAGS=""-m32""

Allocator
Selecting a non-default memory allocator when building Redis is done by setting
the MALLOC environment variable. Redis is compiled and linked against libc
malloc by default, with the exception of jemalloc being the default on Linux
systems. This default was picked because jemalloc has proven to have fewer
fragmentation problems than libc malloc.
To force compiling against libc malloc, use:
% make MALLOC=libc

To compile against jemalloc on Mac OS X systems, use:
% make MALLOC=jemalloc

Verbose build
Redis will build with a user friendly colorized output by default.
If you want to see a more verbose output use the following:
% make V=1

Running Redis
To run Redis with the default configuration just type:
% cd src
% ./redis-server

If you want to provide your redis.conf, you have to run it using an additional
parameter (the path of the configuration file):
% cd src
% ./redis-server /path/to/redis.conf

It is possible to alter the Redis configuration by passing parameters directly
as options using the command line. Examples:
% ./redis-server --port 9999 --replicaof 127.0.0.1 6379
% ./redis-server /etc/redis/6379.conf --loglevel debug

All the options in redis.conf are also supported as options using the command
line, with exactly the same name.
Playing with Redis
You can use redis-cli to play with Redis. Start a redis-server instance,
then in another terminal try the following:
% cd src
% ./redis-cli
redis> ping
PONG
redis> set foo bar
OK
redis> get foo
""bar""
redis> incr mycounter
(integer) 1
redis> incr mycounter
(integer) 2
redis>

You can find the list of all the available commands at http://redis.io/commands.
Installing Redis
In order to install Redis binaries into /usr/local/bin just use:
% make install

You can use make PREFIX=/some/other/directory install if you wish to use a
different destination.
Make install will just install binaries in your system, but will not configure
init scripts and configuration files in the appropriate place. This is not
needed if you want just to play a bit with Redis, but if you are installing
it the proper way for a production system, we have a script doing this
for Ubuntu and Debian systems:
% cd utils
% ./install_server.sh

The script will ask you a few questions and will setup everything you need
to run Redis properly as a background daemon that will start again on
system reboots.
You'll be able to stop and start Redis using the script named
/etc/init.d/redis_<portnumber>, for instance /etc/init.d/redis_6379.
Code contributions
Note: by contributing code to the Redis project in any form, including sending
a pull request via Github, a code fragment or patch via private email or
public discussion groups, you agree to release your code under the terms
of the BSD license that you can find in the COPYING file included in the Redis
source distribution.
Please see the CONTRIBUTING file in this source distribution for more
information.
Redis internals
If you are reading this README you are likely in front of a Github page
or you just untarred the Redis distribution tar ball. In both the cases
you are basically one step away from the source code, so here we explain
the Redis source code layout, what is in each file as a general idea, the
most important functions and structures inside the Redis server and so forth.
We keep all the discussion at a high level without digging into the details
since this document would be huge otherwise and our code base changes
continuously, but a general idea should be a good starting point to
understand more. Moreover most of the code is heavily commented and easy
to follow.
Source code layout
The Redis root directory just contains this README, the Makefile which
calls the real Makefile inside the src directory and an example
configuration for Redis and Sentinel. You can find a few shell
scripts that are used in order to execute the Redis, Redis Cluster and
Redis Sentinel unit tests, which are implemented inside the tests
directory.
Inside the root are the following important directories:

src: contains the Redis implementation, written in C.
tests: contains the unit tests, implemented in Tcl.
deps: contains libraries Redis uses. Everything needed to compile Redis is inside this directory; your system just needs to provide libc, a POSIX compatible interface and a C compiler. Notably deps contains a copy of jemalloc, which is the default allocator of Redis under Linux. Note that under deps there are also things which started with the Redis project, but for which the main repository is not antirez/redis. An exception to this rule is deps/geohash-int which is the low level geocoding library used by Redis: it originated from a different project, but at this point it diverged so much that it is developed as a separated entity directly inside the Redis repository.

There are a few more directories but they are not very important for our goals
here. We'll focus mostly on src, where the Redis implementation is contained,
exploring what there is inside each file. The order in which files are
exposed is the logical one to follow in order to disclose different layers
of complexity incrementally.
Note: lately Redis was refactored quite a bit. Function names and file
names have been changed, so you may find that this documentation reflects the
unstable branch more closely. For instance in Redis 3.0 the server.c
and server.h files were named redis.c and redis.h. However the overall
structure is the same. Keep in mind that all the new developments and pull
requests should be performed against the unstable branch.
server.h
The simplest way to understand how a program works is to understand the
data structures it uses. So we'll start from the main header file of
Redis, which is server.h.
All the server configuration and in general all the shared state is
defined in a global structure called server, of type struct redisServer.
A few important fields in this structure are:

server.db is an array of Redis databases, where data is stored.
server.commands is the command table.
server.clients is a linked list of clients connected to the server.
server.master is a special client, the master, if the instance is a replica.

There are tons of other fields. Most fields are commented directly inside
the structure definition.
Another important Redis data structure is the one defining a client.
In the past it was called redisClient, now just client. The structure
has many fields, here we'll just show the main ones:
struct client {
    int fd;
    sds querybuf;
    int argc;
    robj **argv;
    redisDb *db;
    int flags;
    list *reply;
    char buf[PROTO_REPLY_CHUNK_BYTES];
    ... many other fields ...
}

The client structure defines a connected client:

The fd field is the client socket file descriptor.
argc and argv are populated with the command the client is executing, so that functions implementing a given Redis command can read the arguments.
querybuf accumulates the requests from the client, which are parsed by the Redis server according to the Redis protocol and executed by calling the implementations of the commands the client is executing.
reply and buf are dynamic and static buffers that accumulate the replies the server sends to the client. These buffers are incrementally written to the socket as soon as the file descriptor is writable.

As you can see in the client structure above, arguments in a command
are described as robj structures. The following is the full robj
structure, which defines a Redis object:
typedef struct redisObject {
    unsigned type:4;
    unsigned encoding:4;
    unsigned lru:LRU_BITS; /* lru time (relative to server.lruclock) */
    int refcount;
    void *ptr;
} robj;

Basically this structure can represent all the basic Redis data types like
strings, lists, sets, sorted sets and so forth. The interesting thing is that
it has a type field, so that it is possible to know what type a given
object has, and a refcount, so that the same object can be referenced
in multiple places without allocating it multiple times. Finally the ptr
field points to the actual representation of the object, which might vary
even for the same type, depending on the encoding used.
Redis objects are used extensively in the Redis internals, however in order
to avoid the overhead of indirect accesses, recently in many places
we just use plain dynamic strings not wrapped inside a Redis object.
server.c
This is the entry point of the Redis server, where the main() function
is defined. The following are the most important steps in order to startup
the Redis server.

initServerConfig() setups the default values of the server structure.
initServer() allocates the data structures needed to operate, setup the listening socket, and so forth.
aeMain() starts the event loop which listens for new connections.

There are two special functions called periodically by the event loop:

serverCron() is called periodically (according to server.hz frequency), and performs tasks that must be performed from time to time, like checking for timedout clients.
beforeSleep() is called every time the event loop fired, Redis served a few requests, and is returning back into the event loop.

Inside server.c you can find code that handles other vital things of the Redis server:

call() is used in order to call a given command in the context of a given client.
activeExpireCycle() handles eviciton of keys with a time to live set via the EXPIRE command.
freeMemoryIfNeeded() is called when a new write command should be performed but Redis is out of memory according to the maxmemory directive.
The global variable redisCommandTable defines all the Redis commands, specifying the name of the command, the function implementing the command, the number of arguments required, and other properties of each command.

networking.c
This file defines all the I/O functions with clients, masters and replicas
(which in Redis are just special clients):

createClient() allocates and initializes a new client.
the addReply*() family of functions are used by commands implementations in order to append data to the client structure, that will be transmitted to the client as a reply for a given command executed.
writeToClient() transmits the data pending in the output buffers to the client and is called by the writable event handler sendReplyToClient().
readQueryFromClient() is the readable event handler and accumulates data from read from the client into the query buffer.
processInputBuffer() is the entry point in order to parse the client query buffer according to the Redis protocol. Once commands are ready to be processed, it calls processCommand() which is defined inside server.c in order to actually execute the command.
freeClient() deallocates, disconnects and removes a client.

aof.c and rdb.c
As you can guess from the names these files implement the RDB and AOF
persistence for Redis. Redis uses a persistence model based on the fork()
system call in order to create a thread with the same (shared) memory
content of the main Redis thread. This secondary thread dumps the content
of the memory on disk. This is used by rdb.c to create the snapshots
on disk and by aof.c in order to perform the AOF rewrite when the
append only file gets too big.
The implementation inside aof.c has additional functions in order to
implement an API that allows commands to append new commands into the AOF
file as clients execute them.
The call() function defined inside server.c is responsible to call
the functions that in turn will write the commands into the AOF.
db.c
Certain Redis commands operate on specific data types, others are general.
Examples of generic commands are DEL and EXPIRE. They operate on keys
and not on their values specifically. All those generic commands are
defined inside db.c.
Moreover db.c implements an API in order to perform certain operations
on the Redis dataset without directly accessing the internal data structures.
The most important functions inside db.c which are used in many commands
implementations are the following:

lookupKeyRead() and lookupKeyWrite() are used in order to get a pointer to the value associated to a given key, or NULL if the key does not exist.
dbAdd() and its higher level counterpart setKey() create a new key in a Redis database.
dbDelete() removes a key and its associated value.
emptyDb() removes an entire single database or all the databases defined.

The rest of the file implements the generic commands exposed to the client.
object.c
The robj structure defining Redis objects was already described. Inside
object.c there are all the functions that operate with Redis objects at
a basic level, like functions to allocate new objects, handle the reference
counting and so forth. Notable functions inside this file:

incrRefcount() and decrRefCount() are used in order to increment or decrement an object reference count. When it drops to 0 the object is finally freed.
createObject() allocates a new object. There are also specialized functions to allocate string objects having a specific content, like createStringObjectFromLongLong() and similar functions.

This file also implements the OBJECT command.
replication.c
This is one of the most complex files inside Redis, it is recommended to
approach it only after getting a bit familiar with the rest of the code base.
In this file there is the implementation of both the master and replica role
of Redis.
One of the most important functions inside this file is replicationFeedSlaves() that writes commands to the clients representing replica instances connected
to our master, so that the replicas can get the writes performed by the clients:
this way their data set will remain synchronized with the one in the master.
This file also implements both the SYNC and PSYNC commands that are
used in order to perform the first synchronization between masters and
replicas, or to continue the replication after a disconnection.
Other C files

t_hash.c, t_list.c, t_set.c, t_string.c and t_zset.c contains the implementation of the Redis data types. They implement both an API to access a given data type, and the client commands implementations for these data types.
ae.c implements the Redis event loop, it's a self contained library which is simple to read and understand.
sds.c is the Redis string library, check http://github.com/antirez/sds for more information.
anet.c is a library to use POSIX networking in a simpler way compared to the raw interface exposed by the kernel.
dict.c is an implementation of a non-blocking hash table which rehashes incrementally.
scripting.c implements Lua scripting. It is completely self contained from the rest of the Redis implementation and is simple enough to understand if you are familar with the Lua API.
cluster.c implements the Redis Cluster. Probably a good read only after being very familiar with the rest of the Redis code base. If you want to read cluster.c make sure to read the Redis Cluster specification.

Anatomy of a Redis command
All the Redis commands are defined in the following way:
void foobarCommand(client *c) {
    printf(""%s"",c->argv[1]->ptr); /* Do something with the argument. */
    addReply(c,shared.ok); /* Reply something to the client. */
}

The command is then referenced inside server.c in the command table:
{""foobar"",foobarCommand,2,""rtF"",0,NULL,0,0,0,0,0},

In the above example 2 is the number of arguments the command takes,
while ""rtF"" are the command flags, as documented in the command table
top comment inside server.c.
After the command operates in some way, it returns a reply to the client,
usually using addReply() or a similar function defined inside networking.c.
There are tons of commands implementations inside the Redis source code
that can serve as examples of actual commands implementations. To write
a few toy commands can be a good exercise to familiarize with the code base.
There are also many other files not described here, but it is useless to
cover everything. We want to just help you with the first steps.
Eventually you'll find your way inside the Redis code base :-)
Enjoy!
",batch2,8:09:15,Done
66,BuiltBrokenModding/ICBM-2,"What is ICBM?
NOTE
The current 1.7.10 versions are complete rewrites of ICBM, most things from 1.6.4 are not implemented yet. There are plans to implement missing content later, either in ICBM or in separate mods.
ICBM is a Minecraft Mod that introduces intercontinental ballistic missiles to Minecraft.
Who owns ICBM
Originally ICBM was owned mostly by Calclavia with the Sentry guns belonging to DarkGuardsman. After a few event and lack of progress ownership was shifted fully to DarkGuardsman. This was in exchange for majority ownership of Resonant Induction to Calclavia.
That being said as far as DarkGuardsman is concerned the mod downloads are community property. At any time the mod is no longer supported or updated anyone is welcome to take over. Given that the new maintainer understands who owns the code at the end of the day.
license

Free to install
Free to modify for personal use
Free to contribute to project
Request to redistribute files
Can only use code for educational use

Read license.md for full lisense
Warranty
THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
Pull Requests
Before you submit a pull request you must sign or agree to the Contributor License Agreement(CLA). Submission by default can be considered as agreeing to the CLA and any terms of the project.
Contributions
Before submitting changes sign the CLA or the submittion will not be accepted https://www.clahub.com/agreements/Universal-Electricity/ICBM
If you do not sign the CLA, summission still counts as accepting the terms of the CLA in some cases. Theses cases are noted, not limited to, as failure to respond to a request to view the CLA, and continued submission of changes after being notified of the CLA.
https://docs.google.com/document/d/105N6q63OJnzrWjMbydFYFUlHmK6spc8P8da8qkduFRs/edit?usp=sharing
Team

Darkguardsman     - Owner, Head Developer
Dmodoomsirus      - Gradle Tech
Dragonlorder      - Asset Developer
Graugger          - Asset Developer

Credits
If you or someone you know is not listed but should be just message us with the contribution. We will then
validate the claim and add the missing name(s) to the list.

Adaptivity        - Russian Translations
Archelf           - Assets
Archadia          - Code, Assets
Azanor            - Consultation
Calclavia         - Original Author
CovertJaguar      - Consultation
Comply_cat_Ed     - Assets
Doppelgangerous   - Concepts
DryTaste          - Assets
Francogp          - Old Spanish Translations
Graugger          - Concepts
Gravitythief      - Code
hublad			- Pl_PL Translations
Jonathan T.       - ???
ObsidianWalker    - Old Italian Translations
Maxwolf           - Code
Pheenixm          - Assets
Scout37           - Assets
Superpatosainz    - Code
Tgame14           - Code
Vexatos           - German Translations
Wdtod             - Laser Sentry Model
Wildex999         - ???
Hennamann         - Code, Norwegian Translations
VictiniX888       - Chinese Translations

Freesound.org Audio Credits

qubodup               - https://www.freesound.org/people/qubodup/sounds/146770/
- https://www.freesound.org/people/qubodup/sounds/67542/
Bird_man              - https://www.freesound.org/people/Bird_man/sounds/275151/
LG                    - https://www.freesound.org/people/LG/sounds/18726/
Yap_Audio_Production  - https://www.freesound.org/people/Yap_Audio_Production/sounds/218482/
jimhancock			- https://www.freesound.org/people/jimhancock/sounds/256128/
FreqMan 				- http://www.freesound.org/people/FreqMan/sounds/24727/
bmaczero				- http://www.freesound.org/people/BMacZero/sounds/94132/
- http://www.freesound.org/people/BMacZero/sounds/94128/
unfa                  - http://www.freesound.org/people/unfa/sounds/222678/
simonsco              - http://www.freesound.org/people/simosco/sounds/235537/
ecfike                - http://www.freesound.org/people/ecfike/sounds/130139/
hoffy1138             - http://www.freesound.org/people/hoffy1138/sounds/276968/

Download Locations

http://builtbroken.com/pages/icbm/

Installing

Install Minecraft Forge.
Install Voltz Engine
Download the mod
Add it to the mods folder
Run Minecraft.
Have fun.

Coverage

",batch2,8:10:18,Done
67,dimagi/commcare-hq,"CommCare HQ
CommCare HQ is a server-side tool to help manage community health workers.
It seamlessly integrates with CommCare J2ME, Android, and Web Apps as well as
providing generic domain management and form data-collection functionality.
Key Components

CommCare application builder
OpenRosa compliant XForms designer
SMS integration
Domain/user/mobile worker management
XForms data collection
Case management
Over-the-air (ota) restore of user and cases
Integrated web and email reporting

More Information

To try CommCare you can use this production instance of hosted CommCare.
To setup a local CommCare HQ developer environment, see Setting up CommCare HQ for Developers.
To setup a production CommCare HQ environment, check out CommCare Cloud, our toolkit for deploying and maintaining CommCare servers.
Additional documentation is available on ReadTheDocs.
We welcome contributions, see Contributing for more.
Questions?  Contact the CommCare community at our forum.

CommCare is built by Dimagi and our open-source contributors.
",batch1,16:59:00,Done
68,jcmdln/emacs,,batch1,16:58:59,Done
69,kernelai/pika-cli,"基于redis官方5.0.9修改
This README is just a fast quick start document. You can find more detailed documentation at redis.io.
What is Redis?
Redis is often referred as a data structures server. What this means is that Redis provides access to mutable data structures via a set of commands, which are sent using a server-client model with TCP sockets and a simple protocol. So different processes can query and modify the same data structures in a shared way.
Data structures implemented into Redis have a few special properties:

Redis cares to store them on disk, even if they are always served and modified into the server memory. This means that Redis is fast, but that is also non-volatile.
Implementation of data structures stress on memory efficiency, so data structures inside Redis will likely use less memory compared to the same data structure modeled using an high level programming language.
Redis offers a number of features that are natural to find in a database, like replication, tunable levels of durability, cluster, high availability.

Another good example is to think of Redis as a more complex version of memcached, where the operations are not just SETs and GETs, but operations to work with complex data types like Lists, Sets, ordered data structures, and so forth.
If you want to know more, this is a list of selected starting points:

Introduction to Redis data types. http://redis.io/topics/data-types-intro
Try Redis directly inside your browser. http://try.redis.io
The full list of Redis commands. http://redis.io/commands
There is much more inside the Redis official documentation. http://redis.io/documentation

Building Redis
Redis can be compiled and used on Linux, OSX, OpenBSD, NetBSD, FreeBSD.
We support big endian and little endian architectures, and both 32 bit
and 64 bit systems.
It may compile on Solaris derived systems (for instance SmartOS) but our
support for this platform is best effort and Redis is not guaranteed to
work as well as in Linux, OSX, and *BSD there.
It is as simple as:
% make

You can run a 32 bit Redis binary using:
% make 32bit

After building Redis, it is a good idea to test it using:
% make test

Fixing build problems with dependencies or cached build options
Redis has some dependencies which are included into the deps directory.
make does not automatically rebuild dependencies even if something in
the source code of dependencies changes.
When you update the source code with git pull or when code inside the
dependencies tree is modified in any other way, make sure to use the following
command in order to really clean everything and rebuild from scratch:
make distclean

This will clean: jemalloc, lua, hiredis, linenoise.
Also if you force certain build options like 32bit target, no C compiler
optimizations (for debugging purposes), and other similar build time options,
those options are cached indefinitely until you issue a make distclean
command.
Fixing problems building 32 bit binaries
If after building Redis with a 32 bit target you need to rebuild it
with a 64 bit target, or the other way around, you need to perform a
make distclean in the root directory of the Redis distribution.
In case of build errors when trying to build a 32 bit binary of Redis, try
the following steps:

Install the packages libc6-dev-i386 (also try g++-multilib).
Try using the following command line instead of make 32bit:
make CFLAGS=""-m32 -march=native"" LDFLAGS=""-m32""

Allocator
Selecting a non-default memory allocator when building Redis is done by setting
the MALLOC environment variable. Redis is compiled and linked against libc
malloc by default, with the exception of jemalloc being the default on Linux
systems. This default was picked because jemalloc has proven to have fewer
fragmentation problems than libc malloc.
To force compiling against libc malloc, use:
% make MALLOC=libc

To compile against jemalloc on Mac OS X systems, use:
% make MALLOC=jemalloc

Verbose build
Redis will build with a user friendly colorized output by default.
If you want to see a more verbose output use the following:
% make V=1

Running Redis
To run Redis with the default configuration just type:
% cd src
% ./redis-server

If you want to provide your redis.conf, you have to run it using an additional
parameter (the path of the configuration file):
% cd src
% ./redis-server /path/to/redis.conf

It is possible to alter the Redis configuration by passing parameters directly
as options using the command line. Examples:
% ./redis-server --port 9999 --replicaof 127.0.0.1 6379
% ./redis-server /etc/redis/6379.conf --loglevel debug

All the options in redis.conf are also supported as options using the command
line, with exactly the same name.
Playing with Redis
You can use redis-cli to play with Redis. Start a redis-server instance,
then in another terminal try the following:
% cd src
% ./redis-cli
redis> ping
PONG
redis> set foo bar
OK
redis> get foo
""bar""
redis> incr mycounter
(integer) 1
redis> incr mycounter
(integer) 2
redis>

You can find the list of all the available commands at http://redis.io/commands.
Installing Redis
In order to install Redis binaries into /usr/local/bin just use:
% make install

You can use make PREFIX=/some/other/directory install if you wish to use a
different destination.
Make install will just install binaries in your system, but will not configure
init scripts and configuration files in the appropriate place. This is not
needed if you want just to play a bit with Redis, but if you are installing
it the proper way for a production system, we have a script doing this
for Ubuntu and Debian systems:
% cd utils
% ./install_server.sh

The script will ask you a few questions and will setup everything you need
to run Redis properly as a background daemon that will start again on
system reboots.
You'll be able to stop and start Redis using the script named
/etc/init.d/redis_<portnumber>, for instance /etc/init.d/redis_6379.
Code contributions
Note: by contributing code to the Redis project in any form, including sending
a pull request via Github, a code fragment or patch via private email or
public discussion groups, you agree to release your code under the terms
of the BSD license that you can find in the COPYING file included in the Redis
source distribution.
Please see the CONTRIBUTING file in this source distribution for more
information.
Redis internals
If you are reading this README you are likely in front of a Github page
or you just untarred the Redis distribution tar ball. In both the cases
you are basically one step away from the source code, so here we explain
the Redis source code layout, what is in each file as a general idea, the
most important functions and structures inside the Redis server and so forth.
We keep all the discussion at a high level without digging into the details
since this document would be huge otherwise and our code base changes
continuously, but a general idea should be a good starting point to
understand more. Moreover most of the code is heavily commented and easy
to follow.
Source code layout
The Redis root directory just contains this README, the Makefile which
calls the real Makefile inside the src directory and an example
configuration for Redis and Sentinel. You can find a few shell
scripts that are used in order to execute the Redis, Redis Cluster and
Redis Sentinel unit tests, which are implemented inside the tests
directory.
Inside the root are the following important directories:

src: contains the Redis implementation, written in C.
tests: contains the unit tests, implemented in Tcl.
deps: contains libraries Redis uses. Everything needed to compile Redis is inside this directory; your system just needs to provide libc, a POSIX compatible interface and a C compiler. Notably deps contains a copy of jemalloc, which is the default allocator of Redis under Linux. Note that under deps there are also things which started with the Redis project, but for which the main repository is not antirez/redis. An exception to this rule is deps/geohash-int which is the low level geocoding library used by Redis: it originated from a different project, but at this point it diverged so much that it is developed as a separated entity directly inside the Redis repository.

There are a few more directories but they are not very important for our goals
here. We'll focus mostly on src, where the Redis implementation is contained,
exploring what there is inside each file. The order in which files are
exposed is the logical one to follow in order to disclose different layers
of complexity incrementally.
Note: lately Redis was refactored quite a bit. Function names and file
names have been changed, so you may find that this documentation reflects the
unstable branch more closely. For instance in Redis 3.0 the server.c
and server.h files were named redis.c and redis.h. However the overall
structure is the same. Keep in mind that all the new developments and pull
requests should be performed against the unstable branch.
server.h
The simplest way to understand how a program works is to understand the
data structures it uses. So we'll start from the main header file of
Redis, which is server.h.
All the server configuration and in general all the shared state is
defined in a global structure called server, of type struct redisServer.
A few important fields in this structure are:

server.db is an array of Redis databases, where data is stored.
server.commands is the command table.
server.clients is a linked list of clients connected to the server.
server.master is a special client, the master, if the instance is a replica.

There are tons of other fields. Most fields are commented directly inside
the structure definition.
Another important Redis data structure is the one defining a client.
In the past it was called redisClient, now just client. The structure
has many fields, here we'll just show the main ones:
struct client {
    int fd;
    sds querybuf;
    int argc;
    robj **argv;
    redisDb *db;
    int flags;
    list *reply;
    char buf[PROTO_REPLY_CHUNK_BYTES];
    ... many other fields ...
}

The client structure defines a connected client:

The fd field is the client socket file descriptor.
argc and argv are populated with the command the client is executing, so that functions implementing a given Redis command can read the arguments.
querybuf accumulates the requests from the client, which are parsed by the Redis server according to the Redis protocol and executed by calling the implementations of the commands the client is executing.
reply and buf are dynamic and static buffers that accumulate the replies the server sends to the client. These buffers are incrementally written to the socket as soon as the file descriptor is writable.

As you can see in the client structure above, arguments in a command
are described as robj structures. The following is the full robj
structure, which defines a Redis object:
typedef struct redisObject {
    unsigned type:4;
    unsigned encoding:4;
    unsigned lru:LRU_BITS; /* lru time (relative to server.lruclock) */
    int refcount;
    void *ptr;
} robj;

Basically this structure can represent all the basic Redis data types like
strings, lists, sets, sorted sets and so forth. The interesting thing is that
it has a type field, so that it is possible to know what type a given
object has, and a refcount, so that the same object can be referenced
in multiple places without allocating it multiple times. Finally the ptr
field points to the actual representation of the object, which might vary
even for the same type, depending on the encoding used.
Redis objects are used extensively in the Redis internals, however in order
to avoid the overhead of indirect accesses, recently in many places
we just use plain dynamic strings not wrapped inside a Redis object.
server.c
This is the entry point of the Redis server, where the main() function
is defined. The following are the most important steps in order to startup
the Redis server.

initServerConfig() setups the default values of the server structure.
initServer() allocates the data structures needed to operate, setup the listening socket, and so forth.
aeMain() starts the event loop which listens for new connections.

There are two special functions called periodically by the event loop:

serverCron() is called periodically (according to server.hz frequency), and performs tasks that must be performed from time to time, like checking for timedout clients.
beforeSleep() is called every time the event loop fired, Redis served a few requests, and is returning back into the event loop.

Inside server.c you can find code that handles other vital things of the Redis server:

call() is used in order to call a given command in the context of a given client.
activeExpireCycle() handles eviciton of keys with a time to live set via the EXPIRE command.
freeMemoryIfNeeded() is called when a new write command should be performed but Redis is out of memory according to the maxmemory directive.
The global variable redisCommandTable defines all the Redis commands, specifying the name of the command, the function implementing the command, the number of arguments required, and other properties of each command.

networking.c
This file defines all the I/O functions with clients, masters and replicas
(which in Redis are just special clients):

createClient() allocates and initializes a new client.
the addReply*() family of functions are used by commands implementations in order to append data to the client structure, that will be transmitted to the client as a reply for a given command executed.
writeToClient() transmits the data pending in the output buffers to the client and is called by the writable event handler sendReplyToClient().
readQueryFromClient() is the readable event handler and accumulates data from read from the client into the query buffer.
processInputBuffer() is the entry point in order to parse the client query buffer according to the Redis protocol. Once commands are ready to be processed, it calls processCommand() which is defined inside server.c in order to actually execute the command.
freeClient() deallocates, disconnects and removes a client.

aof.c and rdb.c
As you can guess from the names these files implement the RDB and AOF
persistence for Redis. Redis uses a persistence model based on the fork()
system call in order to create a thread with the same (shared) memory
content of the main Redis thread. This secondary thread dumps the content
of the memory on disk. This is used by rdb.c to create the snapshots
on disk and by aof.c in order to perform the AOF rewrite when the
append only file gets too big.
The implementation inside aof.c has additional functions in order to
implement an API that allows commands to append new commands into the AOF
file as clients execute them.
The call() function defined inside server.c is responsible to call
the functions that in turn will write the commands into the AOF.
db.c
Certain Redis commands operate on specific data types, others are general.
Examples of generic commands are DEL and EXPIRE. They operate on keys
and not on their values specifically. All those generic commands are
defined inside db.c.
Moreover db.c implements an API in order to perform certain operations
on the Redis dataset without directly accessing the internal data structures.
The most important functions inside db.c which are used in many commands
implementations are the following:

lookupKeyRead() and lookupKeyWrite() are used in order to get a pointer to the value associated to a given key, or NULL if the key does not exist.
dbAdd() and its higher level counterpart setKey() create a new key in a Redis database.
dbDelete() removes a key and its associated value.
emptyDb() removes an entire single database or all the databases defined.

The rest of the file implements the generic commands exposed to the client.
object.c
The robj structure defining Redis objects was already described. Inside
object.c there are all the functions that operate with Redis objects at
a basic level, like functions to allocate new objects, handle the reference
counting and so forth. Notable functions inside this file:

incrRefcount() and decrRefCount() are used in order to increment or decrement an object reference count. When it drops to 0 the object is finally freed.
createObject() allocates a new object. There are also specialized functions to allocate string objects having a specific content, like createStringObjectFromLongLong() and similar functions.

This file also implements the OBJECT command.
replication.c
This is one of the most complex files inside Redis, it is recommended to
approach it only after getting a bit familiar with the rest of the code base.
In this file there is the implementation of both the master and replica role
of Redis.
One of the most important functions inside this file is replicationFeedSlaves() that writes commands to the clients representing replica instances connected
to our master, so that the replicas can get the writes performed by the clients:
this way their data set will remain synchronized with the one in the master.
This file also implements both the SYNC and PSYNC commands that are
used in order to perform the first synchronization between masters and
replicas, or to continue the replication after a disconnection.
Other C files

t_hash.c, t_list.c, t_set.c, t_string.c and t_zset.c contains the implementation of the Redis data types. They implement both an API to access a given data type, and the client commands implementations for these data types.
ae.c implements the Redis event loop, it's a self contained library which is simple to read and understand.
sds.c is the Redis string library, check http://github.com/antirez/sds for more information.
anet.c is a library to use POSIX networking in a simpler way compared to the raw interface exposed by the kernel.
dict.c is an implementation of a non-blocking hash table which rehashes incrementally.
scripting.c implements Lua scripting. It is completely self contained from the rest of the Redis implementation and is simple enough to understand if you are familar with the Lua API.
cluster.c implements the Redis Cluster. Probably a good read only after being very familiar with the rest of the Redis code base. If you want to read cluster.c make sure to read the Redis Cluster specification.

Anatomy of a Redis command
All the Redis commands are defined in the following way:
void foobarCommand(client *c) {
    printf(""%s"",c->argv[1]->ptr); /* Do something with the argument. */
    addReply(c,shared.ok); /* Reply something to the client. */
}

The command is then referenced inside server.c in the command table:
{""foobar"",foobarCommand,2,""rtF"",0,NULL,0,0,0,0,0},

In the above example 2 is the number of arguments the command takes,
while ""rtF"" are the command flags, as documented in the command table
top comment inside server.c.
After the command operates in some way, it returns a reply to the client,
usually using addReply() or a similar function defined inside networking.c.
There are tons of commands implementations inside the Redis source code
that can serve as examples of actual commands implementations. To write
a few toy commands can be a good exercise to familiarize with the code base.
There are also many other files not described here, but it is useless to
cover everything. We want to just help you with the first steps.
Eventually you'll find your way inside the Redis code base :-)
Enjoy!
",batch2,8:09:15,Done
70,dondelelcaro/debbugs,"Debbugs
Debian Bug-Tracking System

What is Debbugs?
Debbugs is a stable, scaleable bug reporting and issue tracking system. Debbugs has a web interface for viewing and searching issues in the database but unlike other bug tracking systems, Debbugs has no web interface for editing bug reports - all modification is done via email.
The most notable deployment of Debbugs is on the Debian project
System Requirements

GNU date
GNU gzip
Perl 5 (5.005 is known to work)
Mailtools and MIME-tools perl modules to manipulate email
Lynx 2.7 or later
The bug system requires its own mail domain. It comes with code
which understands how exim, qmail and sendmail deliver mail for such a
domain to a script.
A webserver. For the old system of static HTML pages generated for
bug reports and index pages, this is easiest if the bug system can
write directly to the webspace; for the new system of CGI scripts
that generate web pages on the fly, write access is not required.
Somewhere to run CGI scripts (unless you don't need the web forms for
searching for bugs by number, package, maintainer or submitter).

Where do I get the Source?
Debbugs is managed in git. You can clone the repository into your local
workspace as follows:
    git clone http://bugs-master.debian.org/debbugs-source/debbugs.git

Additional branches are available from:

Don Armstrong

Installation Instructions
Install the Debian package and read /usr/share/doc/debbugs/README.Debian file.
If you can't use the .deb, do the following:


Clone the repo
git clone http://bugs-master.debian.org/debbugs-source/debbugs.git



Create version and spool directory
cd
mkdir version spool



Optional - Retrieve a partial database of bugs for testing


Get a list of rsync targets from Debbugs
 rsync --list-only rsync://bugs-mirror.debian.org



Grab bugs ending in 00
 mkdir -p splool/db-h/00;
 cd spool/db-h;
 rsync -av rsync://bugs-mirror.debian.org/bts-spool-db/00 .;





Optional - Retrieve bts-versions directory for testing purposes
The database obtained in step 3 requires associated version and index information.


Pull versions directory
 rsync -av rsync://bugs-mirror.debian.org/bts-versions/ versions/



Pull index directory
 rsync -av rsync://bugs-mirror.debian.org/bts-spool-index index





Configure Debbugs config


Create a config directory for Debbugs
 sudo mkdir /etc/debbugs



Copy sample configuration to config directory
 sudo cp ~/debbugs/scripts/config.debian /etc/debbugs/config



Update the following variables

$gConfigDir
$gSpoolDir
$gIndicesDir
$gWebDir
$gDocDir

as follows:
 70,72c70,72
 < $gConfigDir = ""/org/bugs.debian.org/etc""; # directory where this file is
 < $gSpoolDir = ""/org/bugs.debian.org/spool""; # working directory
 < $gIndicesDir = ""/org/bugs.debian.org/indices""; # directory where the indices are
 ---
 > $gConfigDir = ""/etc/debbugs""; # directory where this file is
 > $gSpoolDir = ""/path/to/directory/spool""; # working directory
 > $gIndicesDir = ""/path/to/directory/spool/indices""; # directory  where the indices are
 
 74,75c74,75
 < $gWebDir = ""/org/bugs.debian.org/www""; # base location of web pages
 < $gDocDir = ""/org/ftp.debian.org/ftp/doc""; # location of text doc files
 ---
 > $gWebDir = ""/path/to/directory/debbugs/html""; # base location of web pages
 > $gDocDir = ""/path/to/directory/debbugs/doc""; # location of text doc files





Configure Webserver


Copy example apache config
 sudo cp /path/to/directory/debbugs/examples/apache.conf  /etc/apache2/sites-available/debbugs.conf



Update the directory entries and the DocumentRoot and ScriptAlias variables
 5c5
 < DocumentRoot /var/lib/debbugs/www/
 ---
 > DocumentRoot /path/to/directory/debbugs/html/

 10c10
 < <Directory /var/lib/debbugs/www>
 ---
 > <Directory /path/to/directory/debbugs/html>

 16,17c16,17
 < ScriptAlias /cgi-bin/ /var/lib/debbugs/www/cgi/
 < <Directory ""/var/lib/debbugs/www/cgi/"">
 ---
 > ScriptAlias /cgi-bin/ /path/to/directory/debbugs/cgi/
 > <Directory ""/path/to/directory/debbugs/cgi/"">



Enable required apache mods
 sudo a2enmod rewrite
 sudo a2enmod cgid



Install site
 sudo a2ensite debbugs



Reload apache
 sudo service apache2 reload





Install dependencies
 sudo apt-get install libmailtools-perl ed libmime-tools-perl libio-stringy-perl libmldbm-perl liburi-perl libsoap-lite-perl libcgi-simple-perl libparams-validate-perl libtext-template-perl libsafe-hole-perl libmail-rfc822-address-perl liblist-moreutils-perl libtext-template-perl libfile-libmagic-perl libgravatar-url-perl libwww-perl imagemagick libapache2-mod-perl2



Set up libraries


Create symlinks to link source to their expected locations
 sudo mkdir -p /usr/local/lib/site_perl
 sudo ln -s /path/to/directory/debbugs/Debbugs /usr/local/lib/site_perl/

 sudo mkdir -p /usr/share/debbugs/
 sudo ln -s /path/to/directory/debbugs/templates /usr/share/debbugs/





Create required files


Create files
 touch /etc/debbugs/pseudo-packages.description
 touch /etc/debbugs/Source_maintainers
 touch /etc/debbugs/pseudo-packages.maintainers
 touch /etc/debbugs/Maintainers
 touch /etc/debbugs/Maintainers.override
 mkdir /etc/debbugs/indices
 touch /etc/debbugs/indices/sources



Test
 cd /path/to/directory/debbugs
 perl -c cgi/bugreport.cgi
 REQUEST_METHOD=GET QUERY_STRING=""bug=775300"" perl cgi/bugreport.cgi; 





Install MTA. See README.mail for details.


Note that each line of /etc/debbugs/Maintainers file needs to be formatted as
follows:
package    maintainer name <email@address>

If you need a template, look in /usr/share/doc/debbugs/examples/ directory.
How do I contribute to Debbugs?
Debbugs bugs
Bugs in debbugs are tracked on the Debian bugtracker. The web interface is available at
bugs.debian.org
Start contributing
Make a working branch for your code and check it out to start working:
    git checkout -b example-branch

Stage and commit your changes using appropriate commit messages
    git add example-file

    git commit -m ""Created an example file to demonstrate basic git commands.""

Submitting a Patch
Submitting a patch can be done using git format-patch.
For example
    git format-patch origin/master

Creates .patch files for all commits since the branch diverged from master.
Debbugs bugs are tracked using debbugs (what else). Patches should therefore be
attached to the bug report for the issue. This can be done by emailing the
.patch files to xxxx@bugs.debian.org (where xxxx is the bug number).
Feature patches can also be emailed to the maintaining list at
Debugs mailing list
Further Information and Assistance
Email


Mailing List debian-debbugs@lists.debian.org


To subscribe to the mailing list, email
debian-debbugs-request@lists.debian.org with the word ""subscribe"" in the
subject line.


Website

Code
Debbugs Team

IRC
Join the #debbugs channel on irc.oftc.net
Developers
This bug tracking system was developed by Ian Jackson from 1994-1997,
with assistance from nCipher Corporation Limited in 1997. nCipher allowed
Ian to redistribute modifications he made to the system while working as an
employee of nCipher.
Since then, it has been developed by the various administrators of
bugs.debian.org, including Darren Benham, Adam Heath, Josip Rodin, Anthony
Towns, and Colin Watson. As in the case of Ian, nCipher allowed Colin to
redistribute modifications he made while working as an employee of nCipher.
Copyright and Lack-of-Warranty Notice

Copyright 1999 Darren O. Benham
Copyright 1994-1997 Ian Jackson
Copyright 1997,2003 nCipher Corporation Limited

This bug system is free software; you can redistribute it and/or modify it
under the terms of the GNU General Public License as published by the Free
Software Foundation; version 2 of the License.
This program and documentation is distributed in the hope that it will be
useful, but without any warranty; without even the implied warranty of
merchantability or fitness for a particular purpose. See the GNU General
Public License for more details.
You should have received a copy of the GNU General Public License along
with this program, or one should be available above; if not, write to the
Free Software Foundation, 59 Temple Place - Suite 330, Boston, MA
02111-1307, USA.
",batch2,8:10:18,Done
71,goldstar-zsy/telegraf,"Telegraf  
Telegraf is an agent written in Go for collecting, processing, aggregating,
and writing metrics.
Design goals are to have a minimal memory footprint with a plugin system so
that developers in the community can easily add support for collecting metrics
from local or remote services.
Telegraf is plugin-driven and has the concept of 4 distinct plugins:

Input Plugins collect metrics from the system, services, or 3rd party APIs
Processor Plugins transform, decorate, and/or filter metrics
Aggregator Plugins create aggregate metrics (e.g. mean, min, max, quantiles, etc.)
Output Plugins write metrics to various destinations

For more information on Processor and Aggregator plugins please read this.
New plugins are designed to be easy to contribute,
we'll eagerly accept pull
requests and will manage the set of plugins that Telegraf supports.
Contributing
There are many ways to contribute:

Fix and report bugs
Improve documentation
Review code and feature proposals
Answer questions on github and on the Community Site
Contribute plugins

Installation:
You can download the binaries directly from the downloads page
or from the releases section.
Ansible Role:
Ansible role: https://github.com/rossmcdonald/telegraf
From Source:
Telegraf requires golang version 1.8+, the Makefile requires GNU make.
Dependencies are managed with gdm,
which is installed by the Makefile if you don't have it already.

Install Go
Setup your GOPATH
Run go get -d github.com/influxdata/telegraf
Run cd $GOPATH/src/github.com/influxdata/telegraf
Run make

Nightly Builds
These builds are generated from the master branch:

telegraf_nightly_amd64.deb
telegraf_nightly_arm64.deb
telegraf-nightly.arm64.rpm
telegraf_nightly_armel.deb
telegraf-nightly.armel.rpm
telegraf_nightly_armhf.deb
telegraf-nightly.armv6hl.rpm
telegraf-nightly_freebsd_amd64.tar.gz
telegraf-nightly_freebsd_i386.tar.gz
telegraf_nightly_i386.deb
telegraf-nightly.i386.rpm
telegraf-nightly_linux_amd64.tar.gz
telegraf-nightly_linux_arm64.tar.gz
telegraf-nightly_linux_armel.tar.gz
telegraf-nightly_linux_armhf.tar.gz
telegraf-nightly_linux_i386.tar.gz
telegraf-nightly_linux_s390x.tar.gz
telegraf_nightly_s390x.deb
telegraf-nightly.s390x.rpm
telegraf-nightly_windows_amd64.zip
telegraf-nightly_windows_i386.zip
telegraf-nightly.x86_64.rpm
telegraf-static-nightly_linux_amd64.tar.gz

How to use it:
See usage with:
./telegraf --help

Generate a telegraf config file:
./telegraf config > telegraf.conf

Generate config with only cpu input & influxdb output plugins defined:
./telegraf --input-filter cpu --output-filter influxdb config

Run a single telegraf collection, outputing metrics to stdout:
./telegraf --config telegraf.conf --test

Run telegraf with all plugins defined in config file:
./telegraf --config telegraf.conf

Run telegraf, enabling the cpu & memory input, and influxdb output plugins:
./telegraf --config telegraf.conf --input-filter cpu:mem --output-filter influxdb

Configuration
See the configuration guide for a rundown of the more advanced
configuration options.
Input Plugins

aerospike
amqp_consumer (rabbitmq)
apache
aws cloudwatch
bcache
bond
cassandra
ceph
cgroup
chrony
consul
conntrack
couchbase
couchdb
DC/OS
disque
dmcache
dns query time
docker
dovecot
elasticsearch
exec (generic executable plugin, support JSON, influx, graphite and nagios)
fail2ban
filestat
fluentd
graylog
haproxy
hddtemp
http (generic HTTP plugin, supports using input data formats)
http_response
httpjson (generic JSON-emitting http service plugin)
internal
influxdb
interrupts
ipmi_sensor
iptables
ipset
jolokia (deprecated, use jolokia2)
jolokia2
kapacitor
kubernetes
leofs
lustre2
mailchimp
memcached
mesos
minecraft
mongodb
mysql
nats
net_response
nginx
nginx_plus
nsq
nstat
ntpq
openldap
opensmtpd
pf
phpfpm
phusion passenger
ping
postfix
postgresql_extensible
postgresql
powerdns
procstat
prometheus (can be used for Caddy server)
puppetagent
rabbitmq
raindrops
redis
rethinkdb
riak
salesforce
sensors
smart
snmp
snmp_legacy
solr
sql server (microsoft)
teamspeak
tomcat
twemproxy
unbound
varnish
zfs
zookeeper
win_perf_counters (windows performance counters)
win_services
sysstat
system

cpu
mem
net
netstat
disk
diskio
swap
processes
kernel (/proc/stat)
kernel (/proc/vmstat)
linux_sysctl_fs (/proc/sys/fs)



Telegraf can also collect metrics via the following service plugins:

http_listener
kafka_consumer
mqtt_consumer
nats_consumer
nsq_consumer
logparser
statsd
socket_listener
tail
tcp_listener
udp_listener
webhooks

filestack
github
mandrill
papertrail
particle
rollbar


zipkin

Telegraf is able to parse the following input data formats into metrics, these
formats may be used with input plugins supporting the data_format option:

InfluxDB Line Protocol
JSON
Graphite
Value
Nagios
Collectd
Dropwizard

Processor Plugins

printer
override

Aggregator Plugins

basicstats
minmax
histogram

Output Plugins

influxdb
amon
amqp (rabbitmq)
aws kinesis
aws cloudwatch
cratedb
datadog
discard
elasticsearch
file
graphite
graylog
instrumental
kafka
librato
mqtt
nats
nsq
opentsdb
prometheus
riemann
riemann_legacy
socket_writer
tcp
udp
wavefront

",batch2,8:10:18,Done
72,hxp-plus/hxp-emacs,,batch1,16:58:59,Done
73,jockej/emacs-mirror1,,batch1,16:59:00,Done
74,amake/emacs,,batch1,16:59:00,Done
75,AltugYildirim/istio,"Istio





An open platform to connect, manage, and secure microservices.

Introduction
Repositories
Issue management

In addition, here are some other documents you may wish to read:

Istio Community - describes how to get involved and contribute to the Istio project
Istio Developer's Guide - explains how to set up and use an Istio development environment
Project Conventions - describes the conventions we use within the code base
Creating Fast and Lean Code - performance-oriented advice and guidelines for the code base

You'll find many other useful documents on our Wiki.
Introduction
Istio is an open platform for providing a uniform way to integrate
microservices, manage traffic flow across microservices, enforce policies
and aggregate telemetry data. Istio's control plane provides an abstraction
layer over the underlying cluster management platform, such as Kubernetes,
Mesos, etc.
Visit istio.io for in-depth information about using Istio.
Istio is composed of these components:


Envoy - Sidecar proxies per microservice to handle ingress/egress traffic
between services in the cluster and from a service to external
services. The proxies form a secure microservice mesh providing a rich
set of functions like discovery, rich layer-7 routing, circuit breakers,
policy enforcement and telemetry recording/reporting
functions.

Note: The service mesh is not an overlay network. It
simplifies and enhances how microservices in an application talk to each
other over the network provided by the underlying platform.



Mixer - Central component that is leveraged by the proxies and microservices
to enforce policies such as authorization, rate limits, quotas, authentication, request
tracing and telemetry collection.


Pilot - A component responsible for configuring the proxies at runtime.


Citadel - A centralized component responsible for certificate issuance and rotation.


Citadel Agent - A per-node component responsible for certificate issuance and rotation.


Galley- Central component for validating, ingesting, aggregating, transforming and distributing config within Istio.


Istio currently supports Kubernetes and Consul-based environments. We plan support for additional platforms such as
Cloud Foundry, and Mesos in the near future.
Repositories
The Istio project is divided across a few GitHub repositories.


istio/istio. This is the main repository that you are
currently looking at. It hosts Istio's core components and also
the sample programs and the various documents that govern the Istio open source
project. It includes:

security. This directory contains security related code,
including Citadel (acting as Certificate Authority), citadel agent, etc.
pilot. This directory
contains platform-specific code to populate the
abstract service model, dynamically reconfigure the proxies
when the application topology changes, as well as translate
routing rules into proxy specific configuration.
istioctl. This directory contains code for the
istioctl command line utility.
mixer. This directory
contains code to enforce various policies for traffic passing through the
proxies, and collect telemetry data from proxies and services. There
are plugins for interfacing with various cloud platforms, policy
management services, and monitoring services.



istio/api. This repository defines
component-level APIs and common configuration formats for the Istio platform.


istio/proxy. The Istio proxy contains
extensions to the Envoy proxy (in the form of
Envoy filters), that allow the proxy to delegate policy enforcement
decisions to Mixer.


Issue management
We use GitHub combined with ZenHub to track all of our bugs and feature requests. Each issue we track has a variety of metadata:


Epic. An epic represents a feature area for Istio as a whole. Epics are fairly broad in scope and are basically product-level things.
Each issue is ultimately part of an epic.


Milestone. Each issue is assigned a milestone. This is 0.1, 0.2, ..., or 'Nebulous Future'. The milestone indicates when we
think the issue should get addressed.


Priority/Pipeline. Each issue has a priority which is represented by the Pipeline field within GitHub. Priority can be one of
P0, P1, P2, or >P2. The priority indicates how important it is to address the issue within the milestone. P0 says that the
milestone cannot be considered achieved if the issue isn't resolved.


We don't annotate issues with Releases; Milestones are used instead. We don't use GitHub projects at all, that
support is disabled for our organization.
",batch2,8:09:15,Done
76,cosmicexplorer/emacs,,batch1,16:59:00,Done
77,bestbat/mva,"Mycroft  

Mycroft это доступный для хакинга и изучения голосовой помощник с открытым исходным кодом.
Table of Contents

Содержание
Подготовка к работе
Запуск Mycroft
Using Mycroft

Home Device and Account Manager
Навыки


За кулисами

Pairing Information
Конфигурация
Using Mycroft Without Home
API Key Services
Using Mycroft behind a proxy

Using Mycroft behind a proxy without authentication
Using Mycroft behind an authenticated proxy




Getting Involved
Ссылки

Getting Started
First, get the code on your system!  The simplest method is via git (git installation instructions):

cd ~/
git clone --depth=1 https://github.com/MycroftAI/mycroft-core.git
cd mycroft-core
bash dev_setup.sh

This script sets up dependencies and a virtualenv.  If running in an environment besides Ubuntu/Debian, Arch or Fedora you may need to manually install packages as instructed by dev_setup.sh.
NOTE: The default branch for this repository is 'dev', which should be considered a work-in-progress. If you want to clone a more stable version, switch over to the 'master' branch.
NOTE: If you are willing to contribute to this project, clone the entire repository by

git clone  https://github.com/MycroftAI/mycroft-core.git
instead of
git clone --depth=1 https://github.com/MycroftAI/mycroft-core.git
which is said above.

Running Mycroft
Mycroft provides start-mycroft.sh to perform common tasks. This script uses a virtualenv created by dev_setup.sh.  Assuming you installed mycroft-core in your home directory run:

cd ~/mycroft-core
./start-mycroft.sh debug

The ""debug"" command will start the background services (microphone listener, skill, messagebus, and audio subsystems) as well as bringing up a text-based Command Line Interface (CLI) you can use to interact with Mycroft and see the contents of the various logs. Alternatively you can run ./start-mycroft.sh all to begin the services without the command line interface.  Later you can bring up the CLI using ./start-mycroft.sh cli.
The background services can be stopped as a group with:

./stop-mycroft.sh

Using Mycroft
Home Device and Account Manager
Mycroft AI, Inc. maintains a device and account management system known as Mycroft Home. Developers may sign up at: https://home.mycroft.ai
By default, mycroft-core  is configured to use Home. By saying ""Hey Mycroft, pair my device"" (or any other request verbal request) you will be informed that your device needs to be paired. Mycroft will speak a 6-digit code which you can entered into the pairing page within the Mycroft Home site.
Once paired, your unit will use Mycroft API keys for services such as Speech-to-Text (STT), weather and various other skills.
Skills
Mycroft is nothing without skills.  There are a handful of default skills that are downloaded automatically to your /opt/mycroft/skills directory, but most need to be installed explicitly.  See the Skill Repo to discover skills made by others.  And please share your own interesting work!
Behind the scenes
Pairing Information
Pairing information generated by registering with Home is stored in:
~/.mycroft/identity/identity2.json <-- DO NOT SHARE THIS WITH OTHERS!
Configuration
Mycroft configuration consists of 4 possible locations:

mycroft-core/mycroft/configuration/mycroft.conf(Defaults)
Mycroft Home (Remote)
/etc/mycroft/mycroft.conf(Machine)
$HOME/.mycroft/mycroft.conf(User)

When the configuration loader starts, it looks in these locations in this order, and loads ALL configurations. Keys that exist in multiple configuration files will be overridden by the last file to contain the value. This process results in a minimal amount being written for a specific device and user, without modifying default distribution files.
Using Mycroft Without Home
If you do not wish to use the Mycroft Home service, you may insert your own API keys into the configuration files listed below in configuration.
The place to insert the API key looks like the following:
[WeatherSkill]
api_key = """"
Put a relevant key inside the quotes and mycroft-core should begin to use the key immediately.
API Key Services
These are the keys currently used in Mycroft Core:

STT API, Google STT, Google Cloud Speech
Weather Skill API, OpenWeatherMap
Wolfram-Alpha Skill

Using Mycroft behind a proxy
Many schools, universities and workplaces run a proxy on their network. If you need to type in a username and password to access the external internet, then you are likely behind a proxy.
If you plan to use Mycroft behind a proxy, then you will need to do an additional configuration step.
NOTE: In order to complete this step, you will need to know the hostname and port for the proxy server. Your network administrator will be able to provide these details. Your network administrator may want information on what type of traffic Mycroft will be using. We use https traffic on port 443, primarily for accessing ReST-based APIs.
Using Mycroft behind a proxy without authentication
If you are using Mycroft behind a proxy without authentication, add the following environment variables, changing the proxy_hostname.com and proxy_port for the values for your network. These commands are executed from the Linux command line interface (CLI).
$ export http_proxy=http://proxy_hostname.com:proxy_port
$ export https_port=http://proxy_hostname.com:proxy_port
$ export no_proxy=""localhost,127.0.0.1,localaddress,.localdomain.com,0.0.0.0,::1""
Using Mycroft behind an authenticated proxy
If  you are behind a proxy which requires authentication, add the following environment variables, changing the proxy_hostname.com and proxy_port for the values for your network. These commands are executed from the Linux command line interface (CLI).
$ export http_proxy=http://user:password@proxy_hostname.com:proxy_port
$ export https_port=http://user:password@proxy_hostname.com:proxy_port
$ export no_proxy=""localhost,127.0.0.1,localaddress,.localdomain.com,0.0.0.0,::1""
Getting Involved
This is an open source project and we would love your help. We have prepared a contributing guide to help you get started.
If this is your first PR or you're not sure where to get started,
say hi in Mycroft чат and a team member would be happy to mentor you.
Join the Mycroft форум for questions and answers.
Links

Создание навыка
Documentation
Release Notes
Mycroft чат
Mycroft форум
Mycroft блог

",batch1,16:57:57,Done
78,PureSolTechnologies/Purifinity,"
Purifinity
Purifinty is a source code and architecture analysis tool and framework for complex analysis of large code bases.
General information about the project can be found at:
http://purifinity.com
Also available are (in very early stage):

Installation Manual
User's Manual
Development Manual

Please, use the issue tracker at:
https://bugs.puresol-technologies.net/projects/purifinity

",batch2,8:10:18,Done
79,bigearth/clone-marlin,"Marlin Firmware for Clone
More info at clone.earth.

This is Marlin 1.1.x Firmware which has been optimized for the Clone's hardware.
Much love to the Marlin folks. Thanks for the great work!
Changes
Configuration.h

STRING_CONFIG_H_AUTHOR
STRING_SPLASH_LINE1
STRING_SPLASH_LINE2
TEMP_SENSOR_BED
HEATER_0_MAXTEMP
BED_MAXTEMP
PREVENT_COLD_EXTRUSION
EXTRUDE_MINTEMP
ENDSTOPPULLUPS
X_MIN_ENDSTOP_INVERTING
Y_MIN_ENDSTOP_INVERTING
FIX_MOUNTED_PROBE
X_PROBE_OFFSET_FROM_EXTRUDER
INVERT_X_DIR
INVERT_Z_DIR
GRID_MAX_POINTS_X
Z_MIN_ENDSTOP_INVERTING
Z_MIN_PROBE_ENDSTOP_INVERTING
AUTO_BED_LEVELING_BILINEAR
LEFT_PROBE_BED_POSITION
RIGHT_PROBE_BED_POSITION
BACK_PROBE_BED_POSITION
X_MAX_ENDSTOP_INVERTING
Y_MAX_ENDSTOP_INVERTING
Z_MAX_ENDSTOP_INVERTING
Z_CLEARANCE_BETWEEN_PROBES
DEFAULT_AXIS_STEPS_PER_UNIT

Disabled the following

MIN_SOFTWARE_ENDSTOPS
MAX_SOFTWARE_ENDSTOPS

Calibration steps

G28


Home X, Y and Z


G29


Bed Leveling Routine


G1 X50 Y30 F5000


Move to a good place for the next steps


G1 Z0


Move the Z Axis to it’s absolute 0 point


G92 Z10


Tell the Z it’s now at 10mm, even though it won’t move
Manually move to where paper slides under extruder tip


M114 Z


Gets the axis current values
Take the Z value reported and plug it into this equation (Offset = 10 - Z + 0.1)


M851 Z-Offset


Note this means you would type the letter Z then a minus sign, and then the value of ‘Offset’ from the previous step: Ex: M851 Z-0.7


M500


Save to EEPROM

",batch2,8:09:15,Done
80,mshbeab/anter,"Read Me !
",batch2,8:09:15,Done
81,joseluis2g/forgottenkarmia,"forgottenkarmia   
The Forgotten Karmia is a free and open-source MMORPG server emulator written in C++. It is a fork of the OpenTibia Server project. To connect to the server, you can use OTClient or OpenTibiaUnity.
Getting Started

Compiling, alternatively download AppVeyor builds for Windows
Scripting Reference

Support
If you need help, please visit the support forum on OTLand. Our issue tracker is not a support forum, and using it as one will result in your issue being closed. If you were unable to get assistance in the support forum, you should consider becoming a premium user on OTLand which grants you access to the premium support forum and supports OTLand financially.
Issues
We use the issue tracker on GitHub. Keep in mind that everyone who is watching the repository gets notified by e-mail when there is activity, so be thoughtful and avoid writing comments that aren't meaningful for an issue (e.g. ""+1""). If you'd like for an issue to be fixed faster, you should either fix it yourself and submit a pull request, or place a bounty on the issue.
",batch2,8:10:17,Done
82,drewc/plowshare,,batch2,8:09:16,Done
83,kkman2008/coder-Leetcode,"LeetCode   
If you like this project, please leave me a star. ★

""For coding interview preparation, LeetCode is one of the best online resource providing a rich library of more than 300 real coding interview questions for you to practice from using one of the 7 supported languages - C, C++, Java, Python, C#, JavaScript, Ruby.""

Algorithms



#
Title
Solutions
Time
Space
Video
Difficulty
Tag




1282
Group the People Given the Group Size They Belong To
Solution



Medium



1281
Subtract the Product and Sum of Digits of an Integer
Solution



Easy



1277
Count Square Submatrices with All Ones
Solution



Medium



1271
Hexspeak
Solution



Easy



1267
Count Servers that Communicate
Solution



Medium



1266
Minimum Time Visiting All Points
Solution



Easy



1260
Shift 2D Grid
Solution


📺
Easy



1252
Cells with Odd Values in a Matrix
Solution
O(m*n + k)
O(m*n)

Easy



1237
Find Positive Integer Solution for a Given Equation
Solution



Easy



1243
Array Transformation
Solution


📺
Easy



1232
Check If It Is a Straight Line
Solution



Easy



1228
Missing Number In Arithmetic Progression
Solution



Easy



1217
Play with Chips
Solution



Easy



1213
Intersection of Three Sorted Arrays
Solution


📺
Easy



1207
Unique Number of Occurrences
Solution
O(n)
O(1)
📺
Easy



1200
Minimum Absolute Difference
Solution


📺
Easy



1198
Find Smallest Common Element in All Rows
Solution


📺
Easy



1196
How Many Apples Can You Put into the Basket
Solution



Easy



1189
Maximum Number of Balloons
Solution



Easy



1185
Day of the Week
Solution



Easy



1184
Distance Between Bus Stops
Solution


📺
Easy



1165
Single-Row Keyboard
Solution



Easy



1160
Find Words That Can Be Formed by Characters
Solution
O(n)
O(m)

Easy



1154
Day of the Year
Solution
O(1)
O(1)

Easy



1150
Check If a Number Is Majority Element in a Sorted Array
Solution



Easy



1137
N-th Tribonacci Number
Solution
O(n)
O(n)

Easy



1134
Armstrong Number
Solution


📺
Easy



1133
Largest Unique Number
Solution



Easy



1128
Number of Equivalent Domino Pairs
Solution


📺
Easy



1122
Relative Sort Array
Solution
O(n)
O(n)

Easy



1170
Compare Strings by Frequency of the Smallest Character
Solution



Easy



1119
Remove Vowels from a String
Solution


📺
Easy



1108
Defanging an IP Address
Solution


📺
Easy



1099
Two Sum Less Than K
Solution


📺
Easy



1089
Duplicate Zeros
Solution
O(n^2)
O(1)

Easy



1086
High Five
Solution


📺
Easy



1085
Sum of Digits in the Minimum Number
Solution


📺
Easy



1079
Letter Tile Possibilities
Solution
O(1)
O(1)

Medium



1078
Occurrences After Bigram
Solution
O(n)
O(1)

Easy



1071
Greatest Common Divisor of Strings
Solution
O(m*n)
O(1)

Easy



1065
Index Pairs of a String
Solution
O(nlogn)
O(1)

Medium



1056
Confusing Number
Solution
O(n)
O(1)

Easy



1055
Fixed Point
Solution
O(n)
O(1)

Easy



1051
Height Checker
Solution
O(nlogn)
O(1)

Easy



1047
Remove All Adjacent Duplicates In String
Solution
O(n)
O(1)

Easy



1043
Partition Array for Maximum Sum
Solution
O(nk)
O(n)

Medium
DP


1038
Binary Search Tree to Greater Sum Tree
Solution
O(n)
O(n)

Medium
DFS, tree


1037
Valid Boomerang
Solution
O(1)
O(1)

Easy
Math


1033
Moving Stones Until Consecutive
Solution
O(1)
O(1)

Easy
Math


1030
Matrix Cells in Distance Order
Solution
O(R*C)
O(1)

Easy



1029
Two City Scheduling
Solution
O(nlogn)
O(1)

Easy



1022
Sum of Root To Leaf Binary Numbers
Solution
O(n)
O(n)

Easy



1021
Remove Outermost Parentheses
Solution
O(n)
O(n)

Easy



1020
Number of Enclaves
Solution
O(mn)
O(mn)

Medium
Graph, DFS, BFS, recursion


1018
Binary Prefix Divisible By 5
Solution
O(n)
O(1)

Easy



1014
Best Sightseeing Pair
Solution
O(n)
O(1)

Medium



1013
Partition Array Into Three Parts With Equal Sum
Solution
O(n)
O(1)

Easy



1011
Capacity To Ship Packages Within D Days
Solution
O(nlogn)
O(1)

Medium
Binary Search


1010
Pairs of Songs With Total Durations Divisible by 60
Solution
O(n)
O(1)

Easy



1009
Complement of Base 10 Integer
Solution
O(n)
O(1)

Easy



1008
Construct Binary Search Tree from Preorder Traversal
Solution
O(n)
O(1)

Medium
Recursion


1003
Check If Word Is Valid After Substitutions
Solution
O(n)
O(n)

Medium



1002
Find Common Characters
Solution
O(n)
O(1)

Easy



999
Available Captures for Rook
Solution
O(1)
O(1)

Easy



997
Find the Town Judge
Solution
O(n)
O(n)

Easy



994
Rotting Oranges
Solution
O(m2n*2)
O(m*n)

Easy
BFS


993
Cousins in Binary Tree
Solution
O(n)
O(m) (m is length of the nodes that has the max number of nodes on the same level)

Easy
Tree, BFS


989
Add to Array-Form of Integer
Solution
O(max(N, logk))
O(max(N, logk))

Easy
Array


987
Vertical Order Traversal of a Binary Tree
Solution
O(n)
O(n)

Medium
Recursion


985
Sum of Even Numbers After Queries
Solution
O(n)
O(n)

Easy
Array


979
Distribute Coins in Binary Tree
Solution
O(n)
O(1)

Medium
Recursion


977
Squares of a Sorted Array
Solution
O(nlogn)
O(1)

Easy
Array


976
Largest Perimeter Triangle
Solution
O(nlogn)
O(1)

Easy
Math Array


974
Subarray Sums Divisible by K
Solution
O(n)
O(n)

Medium
Array


973
K Closest Points to Origin
Solution
O(nlogn)
O(K)

Easy
Math Sort


970
Powerful Integers
Solution
O(?)
O(1)

Easy
Math


966
Vowel Spellchecker
Solution
O(hlogn)
O(n)

Medium
Hash Table, String


965
Univalued Binary Tree
Solution
O(n)
O(h)

Easy
DFS, recursion


961
N-Repeated Element in Size 2N Array
Solution
O(n)
O(1)

Easy



954
Array of Doubled Pairs
Solution



Medium



953
Verifying an Alien Dictionary
Solution
O(1)
O(1)

Easy



951
Flip Equivalent Binary Trees
Solution
O(n)
O(h)

Medium
Tree, DFS, recursion


950
Reveal Cards In Increasing Order
Solution
O(nlogn)
O(n)

Medium



944
Delete Columns to Make Sorted
Solution
O(n)
O(1)

Easy



942
DI String Match
Solution
O(n)
O(n)

Easy



941
Valid Mountain Array
Solution
O(n)
O(1)

Easy



938
Range Sum of BST
Solution
O(n)
O(n)

Medium
BST, recursion, DFS


937
Reorder Log Files
Solution
O(n)
O(n)

Easy



935
Knight Dialer
Solution
O(n)
O(1)

Medium



933
Number of Recent Calls
Solution
O(n)
O(n)

Easy



929
Unique Email Addresses
Solution
O(n)
O(n)

Easy



925
Long Pressed Name
Solution
O(n)
O(1)

Easy



922
Sort Array By Parity II
Solution
O(n)
O(1)

Easy



917
Reverse Only Letters
Solution
O(n)
O(n)

Easy



914
X of a Kind in a Deck of Cards
Solution
O(n)
O(n)

Easy



912
Sort an Array
Solution
O(nlogn)
O(1)

Easy



908
Smallest Range I
Solution
O(n)
O(1)

Easy



900
RLE Iterator
Solution
O(n)
O(1)

Medium



897
Increasing Order Search Tree
Solution
O(n)
O(n)

Easy
DFS, recursion


896
Monotonic Array
Solution
O(n)
O(1)

Easy



890
Find and Replace Pattern
Solution
O(n)
O(1)

Medium



888
Fair Candy Swap
Solution
O(n)
O(1)

Easy



885
Spiral Matrix III
Solution



Medium



884
Uncommon Words from Two Sentences
Solution
O(n)
O(k)

Easy



876
Middle of the Linked List
Solution
O(n)
O(1)

Easy



872
Leaf-Similar Trees
Solution
O(n)
O(h)

Easy
DFS, recursion


868
Binary Gap
Solution
O(n)
O(n)

Easy



867
Transpose Matrix
Solution
O(r*c)
O(r*c)

Easy



860
Lemonade Change
Solution
O(n)
O(1)

Easy



859
Buddy Strings
Solution
O(n)
O(n)

Easy



852
Peak Index in a Mountain Array
Solution
O(n)
O(1)

Easy



849
Maximize Distance to Closest Person
Solution
O(n^2)
O(1)

Easy



844
Backspace String Compare
Solution
O(n)
O(1)

Easy



840
Magic Squares In Grid
Solution
O(1)
O(1)

Easy



832
Flipping an Image
Solution
O(n)
O(1)

Easy



830
Positions of Large Groups
Solution
O(n)
O(n)

Easy



824
Goat Latin
Solution
O(n)
O(1)

Easy



821
Shortest Distance to a Character
Solution
O(n)
O(k) (k is the number of char C in S)

Easy



819
Most Common Word
Solution
O(m+n)
O(n)

Easy
HashMap


814
Binary Tree Pruning
Solution
O(n)
O(n)

Medium
recursion, DFS


811
Subdomain Visit Count
Solution
O(n)
O(n)

Easy
HashMap


806
Number of Lines To Write String
Solution
O(n)
O(1)

Easy



804
Unique Morse Code Words
Solution
O(S)
O(S)

Easy



800
Similar RGB Color
Solution
O(1)
O(1)

Easy



799
Champagne Tower
Solution
O(r^2) or O(1)
O(r^2) or O(1)

Medium



796
Rotate String
Solution
O(n)
O(1)

Easy



791
Custom Sort String
Solution
O(n+m)
O(1)

Medium



789
Escape The Ghosts
Solution
O(n)
O(1)

Medium
Math


788
Rotated Digits
Solution
O(n*m)
O(1)

Easy



784
Letter Case Permutation
Solution
O(n*2^n)
O(n*2^n)

Easy



783
Minimum Distance Between BST Nodes
Solution
O(n)
O(h)

Easy



779
K-th Symbol in Grammar
Solution
O(logn)
O(1)

Medium



776
Split BST
Solution
O(n)
O(n)

Medium
Recursion


771
Jewels and Stones
Solution
O(n)
O(m)

Easy



769
Max Chunks To Make Sorted
Solution
O(n)
O(1)

Medium
Array


767
Reorganize String
Solution
O(klogk) k is the number of unique characters in given String
O(k)

Medium



766
Toeplitz Matrix
Solution
O(m*n)
O(1)

Easy



765
Couples Holding Hands
Solution
O(n^2)
O(1)

Hard



764
Largest Plus Sign
Solution
O(n^2)
O(n^2)

Medium
DP


763
Partition Labels
Solution
O(n)
O(1)

Medium



762
Prime Number of Set Bits in Binary Representation
Solution
O(n)
O(1)

Easy



760
Find Anagram Mappings
Solution
O(n^2)
O(1)

Easy



758
Bold Words in String
Solution
O(n*k)
O(n

Easy



756
Pyramid Transition Matrix
Solution
O(?)
O(?)

Medium
Backtracking


755
Pour Water
Solution
O(V*N)
O(1)

Medium
Array


754
Reach a Number
Solution
O(n)
O(1)

Medium
Math


750
Number Of Corner Rectangles
Solution
O((m*n)^2)
O(1)

Medium



748
Shortest Completing Word
Solution
O(n)
O(1)

Easy



747
Largest Number Greater Than Twice of Others
Solution
O(n)
O(1)

Easy



746
Min Cost Climbing Stairs
Solution
O(n)
O(1)

Easy



744
Find Smallest Letter Greater Than Target
Solution
O(logn)
O(1)

Easy



743
Network Delay Time
Solution
O(n^2 + e)
O(n^2)

Medium
Graph, Djikstra


740
Delete and Earn
Solution
O(n)
O(n)

Medium



739
Daily Temperatures
Solution
O(n^2)
O(1)

Medium



738
Monotone Increasing Digits
Solution
O(n)
O(1)

Medium



737
Sentence Similarity II
Solution
O(nlogk + k) n is the length of max(words1, words2), k is the length of pairs
O(k)

Medium
Union Find


735
Asteroid Collision
Solution
O(n)
O(n)

Medium
Stack


734
Sentence Similarity
Solution
O(n*k)
O(1)

Easy
HashTable


733
Flood Fill
Solution
O(m*n)
O(m*n)

Easy
BFS, DFS


729
My Calendar I
Solution
O(n)
O(n)

Medium



728
Self Dividing Numbers
Solution
O(n*k) k is the average number of digits of each number in the given array
O(1)

Easy



727
Minimum Window Subsequence
Solution
O(m*n)
O(m*n)

Hard
DP


725
Split Linked List in Parts
Solution
O(n+k)
O(k)

Medium
LinkedList


724
Find Pivot Index
Solution
O(n)
O(1)

Easy
Array


723
Candy Crush
Solution
O((r*c)^2)
O((r*c))

Medium
Array, Two Pointers


721
Accounts Merge
Solution



Medium
DFS, Union Find


720
Longest Word in Dictionary
Solution
O(∑wi) where wi is the length of words[i]
O(∑wi) where wi is the length of words[i]

Easy
Trie


719
Find K-th Smallest Pair Distance
Solution
O(nlogw + nlogn)
O(1)

Hard
Binary Search


718
Maximum Length of Repeated Subarray
Solution
O(m*n)
O(m*n)

Medium
DP


717
1-bit and 2-bit Characters
Solution
O(n)
O(1)

Easy



716
Max Stack
Solution
O(logn)
O(n)

Hard
Design


714
Best Time to Buy and Sell Stock with Transaction Fee
Solution
O(n)
O(1)

Medium
DP


713
Subarray Product Less Than K
Solution
O(n)
O(1)

Medium



712
Minimum ASCII Delete Sum for Two Strings
Solution
O(m*n)
O(m*n)

Medium
DP


709
To Lower Case
Solution
O(n)
O(1)

Easy
String


706
Design HashMap
Solution
O(n)
O(n)

Easy
Design


705
Design HashSet
Solution
O(1)
O(n)

Easy
Design


704
Binary Search
Solution
O(logn)
O(1)

Easy
Binary Search


703
Kth Largest Element in a Stream
Solution
O(logn)
O(n)

Easy



701
Insert into a Binary Search Tree
Solution
O(n)
O(h)

Medium
DFS, recursion


700
Search in a Binary Search Tree
Solution
O(n)
O(h)

Easy
recusion, dfs


699
Falling Squares
Solution
O(n^2)
O(n)

Hard
Segment Tree


698
Partition to K Equal Sum Subsets
Solution
O(n*(2^n))
O(2^n)

Medium
Backtracking


697
Degree of an Array
Solution
O(n)
O(n)

Easy



696
Count Binary Substrings
Solution
O(n)
O(n)

Easy



695
Max Area of Island
Solution
O(m*n)
O(1)

Easy
DFS


694
Number of Distinct Islands
Solution
O(m*n)
O(1)

Medium
DFS


693
Binary Number with Alternating Bits
Solution
O(n)
O(1)

Easy



692
Top K Frequent Words
Solution
O(nlogk)
O(n)

Medium



691
Stickers to Spell Word
Solution
O(?)
O(?)

Hard
DP


690
Employee Importance
Solution
O(n)
O(h)

Easy
DFS


689
Maximum Sum of 3 Non-Overlapping Subarrays
Solution
O(n)
O(n)

Hard
DP


688
Knight Probability in Chessboard
Solution
O(n^2)
O(n^2)

Medium
DP


687
Longest Univalue Path
Solution
O(n)
O(h)

Easy
DFS


686
Repeated String Match
Solution
O(n*(m+n))
O(m+n)

Easy



685
Redundant Connection II
Solution
O(n)
O(n)

Hard
Union Find


684
Redundant Connection
Solution
O(n)
O(n)

Medium
Union Find


683
K Empty Slots
Solution
O(n)
O(n)

Hard



682
Baseball Game
Solution
O(n)
O(1)

Easy



681
Next Closest Time
Solution
O(1)
O(1)

Medium



680
Valid Palindrome II
Solution
O(n)
O(1)

Easy
String


679
24 Game
Solution
O(1) (Upper bound 9216)
O(1)

Hard
Recursion


678
Valid Parenthesis String
Solution
O(n)
O(1)

Medium
Recursion, Greedy


677
Map Sum Pairs
Solution
O(n)
O(n)

Medium
HashMap


676
Implement Magic Dictionary
Solution
O(n^2)
O(n)

Medium



675
Cut Off Trees for Golf Event
Solution
O((m*n)^2)
O(m*n)

Hard
BFS


674
Longest Continuous Increasing Subsequence
Solution
O(n^2)
O(1)

Easy



673
Number of Longest Increasing Subsequence
Solution
O(n^2)
O(n)

Medium
DP


672
Bulb Switcher II
Solution
O(1)
O(1)

Medium
Math


671
Second Minimum Node In a Binary Tree
Solution
O(n)
O(n)

Easy
Tree, DFS


670
Maximum Swap
Solution
O(n^2)
O(1)

Medium
String


669
Trim a Binary Search Tree
Solution
O(n)
O(1)

Easy
Tree, DFS


668
Kth Smallest Number in Multiplication Table
Solution
O(logm*n)
O(1)

Hard
Binary Search


667
Beautiful Arrangement II
Solution
O(n)
O(1)

Medium
Array


666
Path Sum IV
Solution
O(1)
O(1)

Medium
Tree, DFS


665
Non-decreasing Array
Solution
O(n)
O(n)

Easy



664
Strange Printer
Solution
O(n^3)
O(n^2)

Hard
DP


663
Equal Tree Partition
Solution
O(n)
O(n)

Medium
Tree


662
Maximum Width of Binary Tree
Solution
O(n)
O(k)

Medium
BFS, DFS


661
Image Smoother
Solution
O(m*n)
O(1)

Easy
Array


660
Remove 9
Solution
O(n)
O(1)

Hard
Math


659
Split Array into Consecutive Subsequences
Solution
O(n)
O(n)

Medium
HashMap


658
Find K Closest Elements
Solution
O(n)
O(1)

Medium



657
Judge Route Circle
Solution
O(n)
O(1)

Easy



656
Coin Path
Solution
O(n*B)
O(n)

Hard
DP


655
Print Binary Tree
Solution
O(h*2^h)
O(h*2^h)

Medium
Recursion


654
Maximum Binary Tree
Solution
O(n)
O(n)

Medium
Tree


653
Two Sum IV - Input is a BST
Solution



Easy
Tree


652
Find Duplicate Subtrees
Solution
O(n)
O(n)

Medium
Tree


651
4 Keys Keyboard
Solution
O(n^2)
O(n)

Medium
DP


650
2 Keys Keyboard
Solution
O(n^2)
O(n)

Medium
DP


649
Dota2 Senate
Solution
O(n)
O(n)

Medium
Greedy


648
Replace Words
Solution
O(n)
O(n)

Medium
Trie


647
Palindromic Substrings
Solution
O(n^2)
O(1)

Medium
DP


646
Maximum Length of Pair Chain
Solution
O(nlogn)
O(1)

Medium
DP, Greedy


645
Set Mismatch
Solution
O(nlogn)
O(1)

Easy



644
Maximum Average Subarray II
Solution

O(1)

Hard
Binary Search


643
Maximum Average Subarray I
Solution
O(n)
O(1)

Easy



642
Design Search Autocomplete System
Solution
O(n)
O(n)

Hard
Design


640
Solve the Equation
Solution
O(n)
O(n)

Medium



639
Decode Ways II
Solution
O(n)
O(n)

Hard
DP


638
Shopping Offers
Solution
O(2^n)
O(n)

Medium
DP, DFS


637
Average of Levels in Binary Tree
Solution
O(n)
O(1)

Easy



636
Exclusive Time of Functions
Solution
O(n)
O(n/2)

Medium
Stack


635
Design Log Storage System
Solution
O(n)
O(n)

Medium
Design


634
Find the Derangement of An Array
Solution
O(n)
O(1)

Medium
Math


633
Sum of Square Numbers
Solution
O(logn)
O(1)

Easy
Binary Search


632
Smallest Range
Solution
O(n*logk)
O(k)

Hard
Heap


631
Design Excel Sum Formula
Solution



Hard
Design, Topological Sort


630
Course Schedule III
Solution
O(n*logn)
O(n)

Hard
Heap, Greedy


629
K Inverse Pairs Array
Solution
O(n*k)
O(n*k)

Hard
DP


628
Maximum Product of Three Numbers
Solution
O(nlogn)
O(1)

Easy



625
Minimum Factorization
Solution
O(?)
O(?)

Medium



624
Maximum Distance in Arrays
Solution
O(nlogn)
O(1)

Easy
Sort, Array


623
Add One Row to Tree
Solution
O(n)
O(h)

Medium
Tree


621
Task Scheduler
Solution
O(n)
O(26)

Medium
Greedy, Queue


617
Merge Two Binary Trees
Solution
O(n)
O(h)

Easy
Tree, Recursion


616
Add Bold Tag in String
Solution
O(n*k) (n is length of string, k is size of dict)
O(n)

Medium
String


611
Valid Triangle Number
Solution
O(n^2logn)
O(logn)

Medium
Binary Search


609
Find Duplicate File in System
Solution
O(n*x) (x is the average length of each string)
O(n*x)

Medium
HashMap


606
Construct String from Binary Tree
Solution
O(n)
O(n)

Easy
Tree, Recursion


605
Can Place Flowers
Solution
O(n)
O(1)

Easy
Array


604
Design Compressed String Iterator
Solution
O(n)
O(n)

Easy
Design, String


600
Non-negative Integers without Consecutive Ones
Solution
O(log2(max_int) = 32)
O(log2(max_int) = 32)

Hard
Bit Manipulation, DP


599
Minimum Index Sum of Two Lists
Solution
O(max(m,n))
O(max(m,n))

Easy
HashMap


598
Range Addition II
Solution
O(x) (x is the number of operations)
O(1)

Easy



594
Longest Harmonious Subsequence
Solution
O(n)
O(n)

Easy
Array, HashMap


593
Valid Square
Solution
O(1)
O(1)

Medium
Math


592
Fraction Addition and Subtraction
Solution
O(nlogx)
O(n)

Medium
Math


591
Tag Validator
Solution
O(n)
O(n)

Hard
Stack, String


590
N-ary Tree Postorder Traversal
Solution
O(n)
O(n)

Easy
DFS, recursion


589
N-ary Tree Preorder Traversal
Solution
O(n)
O(n)

Easy
DFS, recursion


588
Design In-Memory File System
Solution
O(n)
O(h)

Hard
Trie, Design


587
Erect the Fence
Solution
O(?)
O(?)

Hard
Geometry


583
Delete Operation for Two Strings
Solution
O(m*n)
O(m*n) could be optimized to O(n)

Medium
DP


582
Kill Process
Solution
O(n)
O(h)

Medium
Stack


581
Shortest Unsorted Continuous Subarray
Solution
O(n)
O(1)

Easy
Array, Sort


576
Out of Boundary Paths
Solution
O(Nmn)
O(m*n)

Hard
DP, DFS


575
Distribute Candies
Solution
O(nlogn)
O(1)

Easy
Array


573
Squirrel Simulation
Solution
O(n)
O(1)

Medium
Math


572
Subtree of Another Tree
Solution
O(m*n)
O(1)

Easy
Tree


568
Maximum Vacation Days
Solution
O(n^2*k)
O(n*k)

Hard
DP


567
Permutation in String
Solution
O(l1 + 26*(l2 - l1))
O(1)

Medium
Sliding Windows, Two Pointers


566
Reshape the Matrix
Solution
O(m*n)
O(1)

Easy



565
Array Nesting
Solution
O(n)
O(n)

Medium



563
Binary Tree Tilt
Solution
O(n)
O(n)

Easy
Tree Recursion


562
Longest Line of Consecutive One in Matrix
Solution
O(m*n)
O(m*n)

Medium
Matrix DP


561
Array Partition I
Solution
O(nlogn)
O(1)

Easy
Array


560
Subarray Sum Equals K
Solution
O(n)
O(n)

Medium
Array, HashMap


559
Maximum Depth of N-ary Tree
Solution
O(n)
O(n)

Easy
DFS, recursion


557
Reverse Words in a String III
Solution
O(n)
O(n)

Easy
String


556
Next Greater Element III
Solution
O(n)
O(1)

Medium
String


555
Split Concatenated Strings
Solution
O(n^2)
O(n)

Medium
String


554
Brick Wall
Solution
O(n) (n is total number of bricks in the wall)
O(m) (m is width of the wall)

Medium
HashMap


553
Optimal Division
Solution
O(n)
O(n)

Medium
String, Math


552
Student Attendance Record II
Solution
O(n)
O(1)

Hard
DP


551
Student Attendance Record I
Solution
O(n)
O(1)

Easy
String


549
Binary Tree Longest Consecutive Sequence II
Solution
O(n)
O(n)

Medium
Tree


548
Split Array with Equal Sum
Solution
O(n^2)
O(n)

Medium
Array


547
Friend Circles
Solution
O(n^2)
O(n)

Medium
Union Find


546
Remove Boxes
Solution
O(n^3)
O(n^3)

Hard
DFS, DP


545
Boundary of Binary Tree
Solution
O(n)
O(n)

Medium
Recursion


544
Output Contest Matches
Solution
O(n)
O(n)

Medium
Recursion


543
Diameter of Binary Tree
Solution
O(n)
O(h)

Easy
Tree/DFS/Recursion


542
01 Matrix
Solution
O(m*n)
O(n)

Medium
BFS


541
Reverse String II
Solution
O(n)
O(1)

Easy
String


540
Single Element in a Sorted Array
Solution
O(n)
O(1)

Medium



539
Minimum Time Difference
Solution
O(logn)
O(1)

Medium
String


538
Convert BST to Greater Tree
Solution
O(n)
O(h)

Easy
Tree


537
Complex Number Multiplication
Solution
O(1)
O(1)

Medium
Math, String


536
Construct Binary Tree from String
Solution
O(n)
O(h)

Medium
Recursion, Stack


535
Encode and Decode TinyURL
Solution
O(1)
O(n)

Medium
Design


533
Lonely Pixel II
Solution
O(m*n)
O(m) (m is number of rows)

Medium
HashMap


532
K-diff Pairs in an Array
Solution
O(n)
O(n)

Easy
HashMap


531
Lonely Pixel I
Solution
O(m*n)
O(1)

Medium



530
Minimum Absolute Difference in BST
Solution
O(n)
O(n)

Easy
DFS


529
Minesweeper
Solution
O(m*n)
O(k)

Medium
BFS


527
Word Abbreviation
Solution
O(n^2)
O(n)

Hard



526
Beautiful Arrangement
Solution
O(n)
O(h)

Medium
Backtracking


525
Contiguous Array
Solution
O(n)
O(n)

Medium
HashMap


524
Longest Word in Dictionary through Deleting
Solution
O(n)
O(n)

Medium
Sort


523
Continuous Subarray Sum
Solution
O(n)
O(1)

Medium
DP


522
Longest Uncommon Subsequence II
Solution
O(x*n^2) (x is average length of strings)
O(1)

Medium



521
Longest Uncommon Subsequence I
Solution
O(max(x,y)) (x and y are length of strings)
O(1)

Easy



520
Detect Capital
Solution
O(n)
O(1)

Easy



517
Super Washing Machines
Solution



Hard
DP


516
Longest Palindromic Subsequence
Solution
O(n^2)
O(n^2)

Medium
DP


515
Find Largest Value in Each Tree Row
Solution
O(n)
O(k)

Medium
BFS


514
Freedom Trail
Solution
O(?)
O(?)

Hard
DP


513
Find Bottom Left Tree Value
Solution
O(n)
O(k)

Medium
BFS


509
Fibonacci Number
Solution


📺
Easy
Array


508
Most Frequent Subtree Sum
Solution
O(n)
O(n)

Medium
DFS, Tree


507
Perfect Number
Solution
O(sqrt(n))
O(1)

Easy
Math


506
Relative Ranks
Solution
O(nlogn)
O(n)

Easy



505
The Maze II
Solution
O(m*n)
O(m*n)

Medium
BFS


504
Base 7
Solution
O(1)
O(1)

Easy



503
Next Greater Element II
Solution
O(n)
O(n)

Medium
Stack


502
IPO
Solution
O(nlogn)
O(n)

Hard
Heap, Greedy


501
Find Mode in Binary Tree
Solution
O(n)
O(k)

Easy
Binary Tree


500
Keyboard Row
Solution
O(n)
O(1)

Easy



499
The Maze III
Solution
O(m*n)
O(m*n)

Hard
BFS


496
Next Greater Element I
Solution
O(n*m)
O(1)

Easy



498
Diagonal Traverse
Solution
O(m*n)
O(1)

Medium



495
Teemo Attacking
Solution
O(n)
O(1)

Medium
Array


494
Target Sum
Solution
O(2^n)
O(1)

Medium



493
Reverse Pairs
Solution
O(nlogn)
O(1)

Hard
Recursion


492
Construct the Rectangle
Solution
O(n)
O(1)

Easy
Array


491
Increasing Subsequences
Solution
O(n!)
O(n)

Medium
Backtracking, DFS


490
The Maze
Solution
O(m*n)
O(m*n)

Medium
BFS


488
Zuma Game
Solution
O(?)
O(?)

Hard
DFS, Backtracking


487
Max Consecutive Ones II
Solution
O(n)
O(n)

Medium
Array


486
Predict the Winner
Solution
O(2^n)
O(n^2)

Medium
DP


485
Max Consecutive Ones
Solution
O(n)
O(1)

Easy
Array


484
Find Permutation
Solution
O(n)
O(1)

Medium
Array, String, Greedy


483
Smallest Good Base
Solution
O(logn)
O(1)

Hard
Binary Search, Math


482
License Key Formatting
Solution
O(n)
O(n)

Medium



481
Magical String
Solution
O(?)
O(?)

Medium



480
Sliding Window Median
Solution
O(nlogk)
O(k)

Hard
Heap


479
Largest Palindrome Product
Solution
O(n)
O(1)

Easy



477
Total Hamming Distance
Solution
O(n)
O(1)

Medium
Bit Manipulation


476
Number Complement
Solution
O(n)
O(1)

Easy
Bit Manipulation


475
Heaters
Solution
max(O(nlogn), O(mlogn)) - m is the length of houses, n is the length of heaters
O(1)

Easy
Array Binary Search


474
Ones and Zeroes
Solution
O(n)
O(m*n)

Medium
DP


473
Matchsticks to Square
Solution
O(n!)
O(n)

Medium
Backtracking, DFS


472
Concatenated Words
Solution
O(n^2)
O(n)

Hard
Trie, DP, DFS


471
Encode String with Shortest Length
Solution
O(n^3)
O(n^2)

Hard
DP


469
Convex Polygon
Solution
O(n)
O(1)

Medium
Math


468
Validate IP Address
Solution
O(n)
O(1)

Medium
String


467
Unique Substrings in Wraparound String
Solution
O(n)
O(1)

Medium
DP


466
Count The Repetitions
Solution
O(max(m,n))
O(1)

Hard
DP


465
Optimal Account Balancing
Solution



Hard
DP


464
Can I Win
Solution
O(2^n)
O(n)

Medium
DP


463
Island Perimeter
Solution
O(m*n)
O(1)

Easy



462
Minimum Moves to Equal Array Elements II
Solution
O(nlogn)
O(1)

Medium



461
Hamming Distance
Solution
O(n)
O(1)

Easy



460
LFU Cache
Solution
O(1)
O(n)

Hard
Design, LinkedHashMap, HashMap


459
Repeated Substring Pattern
Solution
O(n)
O(n)

Easy
String, KMP


458
Poor Pigs
Solution
O(1)
O(1)

Easy
Math


457
Circular Array Loop
Solution
O(n)
O(1)

Medium



456
132 Pattern
Solution
O(n)
O(n)

Medium
Stack


455
Assign Cookies
Solution
O(n)
O(1)

Easy



454
4Sum II
Solution
O(n)
O(n)

Medium
HashMap


453
Minimum Moves to Equal Array Elements
Solution
O(n)
O(1)

Easy



452
Minimum Number of Arrows to Burst Balloons
Solution
O(nlogn)
O(1)

Medium
Array, Greedy


451
Sort Characters By Frequency
Solution
O(nlogn)
O(n)

Medium
HashMap


450
Delete Node in a BST
Solution
O(?)
O(?)

Medium
Tree, Recursion


449
Serialize and Deserialize BST
Solution
O(n)
O(h)

Medium
BFS


448
Find All Numbers Disappeared in an Array
Solution
O(n)
O(1)

Easy
Array, HashMap


447
Number of Boomerangs
Solution
O(n^2)
O(n)

Easy
HashMap


446
Arithmetic Slices II - Subsequence
Solution
O(n^2)
O(n^2)

Hard
DP


445
Add Two Numbers II
Solution
O(max(m,n)
O(max(m,n))

Medium
Stack, LinkedList


444
Sequence Reconstruction
Solution
O(n)
O(n)

Medium
Topological Sort, Graph


443
String Compression
Solution
O(n)
O(n)

Easy



442
Find All Duplicates in an Array
Solution
O(n)
O(1)

Medium
Array


441
Arranging Coins
Solution
O(n)
O(1)

Easy



440
K-th Smallest in Lexicographical Order
Solution
O(n^2)
O(1)

Hard



439
Ternary Expression Parser
Solution
O(n)
O(n)

Medium
Stack


438
Find All Anagrams in a String
Solution
O(n)
O(1)

Easy
Sliding Window


437
Path Sum III
Solution
O(n^2)
O(n)

Easy
DFS, recursion


436
Find Right Interval
Solution
O(nlogn)
O(n)

Medium
Binary Search


435
Non-overlapping Intervals
Solution
O(nlogn)
O(1)

Medium
Greedy


434
Number of Segments in a String
Solution
O(n)
O(1)

Easy



432
All O`one Data Structure
Solution
O(1)
O(n)

Hard
Design


429
N-ary Tree Level Order Traversal
Solution
O(n)
O(n)

Easy
BFS, Tree


425
Word Squares
Solution
O(n!)
O(n)

Hard
Trie, Backtracking, Recursion


424
Longest Repeating Character Replacement
Solution
O(n)
O(1)

Medium
Sliding Window


423
Reconstruct Original Digits from English
Solution
O(n)
O(1)

Medium
Math


422
Valid Word Square
Solution
O(n)
O(1)

Easy



421
Maximum XOR of Two Numbers in an Array
Solution
O(n)
O(1)

Medium
Bit Manipulation, Trie


420
Strong Password Checker
Solution
?
?

Hard



419
Battleships in a Board
Solution
O(m*n)
O(1)

Medium
DFS


418
Sentence Screen Fitting
Solution
O(n)
O(1)

Medium



417
Pacific Atlantic Water Flow
Solution
O(mnMax(m,n))
O(m*n)

Medium
DFS


416
Partition Equal Subset Sum
Solution
O(m*n)
O(m*n)

Medium
DP


415
Add Strings
Solution
O(n)
O(1)

Easy



414
Third Maximum Number
Solution
O(n)
O(1)

Easy



413
Arithmetic Slices
Solution
O(n)
O(1)

Medium
DP


412
Fizz Buzz
Solution
O(n)
O(1)

Easy



411
Minimum Unique Word Abbreviation
Solution
O(?)
O(?)

Hard
NP-Hard, Backtracking, Trie, Recursion


410
Split Array Largest Sum
Solution
O(nlogn)
O(1)

Hard
Binary Search, DP


409
Longest Palindrome
Solution
O(n)
O(1)

Easy



408
Valid Word Abbreviation
Solution
O(n)
O(1)

Easy



407
Trapping Rain Water II
Solution



Hard
Heap


406
Queue Reconstruction by Height
Solution
O(nlogn)
O(1)

Medium
LinkedList, PriorityQueue


405
Convert a Number to Hexadecimal
Solution
O(n)
O(1)

Easy



404
Sum of Left Leaves
Solution
O(n)
O(h)

Easy



403
Frog Jump
Solution
O(n^2)
O(n^2)

Hard
DP


402
Remove K Digits
Solution
O(n)
O(n)

Medium
Greedy, Stack


401
Binary Watch
Solution
O(1)
O(1)

Easy



400
Nth Digit
Solution
O(n)
O(1)

Easy



399
Evaluate Division
Solution
O(n*n!)
O(n)

Medium
Graph, DFS, Backtracking


398
Random Pick Index
Solution



Medium
Reservoir Sampling


397
Integer Replacement
Solution
?
?

Easy
BFS


396
Rotate Function
Solution
O(n^2) could be optimized to O(n)
O(1)

Easy



395
Longest Substring with At Least K Repeating Characters
Solution
O(n^2)
O(1)

Medium
Recursion


394
Decode String
Solution
O(n)
O(n)

Medium
Stack Depth-first-search


393
UTF-8 Validation
Solution
O(?)
O(?)

Medium
Bit Manipulation


392
Is Subsequence
Solution
O(m*n)
O(1)

Medium
Array, String


391
Perfect Rectangle
Solution
O(n)
O(1)

Hard



390
Elimination Game
Solution
O(logn)
O(1)

Medium



389
Find the Difference
Solution
O(n)
O(1)

Easy



388
Longest Absolute File Path
Solution
O(n)
O(d)

Medium
Stack


387
First Unique Character in a String
Solution
O(n)
O(n)

Easy
HashMap


386
Lexicographical Numbers
Solution
O(n)
O(1)

Medium



385
Mini Parser
Solution
O(n)
O(h)

Medium
Stack


384
Shuffle an Array
Solution
O(n)
O(n)

Medium



383
Ransom Note
Solution
O(n)
O(n)

Easy
String


382
Linked List Random Node
Solution
O(1)
O(n)

Medium
Reservoir Sampling


381
Insert Delete GetRandom O(1) - Duplicates allowed
Solution



Hard



380
Insert Delete GetRandom O(1)
Solution
O(n)
O(1)

Medium
Design, HashMap


379
Design Phone Directory
Solution
O(1)
O(n)

Medium



378
Kth Smallest Element in a Sorted Matrix
Solution
O(logm*n)
O(1)

Medium
Binary Search


377
Combination Sum IV
Solution
O(n^2)
O(n)

Medium
DP


376
Wiggle Subsequence
Solution
O(n)
O(1)

Medium
DP, Greedy


375
Guess Number Higher or Lower II
Solution
O(n^2)
O(n^2)

Medium
DP


374
Guess Number Higher or Lower
Solution
O(logn)
O(1)

Easy
Binary Search


373
Find K Pairs with Smallest Sums
Solution
O(klogk)
O(k)

Medium
Heap


372
Super Pow
Solution
O(n)
O(1)

Medium
Math


371
Sum of Two Integers
Solution
O(n)
O(1)

Easy



370
Range Addition
Solution
O(n+k)
O(1)

Medium
Array


369
Plus One Linked List
Solution
O(n)
O(1)

Medium
Linked List


368
Largest Divisible Subset
Solution
O(n^2)
O(n)

Medium
DP


367
Valid Perfect Square
Solution
O(n)
O(1)

Medium



366
Find Leaves of Binary Tree
Solution
O(n)
O(h)

Medium
DFS


365
Water and Jug Problem
Solution
O(n)
O(1)

Medium
Math


364
Nested List Weight Sum II
Solution
O(n)
O(h)

Medium
DFS


363
Max Sum of Rectangle No Larger Than K
Solution



Hard
DP


362
Design Hit Counter
Solution
O(1) amortized
O(k)

Medium
Design


361
Bomb Enemy
Solution
O(?)
O(?)

Medium



360
Sort Transformed Array
Solution
O(n)
O(1)

Medium
Two Pointers, Math


359
Logger Rate Limiter
Solution
amortized O(1)
O(k)

Easy
HashMap


358
Rearrange String k Distance Apart
Solution
O(n)
O(n)

Hard
HashMap, Heap, Greedy


357
Count Numbers with Unique Digits
Solution
O(n)
O(1)

Medium
DP, Math


356
Line Reflection
Solution
O(n)
O(n)

Medium
HashSet


355
Design Twitter
Solution
O(n)
O(n)

Medium
Design, HashMap, Heap


354
Russian Doll Envelopes
Solution
O(nlogn)
O(1)

Hard
DP, Binary Search


353
Design Snake Game
Solution
O(?)
O(?)

Medium



352
Data Stream as Disjoint Intervals
Solution
O(logn)
O(n)

Hard
TreeMap


351
Android Unlock Patterns
Solution
O(?)
O(?)

Medium



350
Intersection of Two Arrays II
Solution
O(m+n)
O((m+n)) could be optimized

Easy
HashMap, Binary Search


349
Intersection of Two Arrays
Solution
O(m+n)
O(min(m,n))

Easy
Two Pointers, Binary Search


348
Design Tic-Tac-Toe
Solution
O(1)
O(n)

Medium
Design


347
Top K Frequent Elements
Solution
O(n)
O(k) k is is the number of unique elements in the given array

Medium
HashTable, Heap, Bucket Sort


346
Moving Average from Data Stream
Solution
O(1)
O(w))

Easy
Queue


345
Reverse Vowels of a String
Solution
O(n)
O(1)

Easy
String


344
Reverse String
Solution
O(n)
O(1)

Easy
String


343
Integer Break
Solution
O(1)
O(1)

Medium
Math


342
Power of Four
Solution
O(n)
O(1)

Easy
Math


341
Flatten Nested List Iterator
Solution
O(n)
O(n)

Medium
Stack


340
Longest Substring with At Most K Distinct Characters
Solution
O(n)
O(1)

Hard
Sliding Window


339
Nested List Weight Sum
Solution
O(n)
O(h))

Easy
DFS


338
Counting Bits
Solution
O(nlogn)
O(h)

Medium



337
House Robber III
Solution
O(n)
O(n)

Medium
DP


336
Palindrome Pairs
Solution
O(n^2)
O(n)

Hard



335
Self Crossing
Solution
O(n)
O(1)

Hard
Math


334
Increasing Triplet Subsequence
Solution
O(n)
O(1)

Medium



333
Largest BST Subtree
Solution
O(n)
O(n)

Medium
Tree


332
Reconstruct Itinerary
Solution
O(n)
O(n)

Medium
Graph, DFS


331
Verify Preorder Serialization of a Binary Tree
Solution
O(n)
O(n)

Medium
Stack


330
Patching Array
Solution
O(m+logn)
O(1)

Hard
Greedy


329
Longest Increasing Path in a Matrix
Solution
O(m*n)
O(m*n)

Hard
DFS, DP


328
Odd Even Linked List
Solution
O(n)
O(1)

Medium
Linked List


327
Count of Range Sum
Solution
O(nlogn)
O(n)

Hard
BST, Divide and Conquer


326
Power of Three
Solution
O(1)
O(1)

Easy
Math


325
Maximum Size Subarray Sum Equals k
Solution
O(n)
O(n)

Medium
HashTable


324
Wiggle Sort II
Solution
O(n)
O(n)

Medium
Sort


323
Number of Connected Components in an Undirected Graph
Solution
O(?)
O(?)

Medium



322
Coin Change
Solution
O(n*k)
O(k)

Medium
DP


321
Create Maximum Number
Solution
O(?)
O(?)

Hard



320
Generalized Abbreviation
Solution
O(n*2^n)
O(n)

Medium
Backtracking, Bit Manipulation


319
Bulb Switcher
Solution
O(1)
O(1)

Medium
Brainteaser


318
Maximum Product of Word Lengths
Solution
O(n^2)
O(n)

Medium



317
Shortest Distance from All Buildings
Solution
O(?)
O(?)

Hard



316
Remove Duplicate Letters
Solution
O(n)
O(1)

Hard
Stack, Recursion, Greedy


315
Count of Smaller Numbers After Self
Solution
O(?)
O(?)

Hard
Tree


314
Binary Tree Vertical Order Traversal
Solution
O(n)
O(n)

Medium
HashMap, BFS


313
Super Ugly Number
Solution
O(?)
O(?)

Medium



312
Burst Balloons
Solution
O(?)
O(?)

Hard
DP


311
Sparse Matrix Multiplication
Solution
O(mnl)
O(m*l)

Medium



310
Minimum Height Trees
Solution
?
?

Medium



309
Best Time to Buy and Sell Stock with Cooldown
Solution
O(n)
O(1)

Medium
DP


308
Range Sum Query 2D - Mutable
Solution
?
?

Hard
Tree


307
Range Sum Query - Mutable
Solution
?
?

Medium
Tree


306
Additive Number
Solution
O(n^2)
O(n)

Medium



305
Number of Islands II
Solution
?
?

Hard
Union Find


304
Range Sum Query 2D - Immutable
Solution
?
?

Medium



303
Range Sum Query - Immutable
Solution
O(n)
O(1)

Easy



302
Smallest Rectangle Enclosing Black Pixels
Solution
?
O(m*n)

Hard
DFS, BFS


301
Remove Invalid Parentheses
Solution
?
?

Hard
BFS


300
Longest Increasing Subsequence
Solution
O(logn)
O(n)

Medium
DP


299
Bulls and Cows
Solution
O(n)
O(1)

Easy



298
Binary Tree Longest Consecutive Sequence
Solution
O(n)
O(n)

Medium
Tree


297
Serialize and Deserialize Binary Tree
Solution
O(n)
O(h)

Hard
BFS


296
Best Meeting Point
Solution
?
?

Hard



295
Find Median from Data Stream
Solution
O(logn)
O(n)

Hard
Heap


294
Flip Game II
Solution
O(?)
O(?)

Medium
Backtracking


293
Flip Game
Solution
O(n)
O(1)

Easy



292
Nim Game
Solution
O(1)
O(1)

Easy



291
Word Pattern II
Solution
O(n)
O(n)

Hard
Recursion, Backtracking


290
Word Pattern
Solution
O(n)
O(n)

Easy
HashMap


289
Game of Life
Solution
O(m*n)
O(m*n), could be optimized to O(1)

Medium



288
Unique Word Abbreviation
Solution
O(n)
O(1)

Easy



287
Find the Duplicate Number
Solution
O(n)
O(1)

Medium



286
Walls and Gates
Solution
O(m*n)
O(g)

Medium
BFS


285
Inorder Successor In BST
Solution
O(h)
O(1)

Medium
Tree


284
Peeking Iterator
Solution
O(n)
O(n)

Medium
Design


283
Move Zeroes
Solution
O(n)
O(1)

Easy



282
Expression Add Operators
Solution
O(?)
O(?)

Hard



281
Zigzag Iterator
Solution
O(1)
O(k)

Medium



280
Wiggle Sort
Solution
O(n)
O(1)

Medium



279
Perfect Squares
Solution
O(n)
O(1)

Medium



278
First Bad Version
Solution
O(logn)
O(1)

Easy
Binary Search


277
Find the Celebrity
Solution
O(n)
O(1)

Medium



276
Paint Fence
Solution
O(n)
O(1)

Easy
DP


275
H-Index II
Solution
O(logn)
O(1)

Medium
Binary Search


274
H-Index
Solution
O(nlogn)
O(1)

Medium



273
Integer to English Words
Solution
O(n)
O(1)

Hard
Math, String


272
Closest Binary Search Tree Value II
Solution
O(h+k)
O(h)

Hard
Stack


271
Encode and Decode Strings
Solution
O(n)
O(1)

Medium



270
Closest Binary Search Tree Value
Solution
O(h)
O(1)

Easy
DFS


269
Alien Dictionary
Solution
O(?)
O(?)

Hard
Topological Sort


268
Missing Number
Solution
O(n)
O(1)

Easy
Bit Manipulation


267
Palindrome Permutation II
Solution
O(n*n!)
O(n)

Medium



266
Palindrome Permutation
Solution
O(n)
O(1)

Easy



265
Paint House II
Solution
O(n*k)
O(1)

Hard
DP


264
Ugly Number II
Solution
O(n)
O(n)

Medium
DP


263
Ugly Number
Solution
O(n)
O(1)

Easy



261
Graph Valid Tree
Solution
O(V+E)
O(V+E)

Medium



260
Single Number III
Solution
O(n)
O(n)

Medium



259
3Sum Smaller
Solution
O(n^2)
O(1)

Medium



258
Add Digits
Solution
O(1)
O(1)

Easy



257
Binary Tree Paths
Solution
O(n*h)
O(h)


DFS/Recursion


256
Paint House
Solution
O(n)
O(1)

Medium
DP


255
Verify Preorder Sequence in Binary Search Tree
Solution
O(n)
O(h)

Medium
Tree


254
Factor Combinations
Solution
O(nlogn)
O(nlogn)

Medium
Backtracking


253
Meeting Rooms II
Solution
O(nlogn)
O(h)

Medium
Heap


252
Meeting Rooms
Solution
O(nlogn)
O(1)

Easy



251
Flatten 2D Vector
Solution
O(1)
O(m*n)

Medium



250
Count Univalue Subtrees
Solution
O(n)
O(h)

Medium
DFS


249
Group Shifted Strings
Solution
O(nlogn)
O(n)





248
Strobogrammatic Number III
Solution
O(?)
O(?)

Hard
Recursion, DFS


247
Strobogrammatic Number II
Solution
O(n^2)
O(n)

Medium
Recursion


246
Strobogrammatic Number
Solution
O(n)
O(1)

Easy



245
Shortest Word Distance III
Solution
O(n)
O(1)

Medium



244
Shortest Word Distance II
Solution
O(n)
O(n)

Medium
HashMap


243
Shortest Word Distance
Solution
O(n)
O(1)

Easy



242
Valid Anagram
Solution
O(n)
O(1)

Easy



241
Different Ways to Add Parentheses
Solution
O(O(n * 4^n / n^(3/2)))
O(n * 4^n / n^(3/2))

Medium
Divide and Conquer


240
Search a 2D Matrix II
Solution
O(m+n)
O(1)

Medium
Binary Search


239
Sliding Window Maximum
Solution
O(nlogn)
O(k)

Hard
Heap


238
Product of Array Except Self
Solution
O(n)
O(1)

Medium
Array


237
Delete Node in a Linked List
Solution
O(1)
O(1)

Easy
LinkedList


236
Lowest Common Ancestor of a Binary Tree
Solution
O(n)
O(h)

Medium
DFS


235
Lowest Common Ancestor of a Binary Search Tree
Solution
O(h)
O(1)

Easy
DFS


234
Palindrome Linked List
Solution
O(n)
O(1)

Easy
Linked List


233
Number of Digit One
Solution
O(n)
O(1)

Hard
Math


232
Implement Queue using Stacks
Solution
O(n)
O(n)

Medium
Stack, Design


231
Power of Two
Solution
O(1)
O(1)

Easy



230
Kth Smallest Element in a BST
Solution
O(n)
O(k)

Medium
Tree


229
Majority Element II
Solution
O(n)
O(1)

Medium



228
Summary Ranges
Solution
O(n)
O(1)

Medium
Array


227
Basic Calculator II
Solution
O(n)
O(n)

Medium
String


226
Invert Binary Tree
Solution
O(n)
O(h)

Easy
DFS, recursion


225
Implement Stack using Queues
Solution
O(n)
O(n)

Easy
Stack, Queue


224
Basic Calculator
Solution
?
?

Hard



223
Rectangle Area
Solution
O(1)
O(1)

Easy



222
Count Complete Tree Nodes
Solution
O(?)
O(h)

Medium
Recursion


221
Maximal Square
Solution
O(?)
O(h)

Medium
Recursion


220
Contains Duplicate III
Solution
O(nlogn)
O(n)

Medium
TreeSet


219
Contains Duplicate II
Solution
O(n)
O(n)

Easy
HashMap


218
The Skyline Problem
Solution
O(n)
O(n)

Hard
TreeMap, Design


217
Contains Duplicate
Solution
O(n)
O(n)

Easy
HashSet


216
Combination Sum III
Solution
O(k * C(n, k))
O(k)

Medium
Backtracking


215
Kth Largest Element in an Array
Solution
O(nlogn)
O(n)

Medium
Heap


214
Shortest Palindrome
Solution
O(?)
O(?)

Hard
KMP


213
House Robber II
Solution
O(n)
O(n)

Medium
DP


212
Word Search II
Solution
O(mnl)
O(l)

Hard
Trie


211
Add and Search Word - Data structure design
Solution
O(n)
O(h)

Medium
Trie


210
Course Schedule II
Solution
O(?)
O(?)

Medium



209
Minimum Size Subarray Sum
Solution
O(n)
O(1)

Medium



208
Implement Trie
Solution
O(n)
O(1)

Medium
Trie


207
Course Schedule
Solution
O(?)
O(?)

Medium



206
Reverse Linked List
Solution
O(n)
O(1)

Easy
Linked List


205
Isomorphic Strings
Solution
O(n)
O(1)

Easy



204
Count Primes
Solution
O(nloglogn)
O(n)

Easy
The Sieve of Eratosthenes


203
Remove Linked List Elements
Solution
O(n)
O(1)

Easy



202
Happy Number
Solution
O(k)
O(k)

Easy



201
Bitwise AND of Numbers Range
Solution
O(min(m,n))
O(1)

Medium
Bit Manipulation


200
Number of Islands
Solution
O(m*n)
O(m*n)

Medium
Union Find, DFS


199
Binary Tree Right Side View
Solution
O(n)
O(h)

Medium
BFS


198
House Robber
Solution
O(n)
O(n)

Easy
DP


191
Number of 1 Bits
Solution
O(1)
O(1)

Easy
Bit Manipulation


190
Reverse Bits
Solution
O(n)
O(1)

Easy
Bit Manipulation


189
Rotate Array
Solution
O(n)
O(n), could be optimized to O(1)

Easy



188
Best Time to Buy and Sell Stock IV
Solution
O(n*k)
O(n*k)

Hard
DP


187
Repeated DNA Sequences
Solution
O(n)
O(n)

Medium



186
Reverse Words in a String II
Solution
O(n)
O(1)

Medium



179
Largest Number
Solution
O(?)
O(?)

Medium



174
Dungeon Game
Solution
O(m*n)
O(m*n)

Hard
DP


173
Binary Search Tree Iterator
Solution
O(1)
O(h)

Medium
Stack, Design


172
Factorial Trailing Zeroes
Solution
O(logn)
O(1)

Easy



171
Excel Sheet Column Number
Solution
O(n)
O(1)

Easy



170
Two Sum III - Data structure design
Solution
O(n)
O(n)

Easy



169
Majority Element
Solution
O(n)
O(1)

Easy



168
Excel Sheet Column Title
Solution
O(n)
O(1)

Easy



167
Two Sum II - Input array is sorted
Solution
O(n)
O(1)

Easy
Binary Search


166
Fraction to Recurring Decimal
Solution
O(1)
O(1)

Medium
HashMap


165
Compare Version Numbers
Solution
O(n)
O(1)

Easy



164
Maximum Gap
Solution
O(n)
O(n)

Hard



163
Missing Ranges
Solution
O(n)
O(1)





162
Find Peak Element
Solution
O(1)
O(logn)/O(n)

Binary Search



161
One Edit Distance
Solution
O(n)
O(1)





160
Intersection of Two Linked Lists
Solution
O(m+n)
O(1)

Easy
Linked List


159
Longest Substring with At Most Two Distinct Characters
Solution
O(n)
O(1)

Hard
String, Sliding Window


158
Read N Characters Given Read4 II - Call multiple times
Solution
O(n)
O(1)

Hard



157
Read N Characters Given Read4
Solution
O(n)
O(1)

Easy



156
Binary Tree Upside Down
Solution
O(n)
O(h)

Medium
Tree, Recursion


155
Min Stack
Solution
O(1)
O(n)

Easy
Stack


154
Find Minimum in Rotated Sorted Array II
Solution
O(logn)
O(1)

Hard
Array, Binary Search


153
Find Minimum in Rotated Sorted Array
Solution
O(logn)
O(1)

Medium
Array, Binary Search


152
Maximum Product Subarray
Solution
O(n)
O(1)

Medium
Array


151
Reverse Words in a String
Solution
O(n)
O(n)

Medium
String


150
Evaluate Reverse Polish Notation
Solution
O(?)
O(?)

Medium



149
Max Points on a Line
Solution
O(?)
O(?)

Hard



148
Sort List
Solution
O(nlogn)
O(h)

Medium
Linked List, Sort


147
Insertion Sort List
Solution
O(n^2)
O(1)

Medium
Linked List


146
LRU Cache
Solution
amortized O(1)
O(k)

Hard
Doubly Linked List, LinkedHashMap


145
Binary Tree Postorder Traversal
Solution
O(n)
O(h)

Hard
Binary Tree


144
Binary Tree Preorder Traversal
Solution
O(n)
O(h)

Medium
Binary Tree


143
Reorder List
Solution
O(n)
O(1)

Medium



142
Linked List Cycle II
Solution
O(n)
O(1)

Medium
Linked List


141
Linked List Cycle
Solution
O(n)
O(1)

Easy
Linked List


140
Word Break II
Solution
?
O(n^2)

Hard
Backtracking/DFS


139
Word Break
Solution
O(n^2)
O(n)

Medium
DP, Pruning


138
Copy List with Random Pointer
Solution
O(n)
O(n)

Medium
LinkedList, HashMap


137
Single Number II
Solution
O(n)
O(1)

Medium
Bit Manipulation


136
Single Number
Solution
O(n)
O(1)

Easy
Bit Manipulation


135
Candy
Solution
O(n)
O(1)

Hard
Greedy


134
Gas Station
Solution
O(n)
O(1)

Medium
Greedy


133
Clone Graph
Solution
O(n)
O(n)

Medium
HashMap, BFS, Graph


132
Palindrome Partitioning II
Solution
O(n^2)
O(n^2)

Hard



131
Palindrome Partitioning
Solution
O(n^2)
O(n^2)

Medium



130
Surrounded Regions
Solution
O(?)
O(?)

Medium



129
Sum Root to Leaf Numbers
Solution
O(n)
O(h)

Medium
DFS


128
Longest Consecutive Sequence
Solution
O(n)
O(n)

Hard
Union Find


127
Word Ladder
Solution
O(n^2)
O(n)

Medium
BFS


126
Word Ladder II
Solution
O(?)
O(?)

Hard
BFS


125
Valid Palindrome
Solution
O(n)
O(1)

Easy
Two Pointers


124
Binary Tree Maximum Path Sum
Solution
O(n)
O(h)

Hard
Tree, DFS


123
Best Time to Buy and Sell Stock III
Solution
O(n)
O(1)

Hard
DP


122
Best Time to Buy and Sell Stock II
Solution
O(n)
O(1)

Easy
Greedy


121
Best Time to Buy and Sell Stock
Solution
O(n)
O(1)

Easy



120
Triangle
Solution
O(m*n)
O(n)

Medium
DP


119
Pascal's Triangle II
Solution
O(n^2)
O(k)

Easy



118
Pascal's Triangle
Solution
O(n^2)
O(1)

Easy



117
Populating Next Right Pointers in Each Node II
Solution
O(n)
O(1)

Medium
BFS


116
Populating Next Right Pointers in Each Node
Solution
O(n)
O(1)

Medium
BFS


115
Distinct Subsequences
Solution
O(m*n)
O(m*n)

Hard
DP


114
Flatten Binary Tree to Linked List
Solution
O(n)
O(h)

Medium
Tree


113
Path Sum II
Solution
O(n)
O(h)

Medium
DFS, Backtracking


112
Path Sum
Solution
O(n)
O(1)

Easy
DFS


111
Minimum Depth of Binary Tree
Solution
O(n)
O(1)~O(h)

Easy
BFS, DFS


110
Balanced Binary Tree
Solution
O(n)
O(1)~O(h)

Easy
DFS


109
Convert Sorted List to Binary Search Tree
Solution
O(n)
O(h)

Medium
DFS, Recursion


108
Convert Sorted Array to Binary Search Tree
Solution
O(n)
O(h)

Easy
Tree


107
Binary Tree Level Order Traversal II
Solution
O(nlogn)
O(h)

Easy
BFS


106
Construct Binary Tree from Inorder and Postorder Traversal
Solution
O(n)
O(n)

Medium
Recursion, Tree


105
Construct Binary Tree from Preorder and Inorder Traversal
Solution
O(n)
O(n)

Medium
Recursion, Tree


104
Maximum Depth of Binary Tree
Solution
O(n)
O(h)

Easy
DFS


103
Binary Tree Zigzag Level Order Traversal
Solution
O(n)
O(h)

Medium
BFS,DFS


102
Binary Tree Level Order Traversal
Solution
O(n)
O(h)

Medium
BFS


101
Symmetric Tree
Solution
O(n)
O(h)
📺
Easy
DFS


100
Same Tree
Solution
O(n)
O(h)
📺
Easy
DFS


99
Recover Binary Search Tree
Solution
O(?)
O(?)

Hard



98
Validate Binary Search Tree
Solution
O(n)
O(h)

Medium
DFS/Recursion


97
Interleaving String
Solution
O(?)
O(?)

Hard
DP


96
Unique Binary Search Trees
Solution
O(n^2)
O(n)

Medium
Recursion, DP


95
Unique Binary Search Trees II
Solution
O(?)
O(?)

Medium
Recursion


94
Binary Tree Inorder Traversal
Solution
O(n)
O(h)

Medium
Binary Tree


93
Restore IP Addresses
Solution
O(?)
O(?)

Medium
Backtracking


92
Reverse Linked List II
Solution
O(n)
O(1)

Medium



91
Decode Ways
Solution
O(n)
O(n)

Medium
DP


90
Subsets II
Solution
O(n^2)
O(1)

Medium
Backtracking


89
Gray Code
Solution
O(n)
O(1)

Medium
Bit Manipulation


88
Merge Sorted Array
Solution
O(max(m,n))
O(1)

Easy



87
Scramble String
Solution
O(n^4)
O(n^3

Hard
Recursion


86
Partition List
Solution
O(n)
O(1)

Medium
Linked List


85
Maximal Rectangle
Solution
O(m*n)
O(n)

Hard
DP


84
Largest Rectangle in Histogram
Solution
O(n)
O(n)

Hard
Array, Stack


83
Remove Duplicates from Sorted List
Solution
O(n)
O(1)

Easy
Linked List


82
Remove Duplicates from Sorted List II
Solution
O(n)
O(1)

Medium
Linked List


81
Search in Rotated Sorted Array II
Solution
O(logn)
O(1)

Medium
Binary Search


80
Remove Duplicates from Sorted Array II
Solution
O(n)
O(n)

Medium



79
Word Search
Solution
O((m*n)^2)
O(m*n)

Medium
Backtracking, DFS


78
Subsets
Solution
O(n^2)
O(1)

Medium
Backtracking


77
Combinations
Solution
O(n!)
O(n)

Medium
Backtracking


76
Minimum Window Substring
Solution
O(n)
O(k)

Hard
Two Pointers


75
Sort Colors
Solution
O(n)
O(1)

Medium
Two Pointers


74
Search a 2D Matrix
Solution
O(log(m*n))
O(1)

Medium
Binary Search


73
Set Matrix Zeroes
Solution
O(mn)
O(1)

Medium



72
Edit Distance
Solution
O(m*n)
O(m+n)

Hard



71
Simplify Path
Solution
O(n)
O(n)

Medium
Stack


70
Climbing Stairs
Solution
O(n)
O(n)

Easy
DP


69
Sqrt(x)
Solution
O(logn)
O(1)

Easy



68
Text Justification
Solution
O(n)
O(1)

Hard



67
Add Binary
Solution
O(n)
O(1)

Easy



66
Plus One
Solution
O(n)
O(1)

Easy



65
Valid Number
Solution
O(n)
O(1)

Hard



64
Minimum Path Sum
Solution
O(m*n)
O(m*n)

Medium
DP


63
Unique Paths II
Solution
O(m*n)
O(m*n)

Medium
DP


62
Unique Paths
Solution
O(m*n)
O(m*n)

Medium
DP


61
Rotate List
Solution
O(n)
O(1)

Medium
Linked List


60
Permutation Sequence
Solution
O(n^2)
O(n)

Medium
Math, Backtracking


59
Spiral Matrix II
Solution
O(n)
O(n)

Medium



58
Length of Last Word
Solution
O(n)
O(1)

Easy



57
Insert Intervals
Solution
O(n)
O(1)

Hard
Array, Sort


56
Merge Intervals
Solution
O(n*logn)
O(1)

Medium
Array, Sort


55
Jump Game
Solution
O(n)
O(1)

Medium
Greedy


54
Spiral Matrix
Solution
O(m*n)
O(m*n)

Medium
Array


53
Maximum Subarray
Solution
O(n)
O(1)

Easy
Array


52
N-Queens II
Solution
O(n!)
O(n)

Hard
Backtracking


51
N-Queens
Solution
O(n!)
O(n)

Hard



50
Pow(x, n)
Solution
O(logn)
O(logn)

Medium



49
Group Anagrams
Solution
O(m*klogk)
O(m*k)

Medium
HashMap


48
Rotate Image
Solution
O(n^2)
O(1)

Medium
Array


47
Permutations II
Solution
O(n*n!)
O(n)

Medium
Backtracking


46
Permutations
Solution
O(n*n!)
O(n)

Medium
Backtracking


45
Jump Game II
Solution
O(n)
O(1)

Hard
Array, Greedy


44
Wildcard Matching
Solution
O(m*n)
O(m*n)

Hard
Backtracking, DP, Greedy, String


43
Multiply Strings
Solution
O(n)
O(1)

Medium
Array, String


42
Trapping Rain Water
Solution
O(n)
O(1)

Hard



41
First Missing Positive
Solution
O(n)
O(1)

Hard
Array


40
Combination Sum II
Solution
O(k*n^k)
O(k)

Medium
Backtracking


39
Combination Sum
Solution
O(k*n^k)
O(k)

Medium
Backtracking


38
Count and Say
Solution
O(n*2^n)
O(2^n)

Easy
Recursion, LinkedList


37
Sudoku Solver
Solution
O((9!)^9)
O(1)

Hard



36
Valid Sudoku
Solution
O(1)
O(1)

Medium



35
Search Insert Position
Solution
O(n)
O(1)

Easy
Array


34
Search for a Range
Solution
O(logn)
O(1)

Medium
Array, Binary Search


33
Search in Rotated Sorted Array
Solution
O(logn)
O(1)

Medium
Binary Search


32
Longest Valid Parentheses
Solution
O(n)
O(n)

Hard
Stack, DP


31
Next Permutation
Solution
O(n)
O(1)

Medium
Array


30
Substring with Concatenation of All Words
Solution
O(n^2)
O(n)

Hard
HashMap


29
Divide Two Integers
Solution
O(?)
O(?)

Medium



28
Implement strStr()
Solution
O(n)
O(1)

Easy
String


27
Remove Element
Solution
O(n)
O(1)

Easy



26
Remove Duplicates from Sorted Array
Solution
O(n)
O(1)

Easy
Array


25
Reverse Nodes in k-Group
Solution
O(n)
O(1)

Hard
Recursion, LinkedList


24
Swap Nodes in Pairs
Solution
O(n)
O(h)

Medium
Recursion, LinkedList


23
Merge k Sorted Lists
Solution
O(n*logk)
O(k)

Hard
Heap


22
Generate Parentheses
Solution
TBD
O(n)

Medium
Backtracking


21
Merge Two Sorted Lists
Solution
O(n)
O(h)

Easy
Recursion


20
Valid Parentheses
Solution
O(n)
O(n)
📺
Easy
Stack


19
Remove Nth Node From End of List
Solution
O(n)
O(1)

Medium
Linked List


18
4 Sum
Solution
O(n^2)
O(1)

Medium
Two Pointers


17
Letter Combinations of a Phone Number
Solution
O(n*4^n)
O(n)

Medium
Backtracking


16
3Sum Closest
Solution
O(nlogn)
O(1)

Medium
Two Pointers


15
3Sum
Solution
O(n^2)
O(1)
📺
Medium
Two Pointers, Binary Search


14
Longest Common Prefix
Solution
O(S) (S is the sum of all characters in all strings)
O(1)
📺
Easy



13
Roman to Integer
Solution
O(1)
O(1)

Easy
Math, String


12
Integer to Roman
Solution
O(1)
O(1)

Medium
Math, String


11
Container With Most Water
Solution
O(n)
O(1)

Medium



10
Regular Expression Matching
Solution
O(m*n)
O(m*n)

Hard
DP


9
Palindrome Number
Java, C++
O(n)
O(1)

Easy



8
String to Integer (atoi)
Solution
O(n)
O(1)

Medium



7
Reverse Integer
Solution
O(1)
O(1)

Easy



6
ZigZag Conversion
Solution
O(n)
O(n)

Easy



5
Longest Palindromic Substring
Solution
O(n^2)
O(1)

Medium



4
Median of Two Sorted Arrays
Solution
?
?

Hard
Divide and Conquer


3
Longest Substring Without Repeating Characters
Solution
O(n)
O(k)

Medium
HashMap, Sliding Window


2
Add Two Numbers
Solution
O(max(m,n))
O(1)

Medium
LinkedList


1
Two Sum
Java, C++
O(n)
O(n)
📺
Easy
HashMap



Database



#
Title
Solutions
Time
Space
Difficulty
Tag




1280
Students and Examinations
Solution


Easy



1251
Average Selling Price
Solution


Easy



1179
Reformat Department Table
Solution


Easy



1069
Product Sales Analysis II
Solution


Easy



1068
Product Sales Analysis I
Solution


Easy



627
Swap Salary
Solution


Easy



626
Exchange Seats
Solution


Medium



620
Not Boring Movies
Solution


Easy



619
Biggest Single Number
Solution


Easy



618
Students Report By Geography
Solution


Hard
Session Variables


615
Average Salary: Departments VS Company
Solution


Hard



614
Second Degree Follower
Solution


Medium
Inner Join


613
Shortest Distance in a Line
Solution


Easy



612
Shortest Distance in a Plane
Solution


Medium



610
Triangle Judgement
Solution


Easy



608
Tree Node
Solution


Medium
Union


607
Sales Person
Solution


Easy



603
Consecutive Available Seats
Solution


Easy



602
Friend Requests II: Who Has the Most Friends
Solution


Medium



601
Human Traffic of Stadium
Solution


Hard



597
Friend Requests I: Overall Acceptance Rate
Solution


Easy



596
Classes More Than 5 Students
Solution


Easy



595
Big Countries
Solution
O(n)
O(1)
Easy



586
Customer Placing the Largest Number of Orders
Solution


Easy



585
Investments in 2016
Solution


Medium



584
Find Customer Referee
Solution


Easy



580
Count Student Number in Departments
Solution


Medium
Left Join


578
Get Highest Answer Rate Question
Solution


Medium



577
Employee Bonus
Solution


Easy



574
Winning Candidate
Solution


Medium



571
Find Median Given Frequency of Numbers
Solution


Hard



570
Managers with at Least 5 Direct Reports
Solution


Medium



569
Median Employee Salary
Solution


Hard



511
Game Play Analysis I
Solution


Easy



262
Trips and Users
Solution


Hard
Inner Join


197
Rising Temperature
Solution
O(n^2)
O(n)
Easy



196
Delete Duplicate Emails
Solution
O(n^2)
O(n)
Easy



185
Department Top Three Salaries
Solution


Hard



184
Department Highest Salary
Solution
O(n^2)
O(n)
Medium



183
Customers Who Never Order
Solution
O(n^2)
O(n)
Easy



182
Duplicate Emails
Solution
O(n^2)
O(n)
Easy



181
Employees Earning More Than Their Managers
Solution
O(n^2)
O(n)
Easy



180
Consecutive Numbers
Solution
O(n)
O(n)
Medium



178
Rank Scores
Solution
?
?
Medium



177
Nth Highest Salary
Solution
O(n)
O(1)
Medium



176
Second Highest Salary
Solution
O(n)
O(1)
Easy



175
Combine Two Tables
Solution
O(m+n)
O(m+n)
Easy




Shell



#
Title
Solutions
Time
Space
Difficulty
Tag




195
Tenth Line
Solution
O(n)
O(1)
Easy



194
Transpose File
Solution
O(n^2)
O(n^2)
Medium



193
Valid Phone Numbers
Solution
O(n)
O(1)
Easy



192
Word Frequency
Solution
O(n)
O(k)
Medium




Contributing
Your ideas/fixes/algorithms are more than welcome!

Fork this repo
Clone your forked repo (git clone https://github.com/YOUR_GITHUB_USERNAME/Leetcode.git) onto your local machine
cd into your cloned directory, create your feature branch (git checkout -b my-awesome-fix)
git add your desired changes to this repo
Commit your changes (git commit -m 'Added some awesome features/fixes')
Push to the branch (git push origin my-awesome-feature)
Open your forked repo on Github website, create a new Pull Request to this repo!

Best way to open this project

Install Intellij on your machine, either CE or UE.
git clone this repo to your local disk
import this project as a new project (does need to be imported as a gradle project)
If you run into ""Could not determine Java version using executable ..."" error, use local gradle distribution: ""/usr/local/Cellar/gradle/4.8.1/libexec/"" instead of the default one. More details, see Stackoverflow.

",batch2,8:10:18,Done
84,detekt/detekt,"detekt








Meet detekt, a static code analysis tool for the Kotlin programming language.
It operates on the abstract syntax tree provided by the Kotlin compiler.

Features

Code smell analysis for your Kotlin projects
Complexity reports based on lines of code, cyclomatic complexity and amount of code smells
Highly configurable rule sets
Suppression of findings with Kotlin's @Suppress and Java's @SuppressWarnings annotations
Specification of quality gates which will break your build
Code Smell baseline and suppression for legacy projects
Gradle plugin for code analysis via Gradle builds
SonarQube integration
Extensibility by enabling incorporation of personal rule sets, FileProcessListener's and OutputReport's
IntelliJ integration
Third party integrations for Maven, Bazel and Github Actions (Docker based and Javascript based)

Project Website
Visit the project website for installation guides, release notes, migration guides, rule descriptions and configuration options.
Quick-Links

Changelog and migration guides
Available CLI options
Rule set and rule descriptions
Writing custom rules and extending detekt
Suppressing issues in code
Suppressing issues via baseline file
Configuring detekt
Sample Gradle integrations examples:

multi project (Kotlin DSL) with precompiled script plugin
single project (Groovy DSL)
single project (Unofficial Maven plugin)
setup additional detekt task for all modules (Kotlin DSL)
setup additional formatting task for all modules (Kotlin DSL)



Quick Start ...
with the command-line interface
curl -sSLO https://github.com/detekt/detekt/releases/download/v[version]/detekt-cli-[version]-all.jar
java -jar detekt-cli-[version]-all.jar --help
You can find other ways to install detekt here
with Gradle
plugins {
    id(""io.gitlab.arturbosch.detekt"").version(""[version]"")
}

repositories {
    mavenCentral()
}

detekt {
    buildUponDefaultConfig = true // preconfigure defaults
    allRules = false // activate all available (even unstable) rules.
    config = files(""$projectDir/config/detekt.yml"") // point to your custom config defining rules to run, overwriting default behavior
    baseline = file(""$projectDir/config/baseline.xml"") // a way of suppressing issues before introducing detekt

    reports {
        html.enabled = true // observe findings in your browser with structure and code snippets
        xml.enabled = true // checkstyle like format mainly for integrations like Jenkins
        txt.enabled = true // similar to the console output, contains issue signature to manually edit baseline files
        sarif.enabled = true // standardized SARIF format (https://sarifweb.azurewebsites.net/) to support integrations with Github Code Scanning
    }
}


// Groovy DSL
tasks.withType(Detekt).configureEach {
    jvmTarget = ""1.8""
}

// or

// Kotlin DSL
tasks.withType<Detekt>().configureEach {
    // Target version of the generated JVM bytecode. It is used for type resolution.
    jvmTarget = ""1.8""
}
See maven central for releases and sonatype for snapshots.
If you want to use a SNAPSHOT version, you can find more info on this documentation page.
Requirements
Gradle 6.1+ is the minimum requirement. However, the recommended versions together with the other tools recommended versions are:



Detekt Version
Gradle
Kotlin
AGP
Java Target Level
JDK Max Version




1.17.0
7.0.1
1.4.32
4.2.0
1.8
15



The list of recommended versions for previous detekt version is listed here.
Adding more rule sets
detekt itself provides a wrapper over ktlint as a formatting rule set
which can be easily added to the Gradle configuration:
dependencies {
    detektPlugins(""io.gitlab.arturbosch.detekt:detekt-formatting:[version]"")
}
Likewise custom extensions can be added to detekt.
Contributors
If you contributed to detekt but your name is not in the list, please feel free to add yourself to it!

Artur Bosch - Maintainer
Marvin Ramin - Collaborator, Bunch of rules, Active on Issues, refactorings, MultiRule
schalks - Collaborator, Active on Issues, Bunch of rules, Project metrics
Niklas Baudy - Active on Issues, Bunch of rules, Bug fixes
lummax - Cli enhancements
Svyatoslav Chatchenko - Active on Issues, NamingConventions and UnusedImport fixes
Sean Flanigan - Config from classpath resource
Sebastian Schuberth - Active on Issues, Windows support
Olivier Lemasle - NP-Bugfix, rules fixes, Gradle plugin improvement
Marc Prengemann - Support for custom output formats, prototyped Rule-Context-Issue separation
Sebastiano Poggi - Build tooling improvements, rules improvements and fixes, docs fixes, Gradle plugin improvements
Ilya Tretyakov - Sonar runs should not auto correct formatting.
Andrey T - Readme fix
Ivan Balaksha - Rules: UnsafeCast, SpreadOperator, UnsafeCallOnNullableType, LabeledExpression
Anna Y - Readme fix
Karol Wrótniak - Treat comments as not empty blocks
Radim Vaculik - VariableMaxLength - bugfix
Martin Nonnenmacher - UndocumentedPublicClass - enum support
Dmytro Troynikov - Updated Magic Number rule to ignore Named Arguments
Andrew Ochsner - Updated Readme for failFast option
Paul Merlin - Gradle build improvements
Konstantin Aksenov - Coding improvement
Matthew Haughton - Added type resolution, Dependency updates, Coding + Documentation improvements
Janusz Bagiński - Fixed line number reporting for MaxLineLengthRule
Mike Kobit - Gradle build improvements
Philipp Hofmann - Readme improvements
Olivier PEREZ - Fixed Typo in Readme
Sebastian Kaspari - Html-Output-Format, Documentation fix
Ilya Zorin - Rule improvement: UnnecessaryAbstractClass
Gesh Markov - Improve error message for incorrect configuration file
Patrick Pilch - Rule improvement: ReturnCount
Serj Lotutovici - Rule improvement: LongParameterList
Dmitry Primshyts - Rule improvement: MagicNumber
Egor Neliuba - Rule improvement: EmptyFunctionBlock, EmptyClassBlock
Said Tahsin Dane - Gradle plugin improvements
Misa Torres - Added: TrailingWhitespace and NoTabs rules
R.A. Porter - Updated Readme links to RuleSets
Robbin Voortman - Rule improvement: MaxLineLength
Mike Gorunov — Rule improvement: UndocumentedPublicFunction
Joey Kaan - New rule: MandatoryBracesIfStatements
Dmitriy Samaryan - Rule fix: SerialVersionUIDInSerializableClass
Mariano Simone - Rule improvement: UnusedPrivateMember. New Rules: UnusedPrivateClass, VarCouldBeVal
Shunsuke Maeda - Fix: to work on multi module project using maven plugin
Mikhail Levchenko - New rules: Unnecessary let, ExplicitItLambdaParameter
Scott Kennedy - Minor fixes
Mickele Moriconi - Added: ConstructorParameterNaming and FunctionParameterNaming rules
Lukasz Jazgar - Fixed configuring formatting rules
Pavlos-Petros Tournaris - Lazy evaluation of Regex in Rules
Erhard Pointl - Kotlin DSL and Gradle enhancements
Tyler Thrailkill - FunctionNaming rule enhancements
Tarek Belkahia - TooManyFunctions rule options
Bournane Abdelkrim - Fix typos
Rafael Toledo - Fix Gradle plugin badge
Alberto Ballano - ExceptionRaisedInUnexpectedLocation rule improvements
Guido Pio Mariotti - Documentation improvement
Mygod - UnusedImports rule improvement
Andreas Volkmann - yaml code comments
glammers - Documentation improvement
Ahmad El-Melegy - yaml syntax fix
Arjan Kleene - Add unnecessary apply rule
Paweł Gajda - Rule improvement: FunctionParameterNaming
Alistair Sykes - Doc improvement
Andrew Arnott - UnusedPrivateMember improvement
Tyler Wong - UnderscoresInNumericLiterals rule
Daniele Conti - ObjectPropertyNaming improvement
Nicola Corti - Fixed Suppress of MaxLineLenght
Michael Lotkowski - Rule improvement: False positive UnusedImport for componentN
Nuno Caro - Adds TXT report support on Gradle plugin
Minsuk Eom - Rule fix: PackageNaming
Jonas Alves - Rule fix: MagicNumber with ignoreNamedArgument and a negative value
Natig Babayev - Readme improvements
David Phillips - New rule: MandatoryBracesLoops
Volkan Şahin - Documentation improvement
Remco Mokveld - Rename Blacklist/Whitelist to more meaningful names
Zachary Moore - Rule, cli, gradle plugin, and config improvements
Veyndan Stuart - New rule: UseEmptyCounterpart; Rule improvement: UselessCallOnNotNull
Parimatch Tech - New rule: LibraryEntitiesShouldNotBePublic, UnnecessaryFilter
Chao Zhang - SARIF report format; Rule improvements
Marcelo Hernandez - New rule: SuspendFunWithFlowReturnType, ObjectExtendsThrowable
Harold Martin - Rule improvement: ClassOrdering
Roman Ivanov - Rule improvement: ReturnFromFinally
Severn Everett - New rule: SleepInsteadOfDelay
Adam Kobor - New rule: MultilineLambdaItParameter
Slawomir Czerwinski - Rule improvement: FunctionOnlyReturningConstant
Ivo Smid - Fix Local development on Windows
Krzysztof Kruczynski - Rule fix: ThrowingExceptionInMain, ExitOutsideMain
Paya Do - Designer for Detekt's logo
zmunm - New rule: ObjectLiteralToLambda
Vinicius Montes Munhoz - Documentation improvement
Eliezer Graber - Rule fix: ModifierOrder
Dominik Labuda - Gradle plugin improvement

Mentions


As mentioned in...

KotlinConf 2018 - Safe(r) Kotlin Code - Static Analysis Tools for Kotlin by Marvin Ramin
droidcon NYC 2018 - Static Code Analysis For Kotlin
Kotlin on Code Quality Tools - by @vanniktech Slides Presentation
@medium/acerezoluna/static-code-analysis-tools-for-kotlin-in-android
@medium/annayan/writing-custom-lint-rules-for-your-kotlin-project-with-detekt
Free Continuous Integration for modern Android apps with CircleCI
Static code analysis for Kotlin in Android
The Art of Android DevOps
Android Basics: Continuous Integration
Kotlin Static Analysis — why and how?
Check the quality of Kotlin code
Kotlin Static Analysis Tools
Speeding up the detekt task in a multi-project Gradle build (for detekt < 1.7.0)
SBCARS '18 -  Are you still smelling it?: A comparative study between Java and Kotlin language by Flauzino et al.
Preventing software antipatterns with Detekt

Integrations:

SonarKotlin
Codacy
Gradle plugin that generates ErrorProne, Findbugs, Checkstyle, PMD, CPD, Lint, Detekt & Ktlint Tasks for every subproject
Java library for parsing report files from static code analysis
sputnik is a free tool for static code review and provides support for detekt
Novoda Gradle Static Analysis plugin
Maven plugin that wraps the Detekt CLI
Bazel plugin that wraps the Detekt CLI
Gradle plugin that helps facilitate GitHub PR checking and automatic commenting of violations
Codefactor
GitHub Action: Detekt All
IntelliJ Platform Plugin Template
MuseDev

Custom rules and reports from 3rd parties:

cph-cachet/detekt-verify-implementation
detekt-hint is a plugin to detekt that provides detection of design principle violations through integration with Danger
GitLab report format

Credits

JetBrains - Creating IntelliJ + Kotlin
PMD & Checkstyle & ktlint - Ideas for threshold values and style rules

",batch2,8:10:18,Done
85,mathiaspet/gms,"Apache Flink (incubating)
Apache Flink is an open source system for expressive, declarative, fast, and efficient data analysis. Flink combines the scalability and programming flexibility of distributed MapReduce-like platforms with the efficiency, out-of-core execution, and query optimization capabilities found in parallel databases.
Learn more about Flink at http://flink.incubator.apache.org/
Build Apache Flink
Build From Source
Requirements

Unix-like environment (We use Linux, Mac OS X, Cygwin)
git
Maven (at least version 3.0.4)
Java 6, 7 or 8 (Note that Oracle's JDK 6 library will fail to build Flink, but is able to run a pre-compiled package without problem)

git clone https://github.com/apache/incubator-flink.git
cd incubator-flink
mvn clean package -DskipTests # this will take up to 5 minutes

Flink is now installed in flink-dist/target
Support
Don’t hesitate to ask!
Please contact the developers on our mailing lists if you need help.
Open an issue if you found a bug in Flink.
Documentation
The documentation of Apache Flink is located on the website: http://flink.incubator.apache.org or in the docs/ directory of the source code.
Fork and Contribute
This is an active open-source project. We are always open to people who want to use the system or contribute to it.
Contact us if you are looking for implementation tasks that fit your skills.
This article describes how to contribute to Apache Flink.
About
Apache Flink is an effort undergoing incubation at The Apache Software Foundation (ASF), sponsored by the Apache Incubator PMC. Incubation is required of all newly accepted projects until a further review indicates that the infrastructure, communications, and decision making process have stabilized in a manner consistent with other successful ASF projects. While incubation status is not necessarily a reflection of the completeness or stability of the code, it does indicate that the project has yet to be fully endorsed by the ASF.
The Apache Flink project originated from the Stratosphere research project.
",batch2,8:09:16,Done
86,kfowler/emacs-mac,,batch1,16:59:00,Done
87,prevorus/botik,"










Discord (Chat & Get Help!): https://discord.gg/VXKxNFr 

NECROBOT - by the community, for the community.
Getting Started
Make sure you check out our [Wiki](https://github.com/NecronomiconCoding/NecroBot/wiki) to get started.

Donating
Feel free to buy us all a beer, by using PayPal:
[![Donate](https://www.paypalobjects.com/en_US/i/btn/btn_donate_LG.gif)](https://www.paypal.com/cgi-bin/webscr?cmd=_donations&business=farhaninoor1%40gmail%2ecom&lc=GB&item_name=NecroBot%20Donations&item_number=POGO&no_note=0¤cy_code=USD&bn=PP%2dDonationsBF%3abtn_donateCC_LG_global%2egif%3aNonHostedGuest)
or by using Bitcoin: 1LxBH4FHhwyEuL1eHbMvLiGsTiBrzjfa1C
[A big part of the Donations goes to the top 3 active Contributors. Other expenses as Server costs will be covered using the Donations.]
Features

[PTC Login / Google]
[Get Map Objects and Inventory]
[Search for gyms/pokestops/spawns]
[Farm pokestops]
[Farm all Pokemon in neighbourhood]
[Throw Berries/use best pokeball]
[Transfers duplicate pokemons]
[Evolve all pokemons]
[Throws away unneeded items]
[Humanlike Walking]
[Configurable Custom Pathing]
[Softban bypass]
[AutoUpdate / VersionCheck]
[Multilanguage Support]
[Use lucky egg while evolve]
[Egg Hatching Automatically]
[Multi bot support]
[Snipe pokemon]
[Power-Up pokemon]
[Telegram Remote Control Support]

Credits
FeroxRev - RocketAPI
LineWalker - POGOProtos-0.31.0
Valmere - TransferWeakPokemon
AeonLucid - POGOProtos

Thanks to everyone who volunteered by contributing via Pull Requests!
Legal
This Website and Project is in no way affiliated with, authorized, maintained, sponsored or endorsed by Niantic, The Pokémon Company, Nintendo or any of its affiliates or subsidiaries. This is an independent and unofficial API for educational use ONLY.
Using the Project might be against the TOS
Inquiries: contact@necrobot.io

",batch2,8:09:16,Done
88,vaughan-rich/gitfiti-wall,"Gitfiti Wall 🎨
A dummy repository for messing about with Gitfiti and Gitfiti Painter, to draw things like this

...and this

",batch1,16:59:00,Done
89,SelfKeyFoundation/Identity-Wallet,"Selfkey Identity Wallet
  
   
Coverage
master

dev

Overview
The Official SelfKey Identity Wallet for Desktop
Features

Storage of identity docs in the Identity Wallet locally on the user’s computer
Marketplace where the user can view a list of available Exchanges
Exchange Details Page where the user can view Exchange information and stake KEY tokens
System to fetch a user’s staking status (stake/no-stake) for an Exchange from the blockchain
Token staking and reclamation flow
Submission / upload of KYC package for processing to KYC-Chain
Transfer of ETH, KEY and Custom ERC20 Tokens
Viewing of token balances and transaction logs for ETH, KEY and ERC20 Tokens

Prerequisites

NodeJS version 9 or better
Wine and RPM packages for OSX/Linux

Using Windows?

patch.exe (for snyk protect)
VC++ 2015.3 v14.00 (v140) toolset for desktop
Windows-Build-Tools

Install Dependencies
yarn
yarn install-app-deps

Build on OSX/macOS/Linux
yarn dist

Development
Scripts
  ""scripts"": {
    ""dev"": ""gulp && electron-webpack dev"",
    ""install-app-deps"": ""electron-builder install-app-deps buildDependenciesFromSource"",
    ""install-all"": ""yarn && yarn install-app-deps"",
    ""compile"": ""gulp && electron-webpack"",
    ""dist"": ""yarn compile && electron-builder"",
    ""dist:dir"": ""yarn dist --dir -c.compression=store -c.mac.identity=null"",
    ""test"": ""yarn test:unit && yarn test:e2e"",
    ""test:unit"": ""jest -i --forceExit"",
    ""test:unit:coverage"": ""yarn test:unit --coverage"",
    ""test:e2e"": ""node test/test.js e2e"",
    ""publish-build"": ""yarn compile && electron-builder -p always"",
    ""precommit"": ""npm run check-deps-precommit && pretty-quick --staged && lint-staged"",
    ""commitmsg"": ""commitlint -E GIT_PARAMS"",
    ""check-deps-precommit"": ""npm-check -i eslint -i redux -s || true"",
    ""check-deps"": ""npm-check -i common"",
    ""coveralls"": ""cat dist/coverage/lcov.info | coveralls""
  }
Run the App on OSX/macOS/Linux/Windows
yarn dev

Tests
yarn test

Contributing
Please see the contributing notes.
License
The MIT License (MIT)
Copyright (c) 2018 SelfKey Foundation
Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:
The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
",batch2,8:09:15,Done
90,JoelCatantan/ci4cms,"CodeIgniter 4 Development



What is CodeIgniter?
CodeIgniter is a PHP full-stack web framework that is light, fast, flexible, and secure.
More information can be found at the official site.
This repository holds the source code for CodeIgniter 4 only.
Version 4 is a complete rewrite to bring the quality and the code into a more modern version,
while still keeping as many of the things intact that has made people love the framework over the years.
This is pre-release code and should not be used in production sites.
More information about the plans for version 4 can be found in the announcement on the forums.
Documentation
The User Guide is the primary documentation for CodeIgniter 4.
The current in-progress User Guide can be found here.
As with the rest of the framework, it is a work in progress, and will see changes over time to structure, explanations, etc.
You might also be interested in the API documentation for the framework components.
Important Change with index.php
index.php is no longer in the root of the project! It has been moved inside the public folder,
for better security and separation of components.
This means that you should configure your web server to ""point"" to your project's public folder, and
not to the project root. A better practice would be to configure a virtual host to point there. A poor practice would be to point your web server to the project root and expect to enter public/..., as the rest of your logic and the
framework are exposed.
Please read the user guide for a better explanation of how CI4 works!
The user guide updating and deployment is a bit awkward at the moment, but we are working on it!
Repository Management
We use Github issues to track BUGS and to track approved DEVELOPMENT work packages.
We use our forum to provide SUPPORT and to discuss
FEATURE REQUESTS.
If you raise an issue here that pertains to support or a feature request, it will
be closed! If you are not sure if you have found a bug, raise a thread on the forum first -
someone else may have encountered the same thing.
Before raising a new Github issue, please check that your bug hasn't already
been reported or fixed.
We use pull requests (PRs) for CONTRIBUTIONS to the repository.
We are looking for contributions that address one of the reported bugs or
approved work packages.
Do not use a PR as a form of feature request.
Unsolicited contributions will only be considered if they fit nicely
into the framework roadmap.
Remember that some components that were part of CodeIgniter 3 are being moved
to optional packages, with their own repository.
Contributing
We are accepting contributions from the community!
We will try to manage the process somewhat, by adding a ""help wanted"" label to those that we are
specifically interested in at any point in time. Join the discussion for those issues and let us know
if you want to take the lead on one of them.
At this time, we are not looking for out-of-scope contributions, only those that would be considered part of our controlled evolution!
Please read the Contributing to CodeIgniter section in the user guide.
Server Requirements
PHP version 7.2 or higher is required, with the following extensions installed:

intl
libcurl if you plan to use the HTTP\CURLRequest library

Additionally, make sure that the following extensions are enabled in your PHP:

json (enabled by default - don't turn it off)
xml (enabled by default - don't turn it off)
mbstring
mysqlnd

Running CodeIgniter Tests
Information on running the CodeIgniter test suite can be found in the README.md file in the tests directory.
",batch2,8:09:15,Done
91,ProgrammingLife2017/hygene,"





Hygene is a multiple-genome graph visualizer for the JVM.
Visualizing Multiple-Genome Graphs

Inspecting Annotations

Demo Video
Watch the demo video to get an impression of Hygene.
The Team



Joël Abrahams
Georgios Andreadis
Casper Boone
Niels de Bruin
Felix Dekker












",batch2,8:10:17,Done
92,lysevi/dariadb,"dariadb - numeric time-series database.
Continuous Integration



version
build & tests
test coverage




master




develop





Features

True columnar storage
Can be used as a server application or an embedded library.
Full featured http api.
Golang client (see folders ""go"" and ""examples/go"")
Accept unordered data.
Each measurement contains:

Id - x32 unsigned integer value.
Time - x64 timestamp.
Value - x64 float.
Flag - x32 unsigned integer.


Write strategies:

wal - little cache and all values storing to disk in write ahead log. optimised for big write load(but slower than 'memory' strategy).
compressed - all values compressed for good disk usage without writing to sorted layer.
memory - all values stored in memory and dropped to disk when memory limit is ended.
cache - all values stored in memory with writes to disk.
memory-only - all valeus stored only in memory.


LSM-like storage struct with three layers:

Memory cache or Append-only files layer, for fast write speed and crash-safety(if strategy is 'wal').
Old values stored in compressed block for better disk space usage.


High write speed:

as embedded engine - to disk - 1.5 - 3.5 millions values per second to disk
as memory storage(when strategy is 'memory') - 7-9 millions.
across the network - 700k - 800k values per second


Shard-engine: you can split values per shard in disk, for better compaction and read speed up.
Crash recovery.
CRC32 for all values.
Two variants of API:

Functor API (async) -  engine apply given function to each measurement in the incoming request.
Standard API - You can Query interval as list or values in time point as dictionary.


Compaction old data with filtration support;
Statistic:

time min/max
value min/max
measurement count
values sum


Statistical functions:

minimum
maximum
count
average
median
sigma(standard deviation)
percentile90
percentile99


Interval aggregation support. Available intervals: raw,minute, half hour, hour, day, week, month, year.

Usage example

See folder ""examples""
How to use dariadb as a embedded storage engine: dariadb-example

Dependencies

Boost 1.54.0 or higher: system, filesystem, date_time,regex, program_options, asio.
cmake 3.1 or higher
c++ 14/17 compiler (MSVC 2015, gcc 6.0, clang 3.8)

Build

Install dependencies
$ sudo add-apt-repository -y ppa:ubuntu-toolchain-r/test
$ sudo apt-get update
$ sudo apt-get install -y libboost-dev  libboost-filesystem-dev libboost-program-options-dev libasio-dev libboost-date-time-dev cmake  g++-6  gcc-6 cpp-6 
$ export CC=""gcc-6""
$ export CXX=""g++-6""
Jemalloc
Optionaly you can install jemalloc for better memory usage.
$ sudo apt-get install libjemalloc-dev
Or you may use builtin jemalloc source in dariadb  - just add build option -DSYSTEM_JEMALLOC=OFF
Git submodules
$ cd dariadb
$ git submodules init 
$ git submodules update
Available build options

DARIADB_ENABLE_TESTS - Enable testing of the dariadb. - ON
DARIADB_ENABLE_INTEGRATION_TESTS - Enable integration test. - ON
DARIADB_ENABLE_SERVER - Enable build dariadb server. - ON
DARIADB_ENABLE_BENCHMARKS - Enable build dariadb benchmarks. - ON
DARIADB_ENABLE_SAMPLES - Build dariadb sample programs. - ON
DARIADB_ASAN_UBSAN  - Enable address & undefined behavior sanitizer for binary. - OFF
DARIADB_MSAN - Enable memory sanitizer for binary. - OFF
DARIADB_SYSTEM_JEMALLOC - Use jemalloc installed in the system. - ON

Configure to build with all benchmarks, but without tests and server.

$ cmake  -DCMAKE_BUILD_TYPE=Release -DDARIADB_ENABLE_TESTS=OFF -DDARIADB_ENABLE_INTEGRATION_TESTS=OFF -DDARIADB_ENABLE_BENCHMARKS=ON -DDARIADB_ENABLE_SERVER=OFF . 
clang

Clang currently does not supported.
$ cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_FLAGS_RELEASE=""${CMAKE_CXX_FLAGS_RELEASE} -stdlib=libc++"" -DCMAKE_EXE_LINKER_FLAGS=""${CMAKE_EXE_LINKER_FLAGS} -lstdc++"" .
$ make
gcc

$ cmake -DCMAKE_BUILD_TYPE=Release .
$ make
Microsoft Visual Studio

$ cmake -G ""Visual Studio 14 2015 Win64"" .
$ cmake --build .
if you want to build benchmarks and tests
$ cmake -G ""Visual Studio 14 2015 Win64"" -DBUILD_SHARED_LIBS=FALSE  .
$ cmake --build .
build with non system installed boost

$ cmake  -DCMAKE_BUILD_TYPE=Release -DBOOST_ROOT=""path/to/boost/"" .
$ make
",batch2,8:10:17,Done
93,findsorguk/findsorguk,"The Portable Antiquities Scheme's Database source code
 

A repository for the current generation of the Portable Antiquities Scheme website. A British Museum project that
encourages the voluntary recording of archaeological artefacts found by the public in England and Wales. A working
version of this software can be seen at https://finds.org.uk or at https://marinefinds.org.uk
and comprehensive instructions about how to install a copy can be found in the wiki.
Requirements
The database is written with a Linux operating system in mind.
Operating system requirements:

Recommended Ubuntu 16.04 or other LTS
PHP 5.4+
Solr 4.2
MySQL 5.7
APC Cache or equivalent
Curl
ImageMagick
GD
PDO

PHP libraries
Most of the PHP libaries are included as submodules, which will be pulled on deployment via Git on your server.

Zend Framework 1.12.21dev
Solarium 2
ZendX_JQuery
HTMLPurifier
EasyBib
mpdf
imagecow

Geographical boundaries
The system also makes use of converted OS boundary data in geoJSON format (also included as a submodule.) These are also maintained in a Github repository.
Search index
You will also need to install and configure SOLR for the system to be fully functioning. The schemas for this are
located in our SOLR repo.
Virtual host configuration
A set of example virtual host configuration files for Apache 2.4 are available.
SSL
It is recommended that the system is configured to use https and the free service offered by Letsencrypt is ideal.
Contributing
We welcome code contributions to make this system better. Please refer to our contributing guidelines before submitting patches or new features.
Changes
A change log is now maintained by the project developers to try and explain how things are being improved.
Development team

Daniel Pett
Mary Chester-Kadwell
Minakshi Chidrawar
Stephen Moon
Adetokunbo Aribilola

The Scheme is also grateful for the contributions of Richard Wareham (Cambridge University) and Ethan Gruber (ANS). Full acknowledgements are recorded within this repo.
License
The codebase is released under GPL V3.
Funded by

Heritage Lottery Fund
DCMS
Arts and Humanities Research Council
The British Museum

",batch2,8:09:16,Done
94,qinwang13/MariaDB,,batch1,16:57:59,Done
95,zhangsiqi951016/java-spring-security,"







Spring Security

Spring Security provides security services for the Spring IO Platform. Spring Security 3.1 requires Spring 3.0.3 as
a minimum and also requires Java 5.


For a detailed list of features and access to the latest release, please visit Spring projects.


Code of Conduct


This project adheres to the Contributor Covenant code of conduct.
By participating, you  are expected to uphold this code. Please report unacceptable behavior to spring-code-of-conduct@pivotal.io.




Downloading Artifacts


See downloading Spring artifacts for Maven repository information.




Documentation


Be sure to read the Spring Security Reference.
Extensive JavaDoc for the Spring Security code is also available in the Spring Security API Documentation.




Quick Start


We recommend you visit Spring Security Reference and read the ""Getting Started"" page.




Building from Source


Spring Security uses a Gradle-based build system.
In the instructions below, ./gradlew is invoked from the root of the source tree and serves as
a cross-platform, self-contained bootstrap mechanism for the build.


Prerequisites

Git and the JDK7 build.


Be sure that your JAVA_HOME environment variable points to the jdk1.7.0 folder extracted from the JDK download.



Check out sources


git clone git@github.com:spring-projects/spring-security.git




Install all spring-\* jars into your local Maven cache


./gradlew install




Compile and test; build all jars, distribution zips, and docs


./gradlew build



Discover more commands with ./gradlew tasks.
See also the Gradle build and release FAQ.





Getting Support


Check out the Spring Security tags on Stack Overflow.
Commercial support is available too.




Contributing


Pull requests are welcome; see the contributor guidelines for details.




License


Spring Security is Open Source software released under the
Apache 2.0 license.


",batch2,8:09:15,Done
96,asoltys/elementsjs-lib,"BitcoinJS (bitcoinjs-lib)



A javascript Bitcoin library for node.js and browsers. Written in TypeScript, but committing the JS files to verify.
Released under the terms of the MIT LICENSE.
Should I use this in production?
If you are thinking of using the master branch of this library in production, stop.
Master is not stable; it is our development branch, and only tagged releases may be classified as stable.
Can I trust this code?

Don't trust. Verify.

We recommend every user of this library and the bitcoinjs ecosystem audit and verify any underlying code for its validity and suitability,  including reviewing any and all of your project's dependencies.
Mistakes and bugs happen, but with your help in resolving and reporting issues, together we can produce open source software that is:

Easy to audit and verify,
Tested, with test coverage >95%,
Advanced and feature rich,
Standardized, using prettier and Node Buffer's throughout, and
Friendly, with a strong and helpful community, ready to answer questions.

Documentation
Presently,  we do not have any formal documentation other than our examples, please ask for help if our examples aren't enough to guide you.
Installation
npm install bitcoinjs-lib
Typically we support the Node Maintenance LTS version.
If in doubt, see the .travis.yml for what versions are used by our continuous integration tests.
WARNING: We presently don't provide any tooling to verify that the release on npm matches GitHub.  As such, you should verify anything downloaded by npm against your own verified copy.
Usage
Crypto is hard.
When working with private keys, the random number generator is fundamentally one of the most important parts of any software you write.
For random number generation, we default to the randombytes module, which uses window.crypto.getRandomValues in the browser, or Node js' crypto.randomBytes, depending on your build system.
Although this default is ~OK, there is no simple way to detect if the underlying RNG provided is good enough, or if it is catastrophically bad.
You should always verify this yourself to your own standards.
This library uses tiny-secp256k1, which uses RFC6979 to help prevent k re-use and exploitation.
Unfortunately, this isn't a silver bullet.
Often, Javascript itself is working against us by bypassing these counter-measures.
Problems in Buffer (UInt8Array), for example, can trivially result in catastrophic fund loss without any warning.
It can do this through undermining your random number generation, accidentally producing a duplicate k value, sending Bitcoin to a malformed output script, or any of a million different ways.
Running tests in your target environment is important and a recommended step to verify continuously.
Finally, adhere to best practice.
We are not an authorative source of best practice, but, at the very least:

Don't re-use addresses.
Don't share BIP32 extended public keys ('xpubs'). They are a liability, and it only takes 1 misplaced private key (or a buggy implementation!) and you are vulnerable to catastrophic fund loss.
Don't use Math.random - in any way - don't.
Enforce that users always verify (manually) a freshly-decoded human-readable version of their intended transaction before broadcast.
Don't ask users to generate mnemonics, or 'brain wallets',  humans are terrible random number generators.
Lastly, if you can, use Typescript or similar.

Browser
The recommended method of using bitcoinjs-lib in your browser is through Browserify.
If you're familiar with how to use browserify, ignore this and carry on, otherwise, it is recommended to read the tutorial at https://browserify.org/.
NOTE: We use Node Maintenance LTS features, if you need strict ES5, use --transform babelify in conjunction with your browserify step (using an es2015 preset).
WARNING: iOS devices have problems, use atleast buffer@5.0.5 or greater,  and enforce the test suites (for Buffer, and any other dependency) pass before use.
Typescript or VSCode users
Type declarations for Typescript are included in this library. Normal installation should include all the needed type information.
Examples
The below examples are implemented as integration tests, they should be very easy to understand.
Otherwise, pull requests are appreciated.
Some examples interact (via HTTPS) with a 3rd Party Blockchain Provider (3PBP).

Generate a random address
Import an address via WIF
Generate a 2-of-3 P2SH multisig address
Generate a SegWit address
Generate a SegWit P2SH address
Generate a SegWit 3-of-4 multisig address
Generate a SegWit 2-of-2 P2SH multisig address
Support the retrieval of transactions for an address (3rd party blockchain)
Generate a Testnet address
Generate a Litecoin address
Create a 1-to-1 Transaction
Create (and broadcast via 3PBP) a typical Transaction
Create (and broadcast via 3PBP) a Transaction with an OP_RETURN output
Create (and broadcast via 3PBP) a Transaction with a 2-of-4 P2SH(multisig) input
Create (and broadcast via 3PBP) a Transaction with a SegWit P2SH(P2WPKH) input
Create (and broadcast via 3PBP) a Transaction with a SegWit P2WPKH input
Create (and broadcast via 3PBP) a Transaction with a SegWit P2PK input
Create (and broadcast via 3PBP) a Transaction with a SegWit 3-of-4 P2SH(P2WSH(multisig)) input
Create (and broadcast via 3PBP) a Transaction and sign with an HDSigner interface (bip32)
Import a BIP32 testnet xpriv and export to WIF
Export a BIP32 xpriv, then import it
Export a BIP32 xpub
Create a BIP32, bitcoin, account 0, external address
Create a BIP44, bitcoin, account 0, external address
Create a BIP49, bitcoin testnet, account 0, external address
Use BIP39 to generate BIP32 addresses
Create (and broadcast via 3PBP) a Transaction where Alice can redeem the output after the expiry (in the past)
Create (and broadcast via 3PBP) a Transaction where Alice can redeem the output after the expiry (in the future)
Create (and broadcast via 3PBP) a Transaction where Alice and Bob can redeem the output at any time
Create (but fail to broadcast via 3PBP) a Transaction where Alice attempts to redeem before the expiry
Create (and broadcast via 3PBP) a Transaction where Alice can redeem the output after the expiry (in the future) (simple CHECKSEQUENCEVERIFY)
Create (but fail to broadcast via 3PBP) a Transaction where Alice attempts to redeem before the expiry (simple CHECKSEQUENCEVERIFY)
Create (and broadcast via 3PBP) a Transaction where Bob and Charles can send (complex CHECKSEQUENCEVERIFY)
Create (and broadcast via 3PBP) a Transaction where Alice (mediator) and Bob can send after 2 blocks (complex CHECKSEQUENCEVERIFY)
Create (and broadcast via 3PBP) a Transaction where Alice (mediator) can send after 5 blocks (complex CHECKSEQUENCEVERIFY)

If you have a use case that you feel could be listed here, please ask for it!
Contributing
See CONTRIBUTING.md.
Running the test suite
npm test
npm run-script coverage
Complementing Libraries

BIP21 - A BIP21 compatible URL encoding library
BIP38 - Passphrase-protected private keys
BIP39 - Mnemonic generation for deterministic keys
BIP32-Utils - A set of utilities for working with BIP32
BIP66 - Strict DER signature decoding
BIP68 - Relative lock-time encoding library
BIP69 - Lexicographical Indexing of Transaction Inputs and Outputs
Base58 - Base58 encoding/decoding
Base58 Check - Base58 check encoding/decoding
Bech32 - A BIP173 compliant Bech32 encoding library
coinselect - A fee-optimizing, transaction input selection module for bitcoinjs-lib.
merkle-lib - A performance conscious library for merkle root and tree calculations.
minimaldata - A module to check bitcoin policy: SCRIPT_VERIFY_MINIMALDATA

Alternatives

BCoin
Bitcore
Cryptocoin

LICENSE MIT
",batch2,8:09:16,Done
97,Zachac/RanvierMUD-Experiments,"RanvierMUD Experiments
Fork of https://github.com/RanvierMUD/ranviermud/
A continued attempt to create a mudlike game. This time with RanvierMUD codebase.
",batch2,8:10:17,Done
98,YASHCODE8/CODE-MIRROROR,"CodeMirroror



Funding status: 
CodeMirror is a versatile text editor implemented in JavaScript for
the browser. It is specialized for editing code, and comes with over
100 language modes and various addons that implement more advanced
editing functionality.
A rich programming API and a CSS theming system are available for
customizing CodeMirror to fit your application, and extending it with
new functionality.
You can find more information (and the
manual) on the project
page. For questions and discussion, use the
discussion forum.
See
CONTRIBUTING.md
for contributing guidelines.
The CodeMirror community aims to be welcoming to everybody. We use the
Contributor Covenant
(1.1) as our code of
conduct.
Quickstart
To build the project, make sure you have Node.js installed (at least version 6)
and then npm install. To run, just open index.html in your
browser (you don't need to run a webserver). Run the tests with npm test.
",batch2,8:09:15,Done
99,llovo-code/python-plotly,"plotly.py

📢  Announcement!
Registration is open for a 2 day, Dash master class in Montreal, February 17-18.
Register online here 🎚📈 🇨🇦


plotly.py is an interactive, browser-based graphing library for Python ✨
Built on top of plotly.js, plotly.py is a high-level, declarative charting library. plotly.js ships with over 30 chart types, including scientific charts, 3D graphs, statistical charts, SVG maps, financial charts, and more.
plotly.py is MIT Licensed. Plotly graphs can be viewed in Jupyter notebooks, standalone HTML files, or hosted online on plot.ly.
Contact us for Plotly.js consulting, dashboard development, application integration, and feature additions. Sharing your graphs online or in dashboards? Consider a plot.ly subscription.






Online Documentation
contributing.md
Code of Conduct
New! Announcing Dash
Community


Code and documentation copyright 2017 Plotly, Inc.
Code released under the MIT license.
Docs released under the Creative Commons license.
",batch2,8:10:17,Done
100,immense055/Homebrew-brew,"Linuxbrew Core
Core formulae for the Homebrew package manager.
Homebrew/discussions (forum)
How do I install these formulae?
Just brew install <formula>. This is the default tap for Homebrew and is installed by default.
More Documentation, Troubleshooting, Contributing, Security, Community, Donations, License and Sponsors
See these sections in Homebrew/brew's README.
",batch1,16:57:58,Done
101,stickz/Redstone,"Redstone Nuclear Dawn Server Project
Get Started Using Github Here: https://guides.github.com/activities/hello-world/
The current stable build environment for this github is sourcemod 1.10 build 6453.
Compile checks are run with sourcemod 1.10 build 6453 to point out future issues.
Bugs/Issues with sourcemod can be fixed here https://github.com/alliedmodders/sourcemod
Scripting Resources
https://wiki.alliedmods.net/Introduction_to_sourcemod_plugins
https://wiki.alliedmods.net/Nuclear_Dawn_Events
https://sm.alliedmods.net/new-api/
Build plugins
Run make to compile addons/sourcemod/scripting/*.sp files using latest stable build of SourceMod or make build-dev to compile using latest dev build.
Deployment
If you have write access to this repo then following command will compile and
deploy all the files required by server to the build branch upstream:
export GH_TOKEN=<your token>; make build deploy

You won't likely need to do this manually though because Travis CI does
that automatically each time master branch changes.
",batch2,8:10:17,Done
102,MixinNetwork/ios-app,"Mixin iOS app
Mixin iOS messenger, wallet and light node to the Mixin Network
Requirements
Build

Xcode 12
Swift 5

Deployment target

iOS 12.0

",batch2,8:10:17,Done
103,prosenkiewicz/mycroft-core-new-ivona,"   
 


Mycroft
Mycroft is a hackable open source voice assistant.
Table of Contents

Table of Contents
Getting Started
Running Mycroft
Using Mycroft

Home Device and Account Manager
Skills


Behind the scenes

Pairing Information
Configuration
Using Mycroft Without Home
API Key Services
Using Mycroft behind a proxy

Using Mycroft behind a proxy without authentication
Using Mycroft behind an authenticated proxy




Getting Involved
Links

Getting Started
First, get the code on your system!  The simplest method is via git (git installation instructions):

cd ~/
git clone https://github.com/MycroftAI/mycroft-core.git
cd mycroft-core
bash dev_setup.sh

This script sets up dependencies and a virtualenv.  If running in an environment besides Ubuntu/Debian, Arch or Fedora you may need to manually install packages as instructed by dev_setup.sh.
NOTE: The default branch for this repository is 'dev', which should be considered a work-in-progress. If you want to clone a more stable version, switch over to the 'master' branch.
Running Mycroft
Mycroft provides start-mycroft.sh to perform common tasks. This script uses a virtualenv created by dev_setup.sh.  Assuming you installed mycroft-core in your home directory run:

cd ~/mycroft-core
./start-mycroft.sh debug

The ""debug"" command will start the background services (microphone listener, skill, messagebus, and audio subsystems) as well as bringing up a text-based Command Line Interface (CLI) you can use to interact with Mycroft and see the contents of the various logs. Alternatively you can run ./start-mycroft.sh all to begin the services without the command line interface.  Later you can bring up the CLI using ./start-mycroft.sh cli.
The background services can be stopped as a group with:

./stop-mycroft.sh

Using Mycroft
Home Device and Account Manager
Mycroft AI, Inc. maintains a device and account management system known as Mycroft Home. Developers may sign up at: https://home.mycroft.ai
By default, mycroft-core  is configured to use Home. By saying ""Hey Mycroft, pair my device"" (or any other request verbal request) you will be informed that your device needs to be paired. Mycroft will speak a 6-digit code which you can enter into the pairing page within the Mycroft Home site.
Once paired, your unit will use Mycroft API keys for services such as Speech-to-Text (STT), weather and various other skills.
Skills
Mycroft is nothing without skills.  There are a handful of default skills that are downloaded automatically to your /opt/mycroft/skills directory, but most need to be installed explicitly.  See the Skill Repo to discover skills made by others.  And please share your own interesting work!
Behind the scenes
Pairing Information
Pairing information generated by registering with Home is stored in:
~/.mycroft/identity/identity2.json <-- DO NOT SHARE THIS WITH OTHERS!
Configuration
Mycroft configuration consists of 4 possible locations:

mycroft-core/mycroft/configuration/mycroft.conf(Defaults)
Mycroft Home (Remote)
/etc/mycroft/mycroft.conf(Machine)
$HOME/.mycroft/mycroft.conf(User)

When the configuration loader starts, it looks in these locations in this order, and loads ALL configurations. Keys that exist in multiple configuration files will be overridden by the last file to contain the value. This process results in a minimal amount being written for a specific device and user, without modifying default distribution files.
Using Mycroft Without Home
If you do not wish to use the Mycroft Home service, before starting Mycroft for the first time, create $HOME/.mycroft/mycroft.conf with the following contents:
{
  ""skills"": {
    ""blacklisted_skills"": [
      ""mycroft-configuration.mycroftai"",
      ""mycroft-pairing.mycroftai""
    ]
  }
}

Mycroft will then be unable to perform speech-to-text conversion, so you'll need to set that up as well, using one of the STT engines Mycroft supports.
You may insert your own API keys into the configuration files listed above in Configuration.  For example, to insert the API key for the Weather skill, create a new JSON key in the configuration file like so:
{
  // other configuration settings...
  //
  ""WeatherSkill"": {
    ""api_key"": ""<insert your API key here>""
  }
}

API Key Services
These are the keys currently used in Mycroft Core:

STT API, Google STT, Google Cloud Speech
Weather Skill API, OpenWeatherMap
Wolfram-Alpha Skill

Using Mycroft behind a proxy
Many schools, universities and workplaces run a proxy on their network. If you need to type in a username and password to access the external internet, then you are likely behind a proxy.
If you plan to use Mycroft behind a proxy, then you will need to do an additional configuration step.
NOTE: In order to complete this step, you will need to know the hostname and port for the proxy server. Your network administrator will be able to provide these details. Your network administrator may want information on what type of traffic Mycroft will be using. We use https traffic on port 443, primarily for accessing ReST-based APIs.
Using Mycroft behind a proxy without authentication
If you are using Mycroft behind a proxy without authentication, add the following environment variables, changing the proxy_hostname.com and proxy_port for the values for your network. These commands are executed from the Linux command line interface (CLI).
$ export http_proxy=http://proxy_hostname.com:proxy_port
$ export https_port=http://proxy_hostname.com:proxy_port
$ export no_proxy=""localhost,127.0.0.1,localaddress,.localdomain.com,0.0.0.0,::1""
Using Mycroft behind an authenticated proxy
If  you are behind a proxy which requires authentication, add the following environment variables, changing the proxy_hostname.com and proxy_port for the values for your network. These commands are executed from the Linux command line interface (CLI).
$ export http_proxy=http://user:password@proxy_hostname.com:proxy_port
$ export https_port=http://user:password@proxy_hostname.com:proxy_port
$ export no_proxy=""localhost,127.0.0.1,localaddress,.localdomain.com,0.0.0.0,::1""
Getting Involved
This is an open source project and we would love your help. We have prepared a contributing guide to help you get started.
If this is your first PR or you're not sure where to get started,
say hi in Mycroft Chat and a team member would be happy to mentor you.
Join the Mycroft Forum for questions and answers.
Links

Creating a Skill
Documentation
Skill Writer API Docs
Release Notes
Mycroft Chat
Mycroft Forum
Mycroft Blog

",batch1,16:57:57,Done
104,nus-ncl/services-in-one,"services-in-one
All of NCL micro-services running under the same JVM.




Running the services
With Gradle, it is possible to run all services together in one process or just individual services. All services projects are named services-* where * is the name of the service.
Running all services in one process

Use a console and enter the root of the project
Type ./gradlew bootRun
The application should start up with all services running

Running individual services

Use a console and enter the root of the project
Type ./gradlew :service-<name>:bootRun (where <name> is the name of the service)
The application should start up with the selected service

",batch2,8:10:18,Done
105,nobody-z/iptables,,batch2,8:10:18,Done
106,maxo2/mycroft,"   
 


Mycroft
Mycroft is a hackable open source voice assistant.
Table of Contents

Table of Contents
Getting Started
Running Mycroft
Using Mycroft

Home Device and Account Manager
Skills


Behind the scenes

Pairing Information
Configuration
Using Mycroft Without Home
API Key Services
Using Mycroft behind a proxy

Using Mycroft behind a proxy without authentication
Using Mycroft behind an authenticated proxy




Getting Involved
Links

Getting Started
First, get the code on your system!  The simplest method is via git (git installation instructions):

cd ~/
git clone https://github.com/MycroftAI/mycroft-core.git
cd mycroft-core
bash dev_setup.sh

This script sets up dependencies and a virtualenv.  If running in an environment besides Ubuntu/Debian, Arch or Fedora you may need to manually install packages as instructed by dev_setup.sh.
NOTE: The default branch for this repository is 'dev', which should be considered a work-in-progress. If you want to clone a more stable version, switch over to the 'master' branch.
Running Mycroft
Mycroft provides start-mycroft.sh to perform common tasks. This script uses a virtualenv created by dev_setup.sh.  Assuming you installed mycroft-core in your home directory run:

cd ~/mycroft-core
./start-mycroft.sh debug

The ""debug"" command will start the background services (microphone listener, skill, messagebus, and audio subsystems) as well as bringing up a text-based Command Line Interface (CLI) you can use to interact with Mycroft and see the contents of the various logs. Alternatively you can run ./start-mycroft.sh all to begin the services without the command line interface.  Later you can bring up the CLI using ./start-mycroft.sh cli.
The background services can be stopped as a group with:

./stop-mycroft.sh

Using Mycroft
Home Device and Account Manager
Mycroft AI, Inc. maintains a device and account management system known as Mycroft Home. Developers may sign up at: https://home.mycroft.ai
By default, mycroft-core  is configured to use Home. By saying ""Hey Mycroft, pair my device"" (or any other request verbal request) you will be informed that your device needs to be paired. Mycroft will speak a 6-digit code which you can entered into the pairing page within the Mycroft Home site.
Once paired, your unit will use Mycroft API keys for services such as Speech-to-Text (STT), weather and various other skills.
Skills
Mycroft is nothing without skills.  There are a handful of default skills that are downloaded automatically to your /opt/mycroft/skills directory, but most need to be installed explicitly.  See the Skill Repo to discover skills made by others.  And please share your own interesting work!
Behind the scenes
Pairing Information
Pairing information generated by registering with Home is stored in:
~/.mycroft/identity/identity2.json <-- DO NOT SHARE THIS WITH OTHERS!
Configuration
Mycroft configuration consists of 4 possible locations:

mycroft-core/mycroft/configuration/mycroft.conf(Defaults)
Mycroft Home (Remote)
/etc/mycroft/mycroft.conf(Machine)
$HOME/.mycroft/mycroft.conf(User)

When the configuration loader starts, it looks in these locations in this order, and loads ALL configurations. Keys that exist in multiple configuration files will be overridden by the last file to contain the value. This process results in a minimal amount being written for a specific device and user, without modifying default distribution files.
Using Mycroft Without Home
If you do not wish to use the Mycroft Home service, you may insert your own API keys into the configuration files listed below in configuration.
The place to insert the API key looks like the following:
[WeatherSkill]
api_key = """"
Put a relevant key inside the quotes and mycroft-core should begin to use the key immediately.
API Key Services
These are the keys currently used in Mycroft Core:

STT API, Google STT, Google Cloud Speech
Weather Skill API, OpenWeatherMap
Wolfram-Alpha Skill

Using Mycroft behind a proxy
Many schools, universities and workplaces run a proxy on their network. If you need to type in a username and password to access the external internet, then you are likely behind a proxy.
If you plan to use Mycroft behind a proxy, then you will need to do an additional configuration step.
NOTE: In order to complete this step, you will need to know the hostname and port for the proxy server. Your network administrator will be able to provide these details. Your network administrator may want information on what type of traffic Mycroft will be using. We use https traffic on port 443, primarily for accessing ReST-based APIs.
Using Mycroft behind a proxy without authentication
If you are using Mycroft behind a proxy without authentication, add the following environment variables, changing the proxy_hostname.com and proxy_port for the values for your network. These commands are executed from the Linux command line interface (CLI).
$ export http_proxy=http://proxy_hostname.com:proxy_port
$ export https_port=http://proxy_hostname.com:proxy_port
$ export no_proxy=""localhost,127.0.0.1,localaddress,.localdomain.com,0.0.0.0,::1""
Using Mycroft behind an authenticated proxy
If  you are behind a proxy which requires authentication, add the following environment variables, changing the proxy_hostname.com and proxy_port for the values for your network. These commands are executed from the Linux command line interface (CLI).
$ export http_proxy=http://user:password@proxy_hostname.com:proxy_port
$ export https_port=http://user:password@proxy_hostname.com:proxy_port
$ export no_proxy=""localhost,127.0.0.1,localaddress,.localdomain.com,0.0.0.0,::1""
Getting Involved
This is an open source project and we would love your help. We have prepared a contributing guide to help you get started.
If this is your first PR or you're not sure where to get started,
say hi in Mycroft Chat and a team member would be happy to mentor you.
Join the Mycroft Forum for questions and answers.
Links

Creating a Skill
Documentation
Skill Writer API Docs
Release Notes
Mycroft Chat
Mycroft Forum
Mycroft Blog

",batch1,16:57:57,Done
107,sagargohil91/AzureTest,"
Kudu
Kudu is the engine behind git deployments in Azure App Service. It can also run outside of Azure.

Documentation
See the documentation
License
Apache License 2.0
Project governance
Details here
Questions?
You can use the forum, StackOverflow, or open issues in this repository.
This project is under the benevolent umbrella of the .NET Foundation.
",batch2,8:09:16,Done
108,IkeLewis/emacs,,batch1,16:59:00,Done
109,Kreymacs/Kreymacs,,batch1,16:58:59,Done
110,SuperNEMO-DBD/homebrew-core,"Linuxbrew Core
Core formulae for the Homebrew package manager.
How do I install these formulae?
Just brew install <formula>. This is the default tap for Homebrew and is installed by default.
More Documentation, Troubleshooting, Contributing, Security, Community, Donations, License and Sponsors
See these sections in Homebrew/brew's README.
",batch1,16:58:59,Done
111,ganeshghimire1986/VIC,"Variable Infiltration Capacity (VIC) Model



VIC Links & Badges





VIC Documentation



Travis Build



VIC Users Listserve



Developers Gitter Room



License



Current Release DOI





This repository serves as the public source code repository of the Variable Infiltration Capacity Model, better known as VIC. VIC documentation can be read on the VIC documentation website.
The Variable Infiltration Capacity (VIC) macroscale hydrological model (MHM) has been developed over the last two decades at the University of Washington and Princeton University in collaboration with a large number of other researchers around the globe. Development and maintenance of the official version of the VIC model is currently coordinated by the UW Hydro | Computational Hydrology group in the Department of Civil and Environmental Engineering at the University of Washington. All development activity is coordinated via the VIC github page, where you can also find all archived, current, beta, and development versions of the model.
A skeletal first version of the VIC model was introduced to the community by Wood et al. [1992] and a greatly expanded version, from which current variations evolved, is described by Liang et al. [1994]. As compared to other MHMs, VIC’s distinguishing hydrological features are its representation of subgrid variability in soil storage capacity as a spatial probability distribution to which surface runoff is related, and its parameterization of base flow, which occurs from a lower soil moisture zone as a nonlinear recession. Movement of moisture between the soil layers is modeled as gravity drainage, with the unsaturated hydraulic conductivity a function of the degree of saturation of the soil. Spatial variability in soil properties, at scales smaller than the grid scale, is represented statistically, without assigning infiltration parameters to specific subgrid locations. Over time, many additional features and representations of physical processes have been added to the model. VIC has been used in a large number of regional and continental scale (even global) hydrological studies. In 2016, VIC version 5 was released. This was a major update to the VIC source code focusing mainly on infrastructure improvements. The development of VIC-5 is detailed in Hamman et al. 2018. A selection of VIC applications can be found on the VIC references page.
Every new application addresses new problems and conditions that the model may not currently be able to handle, and as such the model is always under development. The VIC model is an open source development project, which means that contributions are welcome, including to the VIC documentation.
By placing the original source code archive on GitHub, we hope to encourage a more collaborative development environment. A tutorial on how to use the VIC git repository and how to contribute your changes to the VIC model can be found on the working with git page. The most stable version of the model is in the master branch, while beta versions of releases under development can be obtained from the development branch of this repository.
VIC is a research model developed by graduate students, post-docs and research scientists over a long period of time (since the early 1990s). Every new VIC application addresses new problems and conditions which the model may not currently be able to handle. As a result, the model is always under development. Because of the incremental nature of this development, not all sections of the code are equally mature and not every combination of model options has been exhaustively tested or is guaranteed to work. While you are more than welcome to use VIC in your own research endeavors, the model code comes with no guarantees, expressed or implied, as to suitability, completeness, accuracy, and whatever other claim you would like to make. In addition, the model has no graphical user interface, nor does it include a large set of analysis tools, since most people want to use their own set of tools.
While we would like to hear about your particular application (especially a copy of any published paper), we cannot give you individual support in setting up and running the model. The VIC documentation website includes reasonably complete instructions on how to run the model, as well as the opportunity to sign up for the VIC Users Email List. The VIC listserve should be used for questions about model setup and application. It is basically VIC users helping other VIC users. All other exchanges about VIC source code are managed through the VIC github page.
If you make use of this model, please acknowledge Liang et al. [1994] and Hamman et al. [2018] plus any other references appropriate to the features you used that are cited in the model overview.
",batch2,8:10:18,Done
112,otland/forgottenserver,"forgottenserver   
The Forgotten Server is a free and open-source MMORPG server emulator written in C++. It is a fork of the OpenTibia Server project. To connect to the server, you can use OTClient.
Getting Started

Compiling, alternatively download AppVeyor builds for Windows
Scripting Reference
Contributing

Support
If you need help, please visit the support forum on OTLand. Our issue tracker is not a support forum, and using it as one will result in your issue being closed. If you were unable to get assistance in the support forum, you should consider becoming a premium user on OTLand which grants you access to the premium support forum and supports OTLand financially.
Issues
We use the issue tracker on GitHub. Keep in mind that everyone who is watching the repository gets notified by e-mail when there is activity, so be thoughtful and avoid writing comments that aren't meaningful for an issue (e.g. ""+1""). If you'd like for an issue to be fixed faster, you should either fix it yourself and submit a pull request, or place a bounty on the issue.
",batch2,8:10:17,Done
113,genovese/emacs,,batch1,16:59:00,Done
114,KipperIalovskii/buttsv5,"
ranvier
Node.js-based MUD engine
Ranvier is a MUD game engine whose goal is to be a simple but powerful way to build whatever MUD you want with special care given to extensibility. The core code strives to be completely unopinionated toward any specific style of game while using the bundle system to build the game you want without having to dig through the engine's code.
Special Features

Robust bundle system: Nearly every aspect of the game can be modified without changing the core and allows for easy
packaging and sharing of commands/areas/items/npcs/channels/behaviors
Unopinionated network layer: easily swap out telnet for any network layer you like. No need to gut the whole codebase
just to support a different transport type, just drop in a file.
Customizable data layer: You are not tied to saving in any particular database or file storage sytem
Optional coordinate based room system allowing for the flexibilty of a standard MUD world with the easy mappability of
a strict 3D world.
Scripting for all entities in the game for any event along with behaviors to create shared, composable scripts
Skill system with passive/active skills
Effects e.g., buffs/debuffs
Quest system allowing for starting/progress/completion from any event in the game
Communication channels with custom audiences

Documentation
Ranvier prides itself on having thorough documentation which is available on our website: ranviermud.com
Slack
We have a Slack channel you can use to ask questions, suggest features, or just keep up to date with the project: https://ranviermud.slack.com
Get an invite
Requirements

Node.js >= v10.12.0

Demo
Point your favorite client or telnet to ranviermud.com port 4000. This demo server is wiped and updated from the master branch every hour.
",batch2,8:10:18,Done
115,jimmyleray/Emendare,"




Emendare      
What is Emendare ?
Emendare is a citizen, non-profit, distributed and open-source initiative. It helps groups share and improve texts iteratively.
The tool mimics the parliament process, allowing all participants to suggest alterations to a text, called amendments.
All groups seeking to organize themselves horizontally, be they companies, associations, political movements or local authorities, are welcome to use Emendare.
Technical stack
The current technical stack of Emendare is :
TypeScript as the main language for the entire project


Client side :

ReactJS as a framework for the web user interface
Bulma CSS as a css framework based on Flexbox



Instance side :

Node.js for the instance server
Socket.io for the client / server exchanges
MongoDB as database



Register side :

NestJS as a progressive NodeJS framework
TypeORM as an ORM layer between the application and the database
MongoDB as database



Getting started
These instructions explain how to run a local instance of Emendare for development, but also how to build the application for production.
Prerequisites
To set up the project, you must have Node.js installed.
To install Node.js, please refer to the Node.js official documentation
You can optionnaly install Docker-Compose to start more easily the Mongo database.
To install Docker-Compose, please refer to the Docker Compose official documentation
Installation
The installation of the project requires 2 steps: a configuration of the server and another of the client.
Database
The first step will be to create the MongoDB database. To do so, just create a docker container with a Mongo image, following the steps below.
cd server/
then create the Mongo image :
docker-compose build

This 'build' command must only be entered during installation.

you can finaly run the container :
docker-compose up
The database is accessible at the following address: localhost: 27017 / emendare.
Register configuration
This registry behaves like a service registry, where any instance of Emendare can register. It will permit to switch easly between different instances.
First install all the dependencies :
npm install
Then start the server either for production :
npm run build
npm start
Or for development (run these commands in two separates terminals) :
npm run dev
Server configuration
First install all the dependencies :
npm install
Then start the server either for production :
npm run build
npm start
Or for development (run these commands in two separates terminals) :
npm run dev
These commands will start the server at the following address : localhost:3030
Client configuration
To run the client, the first step will be to install the dependencies.
cd client/
npm install
The client can now be started in two modes :

in development :

npm run dev

in production :

npm run build
npm start
The app.js server will allow http redirection to https as well as Gzip compression of the application files.
Contributions
To contribute to this project please refer to Guide for contributors
",batch2,8:10:17,Done
116,saelinklaw/mariadb-server,,batch1,16:57:59,Done
117,pageboard/client,"pageboard client modules
install
npm install @pageboard/client
dev install
Install and run make.
modules


elements
the core client elements for rendering a page.
Exposes window.Pageboard, which is used to run things,
and to provide essential utils.


elements-write
the client libraries for edition, uses pagecut for block edition.


elements-semantic-ui
A full set of elements using the well known framework.


elements-gallery
Powerful portfolio/carousel/medialist combos.


elements-google
Translate, verify site owner, tag manager...


elements-mail
Edit mail pages like a boss.


elements-calendar
Core elements for managing events in a calendar.


pagecut
The core editor module, uses prosemirror to drive HTML wysiwyg editing.
It also contains a simple rendering part used by core pageboard client.


",batch2,8:09:16,Done
118,MasoniteFramework/masonite,"




  



NOTE: Masonite 2.3 is no longer compatible with the masonite-cli tool. Please uninstall that by running pip uninstall masonite-cli. If you do not uninstall masonite-cli you will have command clashes
About Masonite
The modern and developer centric Python web framework that strives for an actual batteries included developer tool with a lot of out of the box functionality with an extremely extendable architecture. Masonite is perfect for beginner developers getting into their first web applications as well as experienced devs that need to utilize the full potential of Masonite to get their applications done.
Masonite works hard to be fast and easy from install to deployment so developers can go from concept to creation in as quick and efficiently as possible. Use it for your next SaaS! Try it once and you’ll fall in love.

Having a simple and expressive routing engine
Extremely powerful command line helpers called craft commands
A simple migration system, removing the ""magic"" and finger crossing of migrations
A great Active Record style ORM called Orator
A great filesystem architecture for navigating and expanding your project
An extremely powerful Service Container (IOC Container)
Service Providers which makes Masonite extremely extendable

Learning Masonite
Masonite strives to have extremely comprehensive documentation. All documentation can be Found Here and would be wise to go through the tutorials there. If you find any discrepencies or anything that doesn't make sense, be sure to comment directly on the documentation to start a discussion!
If you are a visual learner you can find tutorials here: MasoniteCasts
Also be sure to join the Slack channel!
Contributing
Contributing to Masonite is simple:

Hop on Slack Channel! to ask any questions you need.
Read the How To Contribute documentation to see ways to contribute to the project.
Read the Contributing Guide to learn how to contribute to the core source code development of the project.
Read the Installation documentation on how to get started creating a Masonite project.
Check the open issues and milestones.
If you have any questions just open up an issue to discuss with the core maintainers.
Follow Masonite Framework on Twitter to get updates about tips and tricks, announcement and releases.

Requirements
In order to use Masonite, you’ll need:

Python 3.5+
Latest version of OpenSSL
Pip3


All commands of python and pip in this documentation is assuming they are pointing to the correct Python 3 versions. For example, anywhere you see the python command ran it is assuming that is a Python 3.5+ Python installation. If you are having issues with any installation steps just be sure the commands are for Python 3.5+ and not 2.7 or below.

Linux
If you are running on a Linux flavor, you’ll need the Python dev package and the libssl package. You can download these packages by running:
Debian and Ubuntu based Linux distributions
$ sudo apt-get install python-dev libssl-dev python3-pip

Or you may need to specify your python3.x-dev version:
$ sudo apt-get install python3.6-dev libssl-dev python3-pip

Enterprise Linux based distributions (Fedora, CentOS, RHEL, ...)
# dnf install python-devel openssl-devel

Windows
With windows you MAY need to have the latest OpenSSL version. Install OpenSSL 32-bit or 64-bit.
Mac
If you do not have the latest version of OpenSSL you will encounter some installation issues with creating new applications since we need to download a zip of the application via GitHub.
With Mac you can install OpenSSL through brew.
brew install openssl

Python 3.6 does not come preinstalled with certificates so you may need to install certificates with this command:
/Applications/Python\ 3.6/Install\ Certificates.command

You should now be good to install new Masonite application of Mac :)
Python 3.7 and Windows
If you are using Python 3.7, add it to your PATH Environment variable.
Open Windows PowerShell and run: pip install masonite-cli
Add C:\Users\%USERNAME%\.AppData\Programs\Python\Python37\Scripts\ to PATH Environment variable.
Note: PATH variables depend on your installation folder
Quick Install:
Here is the quick and dirty of what you need to run. More step by step instructions are found below.
    $ python3 -m venv venv
    $ source venv/bin/activate
    $ pip install masonite
    $ craft new
    $ craft serve

Go to http://localhost:8000/


* * * *

Not all computers are made the same so you may have some trouble installing Masonite depending on your machine. If you have any issues be sure to read the Known Installation Issues Documentation.

* * * *


Contributing
Please read the Contributing Documentation here. Development will be on the current releasing branch of the Core Repository (typically the develop branch) so check open issues, the current Milestone and the releases in that repository. Ask any questions you like in the issues so we can have an open discussion about the framework, design decisions and future of the project.
Contributors


Joseph Mancuso💻 🐛 💬 🤔
Vaibhav Mule💻 🐛 💬 🤔
Martín Peveri💻 🐛 💬 🤔
Tony Hammack💻 🐛 💬 🤔
Abram C. Isola💻 🐛 💬 🤔
Mitch Dennett💻 🐛 💬 🤔
Marlysson Silva💻 🐛 💬 🤔


Christopher Byrd💻 🐛 💬 🤔
Björn Theart💻 🐛 💬 🤔
Junior Gantin💻 🐛 💬 🤔


Thank you for those who have contributed to Masonite!
License
The Masonite framework is open-sourced software licensed under the MIT license.
Hello World
Getting started is very easy. Below is how you can get a simple Hello World application up and running.
Installation

Be sure to join the Slack Channel for help or guidance.

Masonite excels at being simple to install and get going. If you are coming from previous versions of Masonite, the order of some of the installation steps have changed a bit.
Firstly, open a terminal and head to a directory you want to create your application in. You might want to create it in a programming directory for example:
$ cd ~/programming
$ mkdir myapp
$ cd myapp

If you are on windows you can just create a directory and open the directory in the Powershell.
Activating Our Virtual Environment (optional)
Although this step is technically optional, it is highly recommended. You can create a virtual environment if you don't want to install all of masonite's dependencies on your systems Python. If you use virtual environments then create your virtual environment by running:
$ python -m venv venv
$ source venv/bin/activate

or if you are on Windows:
$ python -m venv venv
$ ./venv/Scripts/activate


The pythoncommand here is utilizing Python 3. Your machine may run Python 2 (typically 2.7) by default for UNIX machines. You may set an alias on your machine for Python 3 or simply run python3anytime you see the pythoncommand.


For example, you would run python3 -m venv venv instead of python -m venv venv

Installing Masonite
Now we can install Masonite. This will give us access to a craft command we can use to finish the install steps for us:
$ pip install masonite

Once Masonite installs you will now have access to the craft command line tool. Craft will become your best friend during your development. You will learn to love it very quickly :).
You can ensure Masonite and craft installed correctly by running:
$ craft

You should see a list of a few commands like install and new
Creating Our Project
Great! We are now ready to create our first project. We should have the new craft command. We can check this by running:
$ craft

We are currently only interested in the craft new command. To create a new project just run:
$ craft new

This command will also run craft install which will install our dependencies.
This will get the latest Masonite project template and unzip it for you. We just need to go into our new project directory and install the dependencies in our requirements.txt file.
Additional Commands
Now that Masonite installed fully we can check all the new commands we have available. There are many :).
$ craft

We should see many more commands now.
Running The Server
After it’s done we can just run the server by using another craft command:
$ craft serve

Congratulations! You’ve setup your first Masonite project! Keep going to learn more about how to use Masonite to build your applications.
{% hint style=""success"" %}
You can learn more about craft by reading The Craft Command documentation or continue on to learning about how to create web application by first reading the Routing documentation
{% endhint %}
{% hint style=""info"" %}
Masonite uses romantic versioning instead of semantic versioning. Because of this, all minor releases (2.0.x) will contain bug fixes and fully backwards compatible feature releases. Be sure to always keep your application up to date with the latest minor release to get the full benefit of Masonite's romantic versioning.
{% endhint %}
Hello World
All web routes are in routes/web.py. In this file is already the route to the welcome controller. To start your hello world example just add something like:
Get('/hello/world', 'HelloWorldController@show'),
our routes constant file should now look something like:
ROUTES = [
    Get('/', 'WelcomeController@show'),
    Get('/hello/world', 'HelloWorldController@show'),
]


* * * *

NOTE: Notice this new interesting string syntax in our route. This will grant our route access to a controller (which we will create below)

* * * *


Since we used a string controller we don't have to import our controller into this file. All imports are done through Masonite on the backend.
You'll notice that we have a reference to the HelloWorldController class which we do not have yet. This framework uses controllers in order to separate the application logic. Controllers can be looked at as the views.py in a Django application. The architectural standard here is 1 controller per file.
In order to make the HelloWorldController we can use a craft command:
$ craft controller HelloWorld

This will scaffold the controller for you and put it in app/http/controllers/HelloWorldController.py. This new file will have all the imports for us.
Inside the HelloWorldController we can make our show method like this:
def show(self, view: View):
    """""" Show Hello World Template """"""
    return view.render('helloworld')
As you see above, we are returning a helloworld template but we do not have that yet. All templates are in resources/templates. We can simply make a file called helloworld.html or run the craft command:
$ craft view helloworld

Which will create the resources/templates/helloworld.html template for us.
Lastly all templates run through the Jinja2 rendering engine so we can use any Jinja2 code inside our template like:
inside the resources/views/helloworld.html
{{ 'Hello World' }}

Now just run the server:
$ craft serve

And navigate to localhost:8000/hello/world and you will see Hello World in your browser.
Happy Crafting!
",batch2,8:10:17,Done
119,smuzaffar/cmssw-test,,batch1,16:57:58,Done
120,tyndy125/tfs-oficial,"forgottenserver   
The Forgotten Server is a free and open-source MMORPG server emulator written in C++. It is a fork of the OpenTibia Server project. To connect to the server, you can use OTClient or OpenTibiaUnity.
Getting Started

Compiling, alternatively download AppVeyor builds for Windows
Scripting Reference

Support
If you need help, please visit the support forum on OTLand. Our issue tracker is not a support forum, and using it as one will result in your issue being closed. If you were unable to get assistance in the support forum, you should consider becoming a premium user on OTLand which grants you access to the premium support forum and supports OTLand financially.
Issues
We use the issue tracker on GitHub. Keep in mind that everyone who is watching the repository gets notified by e-mail when there is activity, so be thoughtful and avoid writing comments that aren't meaningful for an issue (e.g. ""+1""). If you'd like for an issue to be fixed faster, you should either fix it yourself and submit a pull request, or place a bounty on the issue.
",batch2,8:10:17,Done
121,smtcrms/linuxbrew-core,"Linuxbrew Core
Core formulae for the Homebrew package manager.
How do I install these formulae?
Just brew install <formula>. This is the default tap for Homebrew and is installed by default.
More Documentation, Troubleshooting, Contributing, Security, Community, Donations, License and Sponsors
See these sections in Homebrew/brew's README.
",batch1,16:57:58,Done
122,pflanze/util-vserver,,batch2,8:10:18,Done
123,0x6a77/openocd,"openocd
The place where I maintain Kinetis and Mac OSX fixes (until someone can get them into origin/master).
The openocd community are not much fun.  Thanks to the power of git, rather than deal with them, I can maintain Kinetis fixes here.  It's not ideal.  I suppose if they were easier to deal with, then someone could have already gotten Kinetis fixes into origin/master.  Who knows.
I started with a branch called kinetis-merge where I got the somewhat randomly introduced Freescale commits.  I got that to build and then I merged that to master.
I have a lot going on these days, so I generally  only support the MCUs and platforms I need/use.  However, I will always try to be helpful if I can.  I may not be able to immediately respond, but I will try to make time to help.
To build this repo in Mac OSX you'll need to do the following:
./bootstrap
./configure --enable-cmsis-dap --disable-werror
make
I use CMSIS-DAP almost exclusively now (well, whenever I can) and it's much better.  If you're working with Freescale/NXP FRDM boards or a KL20-based CMSIS-DAP design you made yourself then you'll need configs for that.
I also still sometimes use an FTDI-based pod.  It's based on the c232hm-edhsl.
I have configs that make this all work, but I hesitate to put them into this repo.  There is another repo that holds these configs.  The fewer changes I make to openocd repo, the better.
",batch2,8:09:15,Done
124,compnerd/llvm-project-v4,"Disclaimer
The llvm-monorepo-root,
llvm-project-v4,
llvm-project-v4-split,
llvm-project-v3,
llvm-project-v3-split,
llvm-project-v2, and
llvm-project-v2-split
repositories are
WIP repositories that are used for development and prototyping of an llvm-project monorepo
that will be used for Apple's compiler releases and for the open source Swift project.
Please see this forum post
for more details.
Original readme follows:
The LLVM Compiler Infrastructure
This directory and its subdirectories contain source code for LLVM,
a toolkit for the construction of highly optimized compilers,
optimizers, and runtime environments.
",batch1,16:57:58,Done
125,zraul123/keytekia,"Manateki
Web based implementation of the Manacrest trading card game
FAQ
What is it?
This is the repository for the code internally known as Manateki which is running on ManacrestOnline.com allowing people to play Manacrest online using only their browser
Does't this look a lot like Keyteki/Jinteki/Throneteki? The Keyfroge/Android netrunner/AGOT online experience?
Glad you noticed!  Yes, Keyteki/jinteki was a huge inspiration for this project, as the interface is clean and user friendly, so I've tried to make this similar in a lot of ways
Manateki is a fork of the Keyteki sourcecode
Can I contribute?
Sure!  The code is written in node.js(server) and react.js(client).  Feel free to make suggestions, implement new cards, refactor bits of the code that are a bit clunky(there's a few of those atm), raise pull requests or submit bug reports
If you are going to contribute code, try and follow the style of the existing code as much as possible and talk to me before engaging in any big refactors.  Also bear in mind there is an .eslintrc file in the project so try to follow those rules.
Documentation for implementing cards
There is also a list of events raised by the code here. If you're writing abilities which listen for these events, it tells you what parameters the event has and whether it has a handler.  If you're writing code which calls any of these events, please make sure you pass the same parameters.
The biggest help at the moment would be in terms of CSS, as that's a bit of a weakness of mine, feel free to pick up any of the issues tagged 'CSS' in the issue list.
If you're not coding inclined, then just playing games on the site, and reporting bugs and issues that you find is a big help
X Y Z doesn't work
That's not a question, but that still sucks, sorry :(  First, bear in mind the site is in its infancy so a lot of things aren't implemented yet, but you should be able to do most things with a bit of manual input.  If there's anything you can't do that you need to be able to do, let me know by raising an issue.
See this document for features I have planned and a link to the currently implemented cards:
How do I do X Y Z?
Check out the About page of Manateki live deployment.
Development
Docker
If you have docker installed, you can use the containerised version of the site.
Clone the repository, then run the following commands:
git submodule init
git submodule update
npm install
docker-compose up

In another terminal, run the following command:
docker-compose exec lobby node server/scripts/fetchdata

Non Docker
Required Software

Git
Node.js 8
MongoDB
ZeroMQ Libraries

Clone the repository, then run the following commands:
git submodule init
git submodule update
npm install # See https://github.com/JustinTulloss/zeromq.node/issues/283 for zmq errors on OS X
mkdir server/logs

Create config/local.json5 and put the following in it:
{
    dbPath: 'mongodb://localhost:27017/keyforge',
    mqHost: 'localhost',

    lobby: {
        port: 4000
    },

    gameNode: {
        hostname: 'localhost'
    }
}

Run the following commands:
node server/scripts/fetchdata.js
node .
node server/gamenode

There are two exectuable components and you'll need to configure/run both to run a local server.  First is the lobby server and then there are game nodes. The default configurations assume you are running mongo locally on the default port. If you need to change any configurations, edit config/default.json5 or create a config/local.json5 configuration that overrides any desired settings.
To download all supported languages (not needed if you're running just a test / dev server):
node server/scripts/fetchdata.js --language=en
node server/scripts/fetchdata.js --language=es
node server/scripts/fetchdata.js --language=de
node server/scripts/fetchdata.js --language=fr
node server/scripts/fetchdata.js --language=it
node server/scripts/fetchdata.js --language=pl
node server/scripts/fetchdata.js --language=pt
node server/scripts/fetchdata.js --language=zh-hans
node server/scripts/fetchdata.js --language=zh-hant

For production:
npm run build-vendor
npm run build
NODE_ENV=production PORT=4000 node .

Then for each game node (typically one per CPU/core):
PORT={port} SERVER={node-name} node server/gamenode

Running and Testing
The game server should be accessible by browsing to localhost:4000.
You can register 2 or more users, to play against yourself.
They can have fake email addresses.
You can login as both users either from 2 different browsers, or by
using an incognito window.
These users will be normal (non-admin) users. To escalate a user to
the admin role requires manual edits to the Mongo database, but that
is not required for testing in-game functionality.
If you implement or make changes to a card, you can use manual mode
to add it to a deck from within a game. Use manual mode, and the command:
/add-card <card name>

Before you run the unit tests, be sure all the necessary dependencies are installed
npm install

Then, to run the tests:
npm test

Coding Guidelines
All JavaScript code included in Manateki should pass (no errors, no warnings)
linting by ESLint, according to the rules defined in
.eslintrc at the root of this repo. To manually check that that is indeed the
case install ESLint and run
npm run lint

from repository's root.
All tests should also pass.  To run these manually do:
npm test

If you are making any game engine changes, these will not be accepted without unit tests to cover them.
Discord Discusson
Manacrest Discord Server
",batch2,8:09:15,Done
126,Compro-Prasad/emacs,,batch1,16:59:00,Done
127,gem/oq-engine,"OpenQuake Engine

The OpenQuake Engine is an open source application that allows users to compute seismic hazard and seismic risk of earthquakes on a global scale. It runs on Linux, macOS and Windows, on laptops, workstations, standalone servers and multi-node clusters. DOI: 10.13117/openquake.engine






Current stable
Current stable version is the OpenQuake Engine 3.11 'Wegener'. The documentation is available at https://github.com/gem/oq-engine/tree/engine-3.11#openquake-engine.

What's new

Documentation (master tree)
General overview

About
FAQ
Manuals
OQ Commands
Architecture
Calculation Workflow
Hardware Suggestions
Continuous integration and testing
Glossary of Terms

For contributors

Development Philosophy and Coding Guidelines
Source Code/API Documentation
HTTP REST API
Implementing a new GSIM

Installation

Technology stack and requirements
Which installation method should I use?

Linux

Installing on Ubuntu
Installing on RedHat and derivatives
Installing on other flavors (without sudo)
Installing from sources
Installing on a cluster

macOS

Installing on macOS
Installing from sources

Windows

Installing on Windows
Installing from sources
Starting the software

Cloud

Deploy a Docker container

Mirrors
A mirror of this repository, hosted in Pavia (Italy), is available at https://mirror.openquake.org/git/GEM/oq-engine.git.
The main download server (downloads.openquake.org) is hosted in Nürnberg (Germany).
Running the OpenQuake Engine

Using the command line
Using the WebUI

Visualizing outputs via QGIS


Installation
Driving the Engine
Visualizing outputs
Repository
Source code

License
The OpenQuake Engine is released under the GNU Affero Public License 3.
Contacts

Support forum: https://groups.google.com/forum/#!forum/openquake-users
Twitter: @gem_devs
Email: info@openquake.org
IRC: irc.freenode.net, channel #openquake

Thanks
The OpenQuake Engine is developed by the Global Earthquake Model Foundation (GEM) with the support of






































If you would like to help support development of OpenQuake, please contact us at partnership@globalquakemodel.org.
For more info visit the GEM website at https://www.globalquakemodel.org/partners
",batch1,16:58:59,Done
128,cdl-saarland/llvm-vp,,batch1,16:57:59,Done
129,sealday/emacs,,batch1,16:59:02,Done
130,codeforamerica/brigade,"The Code for America Brigade Website
Code for America Brigades are local volunteer groups that bring together community members to help make government work better. Brigades use technology to build new tools to help with local civic issues. Code for America supports Brigade chapters with resources, tools, and access to the wider civic technology movement.
This repo is for the Brigade website [https://brigade.codeforamerica.org/].
Installation
The Code for America Brigade site is built on Flask and Python with a little bit of Javascript. The brigade/views.py file describes the routes. The brigade/templates files have the HTML templates.
Set up a Python virtual environment.
python3 -m venv env
source env/bin/activate

Install the required libraries.
pip install -r requirements.txt

Install Node and frontend dependencies with npm
brew install node
npm install

During development, run webpack.
./node_modules/.bin/webpack --watch

Then run the server in debug mode:
FLASK_ENV=development FLASK_APP=brigade.wsgi flask run

The server will be available at http://localhost:5000/.
or run it using Honcho and the Procfile:
honcho start

You can also run unit tests like this:
python manage.py runtests

Goals
This website is meant to:

Explain what the Brigade program is
Help people find their local Brigade
Show off the fine works of the Brigades
Provide tools that help Brigade work
Make it easy to start a new Brigade

Project Search
The Project Search page is a new service we built to search across thousands of civic technology projects. Go try it out, we think its pretty useful.
Read more at README-Project-Search.md
History
The Brigade program started in 2012 as an experiment, largely copying the success of Chi Hack Night (known at the time as Open Gov Hack Night).
This website is on its third version. V1 Was a Rails site with many contributors. It served the Brigade well as it was growing. As Code for America became better at supporting the volunteer groups, we needed something different.
The CfAPI was built as reaction to how Brigades were operating themselves. We now meet them where they are, instead of trying to get them to log into our site.
V2 was powered by the CfAPI and worked great, yet was built quickly with PHP and Javascript. It was kind of a cobweb of dependent parts.
V3, the current site, is meant to simplify the code and make it easier for Brigade members to get involved in building the Brigade site.
Contacts

Tom Dooner (@tdooner)

Copyright
Copyright (c) 2015–2020 Code for America.
",batch2,8:10:17,Done
131,emacs-ng/emacs-ng,"



emacs-ng
A new approach to Emacs - Including TypeScript, Threading, Async I/O, and WebRender.


homepage •
  Deno/Javascript •
  webrender •
  ng-module •
  handbook •
  faq


Overview
emacs-ng is based off of the master branch of emacs, and regularly
merges in the latest changes. (this branch includes the native
compilation feature from Andrea Corallo).
The last merged commit is a2842a1172 (Fri May 7 2021).
Motivation
The goal of this fork is to explore new development approaches. To accomplish
this, we aim to maintain an inclusive and innovative environment. The project is
not about replacing elisp with a more popular language like Javascript. We just
want to make emacs more approachable for people who don't like lisp as much as
we do.
Contributions are welcome from anyone and we are always happy to invite new
people to the project. We are open towards interesting ideas to make emacs
better. Our only request is that you open an issue before starting work and be
willing to take feedback from the core contributors.
Why Emacs-ng
Emacs-ng is an additive native layer over emacs, bringing features like Deno's
Javascript and Async I/O environment, Mozilla's Webrender (experimental opt-in
feature), and other features in development. emacs-ng's approach is to utilize
multiple new development approaches and tools to bring Emacs to the next
level. emacs-ng is maintained by a team that loves Emacs and everything it
stands for - being totally introspectable, with a fully customizable and free
development environment. We want Emacs to be a editor 40+ years from now that
has the flexibility and design to keep up with progressive technology.
Why JavaScript
One of emacs-ng's primary features is integrating the Deno
Runtime, which allows execution of JavaScript and
Typescript within Emacs. The details of that feature are listed below, however
many users would ask themselves WHY JAVASCRIPT? JavaScript is an extremely
dynamic language that allows for a user to inspect and control their scripting
environment. The key to note is that bringing in Deno isn't JUST JavaScript -
it's an ecosystem of powerful tools and approaches that Emacs just doesn't have
currently.

TypeScript offers an extremely flexible typing system, that allows to user to
have compile time control of their scripting, with the flexibility of types
""getting out of the way"" when not needed.
Deno uses Google's v8 JavaScript engine, which features an extremely powerful
JIT and world-class garbage collector.
Usage of modern Async I/O utilizing Rust's Tokio library.
Emacs-ng has WebWorker support, meaning that multiple JavaScript engines can
be running in parallel within the editor. The only restriction is that only
the 'main' JS Engine can directly call lisp functions.
Emacs-ng also has WebAssembly support - compile your C module as WebAsm and
distribute it to the world. Don't worry about packaging shared libraries or
changing module interfaces, everything can be handled and customized by you
the user, at the scripting layer. No need to be dependent on native
implementation details.

Performance
v8's world-class JIT offers the potential for large performance gains. Async I/O
from Deno, WebWorkers, and WebAsm, gives you the tools to make Emacs a smoother
and faster experience without having to install additional tools to launch as
background processes or worry about shared library versions.
Contributing
Contributions are welcome. We try to maintain a list of ""new contributor""
friendly issues tagged with ""good first issue"".
",batch1,16:58:59,Done
132,vyakymenko/angular-seed-express,"No longer under active maintenance
For starting a new project consider Angular Express.
",batch2,8:10:17,Done
133,jrmuizel/libunwind,,batch2,8:10:17,Done
134,openstack/openstack,"OpenStack
OpenStack is a collection of interoperable components that can be deployed
to provide computing, networking and storage resources. Those infrastructure
resources can then be accessed by end users through programmable APIs.
This repository just represents OpenStack as a collection of git submodules.
You can find the repositories for individual components at:
https://opendev.org/openstack
You can learn more about the various components in OpenStack at:
https://openstack.org/software
To learn more about how to contribute to OpenStack, please head to our
Contributor portal: https://www.openstack.org/community/
To learn more about how OpenStack is governed, you can visit:
https://governance.openstack.org/

Why this repository ?
Our continuous integration system, Zuul, gates all of the contained projects
in an effective single timeline. This means that OpenStack, across all of the
projects, does already have a sequence of combinations that have been
explicitly tested, but it's non-trivial to go from a single commit of a
particular project to the commits that were tested with it.
Gerrit's submodule tracking feature will update a super project every
time a subproject is updated, so the specific sequence created by zuul
will be captured by the super project commits.
This repo is intended to be used in a read-only manner. Any commit in this
repo will get a collection of commits in the other repos that have
explicitly been tested with each other, if that sort of thing is important
to you.
",batch1,16:57:59,Done
135,mbrukman/llvm,,batch1,16:57:58,Done
136,4minitz/4minitz,"master/ 
develop/ 




4Minitz!
Simply the best a decent free webapp for taking meeting minutes.

Create a meeting series and invite others
Specify moderators, invited and informed users
Create an agenda with multiple topics
Attend a meeting via web with reactive live updates
1-button sending of agenda, minutes and action items by email
Use labels to tag items for later retrieval
Upload binary attachments to minutes (e.g., presentations, photos)
Track open action items and unfinished topics across meetings
Full privacy: Host your own server - it's easy!


(Click to enlarge screen shot)


(Click to play Demo Video)
Documentation is ""continuously"" not finished... Nevertheless these WIP docs may be of help:

FAQ - Frequently Asked Questions
User Doc
Admin Guide
Developer Doc

External Project Links

4Minitz Backlog - organizes our project TODOs
4Minitz Demo Server - well, our Demo server ;-)
GitHub Actions - Runs unit and end2end tests on each commit
Code Climate - Keeps an eye on our code quality
CLA Assist - Manages signing of our Contributor License Agreements
Docker Hub - Spin up your own 4Minitz server in seconds

4Minitz is proudly sponsored by

Deployment Quick Start
To quickly set up a local demo of 4Minitz at your site for evaluation and
testing you can use our sample
Docker Compose configuration file and run it with
docker-compose up
You can now access 4Minitz by pointing your browser to http://localhost:3100.
If you don't want to use docker-compose you can use just docker by
starting the two containers manually:
docker run --rm --name mongo mongo
docker run --rm --name 4minitz \
    -v $PWD/4minitz_storage:/4minitz_storage \
    -p 3100:3333 \
    -e MONGO_URL=mongodb://mongo/4minitz \
    --link mongo \
    4minitz/4minitz:stable
Don't miss the Admin Guide
with a more comprehensive coverage of the real production
building & installation topic! Especially see
how to configure your 4Minitz docker server.
Development environment (Linux, Mac)
Attention: This is not a proper setup for production deployment!
It is intended for developers and so it has some security drawbacks (No
password protection for MongoDB) and also consumes some amount of extra RAM
(>700 MB)). Don't miss the Admin Guide
with a more comprehensive coverage of the real production installation topic!
Prerequisites
4minitz is realized with the Meteor JS Framework. So, first install the current version of meteor:
curl https://install.meteor.com/ | sh
meteor --version
On Windows? Download the meteor installer.
As an experienced Windows admin you sure can transfer the below steps for Linux and Mac to your OS.
Run development version (Linux & Mac)
Once Meteor is set up you can clone and run 4Minitz from source like this:
git clone https://github.com/4minitz/4minitz.git
cd 4minitz
cp settings_sample.json settings.json
./runapp.sh
Wait some time for meteor to finish downloading and building.
You can reach 4Minitz via the default port 3100 by opening
http://localhost:3100 in your browser
Hint: There is a settings_sample.json file that has quite a few configuration options
(like sending eMails etc.). Don't miss the Admin Guide with more details
on this topic.
",batch2,8:10:17,Done
137,seanohue/replicantery,"Replicantery
This game is part of MUD Coders' Guild Enter the MUD Jam 2020. The theme of the jam is Board Games, and this game is specifically inspired by social deception games in general.
The goal, if you are a human detective, is to uncover the replicant and avoid falsely accusing other humans.
The goal for replicants is to confuse and deceive the humans.
TODOs

 Roles: 1) Replicant 2) Detective (Player-only) 4) Citizen (NPC-only)
 Assign role on login.
 Create event hooks for point gain or loss.
 Create event hook for death & respawn.
 Detectives can accuse.
 False accusations = lose points
 Correct accusations mean you kill the replicant and gain a point.
 Replicants can zap humans.
 Zap non-detective = lose point. Zap a detective = gain point.
 Detectives can scan other characters to see if they are replicants.
 Make the helper function a pure function, no callback.
 Use new helper function for old role commands.
 Detective scanners have a cooldown/reveal effect on detective.
 Replicant eye zaps have a cooldown/reveal effect on replicant (including rep. targets)
 False accusations disable scan/accuse for 30s.
 Zapping will give away the replicants for 30s. (see glowing eyes, hear zap sound in adjacent rooms)
 One character per account.
 ^ If the account has a character, just log them into that directly.
 Global Leaderboard, per-role Leaderboard - on GameState.
[] ^ Show number of current players on leaderboard
[] ^ Optimize leaderboard eventually, no need to keep a map of every single account and score.
[] Create city map using Axolemma.
[] Implement NPC movement.

About Ranvier
This game is based on Ranvier Tiny:
RanvierMUD Tiny is a barebones starter kit for people who ""know what they're doing"" and just want to have a telnet
connection, login, command parsing, and movement. There is almost nothing here which may be preferable for those that
know what they want and don't want to spend time tearing everything down before they can build it back up.
This setup includes account creation, multiplayer players per account, a very basic command parser that understands
room exits and exact command entry.  That is to say the player needs to type 'look' if they want to look, not 'l'.
This gives you freedom to do whatever kind of handling you want without any code to tear down.
There are exactly 2 commands included: look and quit and there is one starter area with 3 rooms to demonstrate that
movement actually works.
The basic command parser does not support skills or channels, if you're using this bundle you know what you're doing and
can decide how you want those to be parsed.  The input events for the login/character creation flow are nearly identical
to bundle-example-input-events but pared down to the bare minimum. There are no character classes. There is only one
default attribute: health.
Installation
git clone --recursive https://github.com/ranviermud/tiny
cd tiny
npm install
git submodule foreach npm install
Special Thanks
valtrexman
Ranviermud Slack
MUD Coders Guild
",batch2,8:10:17,Done
138,JohnSnowLabs/spark-nlp,"Spark NLP: State of the Art Natural Language Processing
    
Spark NLP is a Natural Language Processing library built on top of Apache Spark ML. It provides simple, performant & accurate NLP annotations for machine learning pipelines that scale easily in a distributed environment. Spark NLP comes with 1100+ pretrained pipelines and models in more than 192+ languages. It supports state-of-the-art transformers such as BERT, XLNet, ELMO, ALBERT, and Universal Sentence Encoder that can be used seamlessly in a cluster. It also offers Tokenization, Word Segmentation, Part-of-Speech Tagging, Named Entity Recognition, Dependency Parsing, Spell Checking, Multi-class Text Classification, Multi-class Sentiment Analysis, Machine Translation (+180 languages), Summarization and Question Answering (Google T5), and many more NLP tasks.
Project's website
Take a look at our official Spark NLP page: http://nlp.johnsnowlabs.com/ for user documentation and examples
Community support

Slack For live discussion with the Spark NLP community and the team
GitHub Bug reports, feature requests, and contributions
Discussions Engage with other community members, share ideas, and show off how you use Spark NLP!
Medium Spark NLP articles
YouTube Spark NLP video tutorials

Table of contents

Features
Requirements
Quick Start
Apache Spark Support
Databricks Support
EMR Support
Using Spark NLP

Spark Packages
Scala

Maven
SBT


Python

Pip/Conda


Compiled JARs
Apache Zeppelin
Jupyter Notebook
Google Colab Notebook
Kaggle Kernel
Databricks Cluser
EMR Cluser
S3 Cluster


Pipelines & Models

Pipelines
Models


Offline
Examples
FAQ
Troubleshooting
Citation
Contributing

Features

Tokenization
Trainable Word Segmentation
Stop Words Removal
Token Normalizer
Document Normalizer
Stemmer
Lemmatizer
NGrams
Regex Matching
Text Matching
Chunking
Date Matcher
Sentence Detector
Deep Sentence Detector (Deep learning)
Dependency parsing (Labeled/unlabeled)
Part-of-speech tagging
Sentiment Detection (ML models)
Spell Checker (ML and DL models)
Word Embeddings (GloVe and Word2Vec)
BERT Embeddings (TF Hub models)
DistilBERT Embeddings (HuggingFace models)
RoBERTa Embeddings (HuggingFace models)
XLM-RoBERTa Embeddings (HuggingFace models)
ALBERT Embeddings (TF Hub models)
XLNet Embeddings
ELMO Embeddings (TF Hub models)
Universal Sentence Encoder (TF Hub models)
BERT Sentence Embeddings (42 TF Hub models)
Sentence Embeddings
Chunk Embeddings
Unsupervised keywords extraction
Language Detection & Identification (up to 375 languages)
Multi-class Sentiment analysis (Deep learning)
Multi-label Sentiment analysis (Deep learning)
Multi-class Text Classification (Deep learning)
Neural Machine Translation (MarianMT)
Text-To-Text Transfer Transformer (Google T5)
Named entity recognition (Deep learning)
Easy TensorFlow integration
GPU Support
Full integration with Spark ML functions
+2000 pre-trained models in +200 languages!
+1700 pre-trained pipelines in +200 languages!
Multi-lingual NER models: Arabic, Bengali, Chinese, Danish, Dutch, English, Finnish, French, German, Hebrew, Italian, Japanese, Korean, Norwegian, Persian, Polish, Portuguese, Russian, Spanish, Swedish, and Urdu.

Requirements
To use Spark NLP you need the following requirements:

Java 8
Apache Spark 3.1.x (or 3.0.x, or 2.4.x, or 2.3.x)

GPU (optional):
Spark NLP 3.1.0 is built with TensorFlow 2.4.1 and requires the followings if you need GPU support

CUDA11 and cuDNN 8.0.2

Quick Start
This is a quick example of how to use Spark NLP pre-trained pipeline in Python and PySpark:
$ java -version
# should be Java 8 (Oracle or OpenJDK)
$ conda create -n sparknlp python=3.7 -y
$ conda activate sparknlp
# spark-nlp by default is based on pyspark 3.x
$ pip install spark-nlp==3.1.0 pyspark==3.1.1
In Python console or Jupyter Python3 kernel:
# Import Spark NLP
from sparknlp.base import *
from sparknlp.annotator import *
from sparknlp.pretrained import PretrainedPipeline
import sparknlp

# Start SparkSession with Spark NLP
# start() functions has 4 parameters: gpu, spark23, spark24, and memory
# sparknlp.start(gpu=True) will start the session with GPU support
# sparknlp.start(spark23=True) is when you have Apache Spark 2.3.x installed
# sparknlp.start(spark24=True) is when you have Apache Spark 2.4.x installed
# sparknlp.start(memory=""16G"") to change the default driver memory in SparkSession
spark = sparknlp.start()

# Download a pre-trained pipeline
pipeline = PretrainedPipeline('explain_document_dl', lang='en')

# Your testing dataset
text = """"""
The Mona Lisa is a 16th century oil painting created by Leonardo.
It's held at the Louvre in Paris.
""""""

# Annotate your testing dataset
result = pipeline.annotate(text)

# What's in the pipeline
list(result.keys())
Output: ['entities', 'stem', 'checked', 'lemma', 'document',
'pos', 'token', 'ner', 'embeddings', 'sentence']

# Check the results
result['entities']
Output: ['Mona Lisa', 'Leonardo', 'Louvre', 'Paris']
For more examples, you can visit our dedicated repository to showcase all Spark NLP use cases!
Apache Spark Support
Spark NLP 3.1.0 has been built on top of Apache Spark 3.x while fully supports Apache Spark 2.3.x and Apache Spark 2.4.x:



Spark NLP
Apache Spark 2.3.x
Apache Spark 2.4.x
Apache Spark 3.0.x
Apache Spark 3.1.x




3.1.x
YES
YES
YES
YES


3.0.x
YES
YES
YES
YES


2.7.x
YES
YES
NO
NO


2.6.x
YES
YES
NO
NO


2.5.x
YES
YES
NO
NO


2.4.x
Partially
YES
NO
NO


1.8.x
Partially
YES
NO
NO


1.7.x
YES
NO
NO
NO


1.6.x
YES
NO
NO
NO


1.5.x
YES
NO
NO
NO



NOTE: Starting 3.0.0 release, the default spark-nlp and spark-nlp-gpu pacakges are based on Scala 2.12 and Apache Spark 3.x by default.
NOTE: Starting the 3.0.0 release, we support all major releases of Apache Spark 2.3.x, Apache Spark 2.4.x, Apache Spark 3.0.x, and Apache Spark 3.1.x
Find out more about Spark NLP versions from our release notes.
Databricks Support
Spark NLP 3.1.0 has been tested and is compatible with the following runtimes:
CPU:

5.5 LTS
5.5 LTS ML
6.4
6.4 ML
7.3
7.3 ML
7.4
7.4 ML
7.5
7.5 ML
7.6
7.6 ML
8.0
8.0 ML
8.1
8.1 ML
8.2
8.2 ML
8.3
8.3 ML

GPU:

8.1 ML & GPU
8.2 ML & GPU
8.3 ML & GPU

NOTE: Spark NLP 3.1.x is based on TensorFlow 2.4.x which is compatible with CUDA11 and cuDNN 8.0.2. The only Databricks runtimes supporting CUDA 11. are 8.1 ML with GPU, 8.2 ML with GPU, and 8.3 ML with GPU.
EMR Support
Spark NLP 3.1.0 has been tested and is compatible with the following EMR releases:

emr-5.20.0
emr-5.21.0
emr-5.21.1
emr-5.22.0
emr-5.23.0
emr-5.24.0
emr-5.24.1
emr-5.25.0
emr-5.26.0
emr-5.27.0
emr-5.28.0
emr-5.29.0
emr-5.30.0
emr-5.30.1
emr-5.31.0
emr-5.32.0
emr-5.33.0
emr-6.1.0
emr-6.2.0
emr-6.3.0

Full list of Amazon EMR 5.x releases
Full list of Amazon EMR 6.x releases
NOTE: The EMR 6.0.0 is not supported by Spark NLP 3.1.0
Usage
Spark Packages
Command line (requires internet connection)
Spark NLP supports all major releases of Apache Spark 2.3.x, Apache Spark 2.4.x, Apache Spark 3.0.x, and Apache Spark 3.1.x. That's being said, you need to choose the right package for the right Apache Spark major release:
Apache Spark 3.x (3.0.x and 3.1.x - Scala 2.12)
# CPU

spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.1.0

pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.1.0

spark-submit --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.1.0
The spark-nlp has been published to the Maven Repository.
# GPU

spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.1.0

pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.1.0

spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.1.0

The spark-nlp-gpu has been published to the Maven Repository.
Apache Spark 2.4.x (Scala 2.11)
# CPU

spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.1.0

pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.1.0

spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.1.0
The spark-nlp-spark24 has been published to the Maven Repository.
# GPU

spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.1.0

pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.1.0

spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.1.0

The spark-nlp-gpu-spark24 has been published to the Maven Repository.
Apache Spark 2.3.x (Scala 2.11)
# CPU

spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.1.0

pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.1.0

spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.1.0
The spark-nlp-spark23 has been published to the Maven Repository.
# GPU

spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark23_2.11:3.1.0

pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark23_2.11:3.1.0

spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark23_2.11:3.1.0

The spark-nlp-gpu-spark23 has been published to the Maven Repository.
NOTE: In case you are using large pretrained models like UniversalSentenceEncoder, you need to have the following set in your SparkSession:
spark-shell \
  --driver-memory 16g \
  --conf spark.kryoserializer.buffer.max=2000M \
  --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.1.0
Scala
Spark NLP supports Scala 2.11.x if you are using Apache Spark 2.3.x or 2.4.x and Scala 2.12.x if you are using Apache Spark 3.0.x or 3.1.x. Our packages are deployed to Maven central. To add any of our packages as a dependency in your application you can follow these coordinates:
Maven
spark-nlp on Apache Spark 3.x:
<!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp -->
<dependency>
    <groupId>com.johnsnowlabs.nlp</groupId>
    <artifactId>spark-nlp_2.12</artifactId>
    <version>3.1.0</version>
</dependency>
spark-nlp-gpu:
<!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu -->
<dependency>
    <groupId>com.johnsnowlabs.nlp</groupId>
    <artifactId>spark-nlp-gpu_2.12</artifactId>
    <version>3.1.0</version>
</dependency>
spark-nlp on Apache Spark 2.4.x:
<!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark24 -->
<dependency>
    <groupId>com.johnsnowlabs.nlp</groupId>
    <artifactId>spark-nlp-spark24_2.11</artifactId>
    <version>3.1.0</version>
</dependency>
spark-nlp-gpu:
<!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark24 -->
<dependency>
    <groupId>com.johnsnowlabs.nlp</groupId>
    <artifactId>spark-nlp-gpu_2.11</artifactId>
    <version>3.1.0/version>
</dependency>
spark-nlp on Apache Spark 2.3.x:
<!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark23 -->
<dependency>
    <groupId>com.johnsnowlabs.nlp</groupId>
    <artifactId>spark-nlp-spark23_2.11</artifactId>
    <version>3.1.0</version>
</dependency>
spark-nlp-gpu:
<!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark23 -->
<dependency>
    <groupId>com.johnsnowlabs.nlp</groupId>
    <artifactId>spark-nlp-gpu-spark23_2.11</artifactId>
    <version>3.1.0</version>
</dependency>
SBT
spark-nlp on Apache Spark 3.x.x:
// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp
libraryDependencies += ""com.johnsnowlabs.nlp"" %% ""spark-nlp"" % ""3.1.0""

spark-nlp-gpu:
// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu
libraryDependencies += ""com.johnsnowlabs.nlp"" %% ""spark-nlp-gpu"" % ""3.1.0""

spark-nlp on Apache Spark 2.4.x:
// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp
libraryDependencies += ""com.johnsnowlabs.nlp"" %% ""spark-nlp-spark24"" % ""3.1.0""

spark-nlp-gpu:
// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu
libraryDependencies += ""com.johnsnowlabs.nlp"" %% ""spark-nlp-gpu-spark24"" % ""3.1.0""

spark-nlp on Apache Spark 2.3.x:
// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark23
libraryDependencies += ""com.johnsnowlabs.nlp"" %% ""spark-nlp-spark23"" % ""3.1.0""

spark-nlp-gpu:
// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark23
libraryDependencies += ""com.johnsnowlabs.nlp"" %% ""spark-nlp-gpu-spark23"" % ""3.1.0""

Maven Central: https://mvnrepository.com/artifact/com.johnsnowlabs.nlp
If you are interested, there is a simple SBT project for Spark NLP to guide you on how to use it in your projects Spark NLP SBT Starter
Python
Spark NLP supports Python 3.6.x and 3.7.x if you are using PySpark 2.3.x or 2.4.x and Python 3.8.x if you are using PySpark 3.x.
Python without explicit Pyspark installation
Pip/Conda
If you installed pyspark through pip/conda, you can install spark-nlp through the same channel.
Pip:
pip install spark-nlp==3.1.0
Conda:
conda install -c johnsnowlabs spark-nlp
PyPI spark-nlp package / Anaconda spark-nlp package
Then you'll have to create a SparkSession either from Spark NLP:
import sparknlp

spark = sparknlp.start()
or manually:
spark = SparkSession.builder \
    .appName(""Spark NLP"")\
    .master(""local[4]"")\
    .config(""spark.driver.memory"",""16G"")\
    .config(""spark.driver.maxResultSize"", ""0"") \    
    .config(""spark.kryoserializer.buffer.max"", ""2000M"")\
    .config(""spark.jars.packages"", ""com.johnsnowlabs.nlp:spark-nlp_2.12:3.1.0"")\
    .getOrCreate()
If using local jars, you can use spark.jars instead for comma-delimited jar files. For cluster setups, of course, you'll have to put the jars in a reachable location for all driver and executor nodes.
Quick example:
import sparknlp
from sparknlp.pretrained import PretrainedPipeline

#create or get Spark Session

spark = sparknlp.start()

sparknlp.version()
spark.version

#download, load and annotate a text by pre-trained pipeline

pipeline = PretrainedPipeline('recognize_entities_dl', 'en')
result = pipeline.annotate('The Mona Lisa is a 16th century oil painting created by Leonardo')
Compiled JARs
Build from source
spark-nlp

FAT-JAR for CPU on Apache Spark 3.x.x

sbt assembly

FAT-JAR for GPU on Apache Spark 3.x.x

sbt -Dis_gpu=true assembly

FAT-JAR for CPU on Apache Spark 2.4.x

sbt -Dis_spark24=true assembly

FAT-JAR for GPU on Apache Spark 2.4.x

sbt -Dis_gpu=true -Dis_spark24=true assembly

FAT-JAR for CPU on Apache Spark 2.3.x

sbt -Dis_spark23=true assembly

FAT-JAR for GPU on Apache Spark 2.3.x

sbt -Dis_gpu=true -Dis_spark23=true assembly
Using the jar manually
If for some reason you need to use the JAR, you can either download the Fat JARs provided here or download it from Maven Central.
To add JARs to spark programs use the --jars option:
spark-shell --jars spark-nlp.jar
The preferred way to use the library when running spark programs is using the --packages option as specified in the spark-packages section.
Apache Zeppelin
Use either one of the following options

Add the following Maven Coordinates to the interpreter's library list

com.johnsnowlabs.nlp:spark-nlp_2.12:3.1.0

Add a path to pre-built jar from here in the interpreter's library list making sure the jar is available to driver path

Python in Zeppelin
Apart from the previous step, install the python module through pip
pip install spark-nlp==3.1.0
Or you can install spark-nlp from inside Zeppelin by using Conda:
python.conda install -c johnsnowlabs spark-nlp
Configure Zeppelin properly, use cells with %spark.pyspark or any interpreter name you chose.
Finally, in Zeppelin interpreter settings, make sure you set properly zeppelin.python to the python you want to use and install the pip library with (e.g. python3).
An alternative option would be to set SPARK_SUBMIT_OPTIONS (zeppelin-env.sh) and make sure --packages is there as shown earlier since it includes both scala and python side installation.
Q: What if I am still on Zeppelin 0.8.x that only supports Apache Spark 2.4.x?
A: You can simply use the spark-nlp-spark24:3.1.0 package or Fat JAR instead.
Jupyter Notebook (Python)
Recomended:
The easiest way to get this done on Linux and macOS is to simply install spark-nlp and pyspark PyPI packages and launch the Jupyter from the same Python environment:
$ conda create -n sparknlp python=3.7 -y
$ conda activate sparknlp
# spark-nlp by default is based on pyspark 3.x
$ pip install spark-nlp==3.1.0 pyspark==3.1.1 jupyter
$ jupyter notebook
The you can use python3 kernel to run your code with creating SparkSession via spark = sparknlp.start().
Optional:
If you are in different operating systems and require to make Jupyter Notebook run by using pyspark, you can follow these steps:
export SPARK_HOME=/path/to/your/spark/folder
export PYSPARK_PYTHON=python3
export PYSPARK_DRIVER_PYTHON=jupyter
export PYSPARK_DRIVER_PYTHON_OPTS=notebook

pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.1.0
Alternatively, you can mix in using --jars option for pyspark + pip install spark-nlp
If not using pyspark at all, you'll have to run the instructions pointed here
Google Colab Notebook
Google Colab is perhaps the easiest way to get started with spark-nlp. It requires no installation or setup other than having a Google account.
Run the following code in Google Colab notebook and start using spark-nlp right away.
# This is only to setup PySpark and Spark NLP on Colab
!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash
This script comes with the two options to define pyspark and spark-nlp versions via options:
# -p is for pyspark
# -s is for spark-nlp
# by default they are set to the latest
!bash colab.sh -p 3.1.1 -s 3.1.0
Spark NLP quick start on Google Colab is a live demo on Google Colab that performs named entity recognitions and sentiment analysis by using Spark NLP pretrained pipelines.
Kaggle Kernel
Run the following code in Kaggle Kernel and start using spark-nlp right away.
# Let's setup Kaggle for Spark NLP and PySpark
!wget http://setup.johnsnowlabs.com/kaggle.sh -O - | bash
Spark NLP quick start on Kaggle Kernel is a live demo on Kaggle Kernel that performs named entity recognitions by using Spark NLP pretrained pipeline.
Databricks Cluster


Create a cluster if you don't have one already


On a new cluster or existing one you need to add the following to the Advanced Options -> Spark tab:
spark.kryoserializer.buffer.max 2000M
spark.serializer org.apache.spark.serializer.KryoSerializer


In Libraries tab inside your cluster you need to follow these steps:
3.1. Install New -> PyPI -> spark-nlp -> Install
3.2. Install New -> Maven -> Coordinates -> com.johnsnowlabs.nlp:spark-nlp_2.12:3.1.0 -> Install


Now you can attach your notebook to the cluster and use Spark NLP!


NOTE: If you are launching a Databricks runtime that is not based on Apache Spark 3.x please choose a compatible Spark NLP package
EMR Cluster
To lanuch EMR cluster with Apache Spark/PySpark and Spark NLP correctly you need to have bootstrap and software configuration.
A sample of your bootstrap script
#!/bin/bash
set -x -e

echo -e 'export PYSPARK_PYTHON=/usr/bin/python3 
export HADOOP_CONF_DIR=/etc/hadoop/conf 
export SPARK_JARS_DIR=/usr/lib/spark/jars 
export SPARK_HOME=/usr/lib/spark' >> $HOME/.bashrc && source $HOME/.bashrc

sudo python3 -m pip install awscli boto spark-nlp

set +x
exit 0

A sample of your software configuration in JSON on S3 (must be public access):
[{
  ""Classification"": ""spark-env"",
  ""Configurations"": [{
    ""Classification"": ""export"",
    ""Properties"": {
      ""PYSPARK_PYTHON"": ""/usr/bin/python3""
    }
  }]
},
{
  ""Classification"": ""spark-defaults"",
    ""Properties"": {
      ""spark.yarn.stagingDir"": ""hdfs:///tmp"",
      ""spark.yarn.preserve.staging.files"": ""true"",
      ""spark.kryoserializer.buffer.max"": ""2000M"",
      ""spark.serializer"": ""org.apache.spark.serializer.KryoSerializer"",
      ""spark.driver.maxResultSize"": ""0"",
      ""spark.jars.packages"": ""com.johnsnowlabs.nlp:spark-nlp_2.12:3.1.0""
    }
}
]

A sample of AWS CLI to launch EMR cluster:

```.sh
aws emr create-cluster \
--name ""Spark NLP 3.1.0"" \
--release-label emr-6.2.0 \
--applications Name=Hadoop Name=Spark Name=Hive \
--instance-type m4.4xlarge \
--instance-count 3 \
--use-default-roles \
--log-uri ""s3://<S3_BUCKET>/"" \
--bootstrap-actions Path=s3://<S3_BUCKET>/emr-bootstrap.sh,Name=custome \
--configurations ""https://<public_access>/sparknlp-config.json"" \
--ec2-attributes KeyName=<your_ssh_key>,EmrManagedMasterSecurityGroup=<security_group_with_ssh>,EmrManagedSlaveSecurityGroup=<security_group_with_ssh> \
--profile <aws_profile_credentials>
S3 Cluster
With no Hadoop configuration
If your distributed storage is S3 and you don't have a standard Hadoop configuration (i.e. fs.defaultFS)
You need to specify where in the cluster distributed storage you want to store Spark NLP's tmp files.
First, decide where you want to put your application.conf file
import com.johnsnowlabs.util.ConfigLoader
ConfigLoader.setConfigPath(""/somewhere/to/put/application.conf"")
And then we need to put in such application.conf the following content
sparknlp {
  settings {
    cluster_tmp_dir = ""somewhere in s3n:// path to some folder""
  }
}
Pipelines and Models
Pipelines
Spark NLP offers more than 450+ pre-trained pipelines in 192 languages.
| dependency_parse                    | 2.4.0 |   en    |
Quick example:
import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline
import com.johnsnowlabs.nlp.SparkNLP

SparkNLP.version()

val testData = spark.createDataFrame(Seq(
(1, ""Google has announced the release of a beta version of the popular TensorFlow machine learning library""),
(2, ""Donald John Trump (born June 14, 1946) is the 45th and current president of the United States"")
)).toDF(""id"", ""text"")

val pipeline = PretrainedPipeline(""explain_document_dl"", lang=""en"")

val annotation = pipeline.transform(testData)

annotation.show()
/*
import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline
import com.johnsnowlabs.nlp.SparkNLP
2.5.0
testData: org.apache.spark.sql.DataFrame = [id: int, text: string]
pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_dl,en,public/models)
annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 10 more fields]
+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+
| id|                text|            document|               token|            sentence|             checked|               lemma|                stem|                 pos|          embeddings|                 ner|            entities|
+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+
|  1|Google has announ...|[[document, 0, 10...|[[token, 0, 5, Go...|[[document, 0, 10...|[[token, 0, 5, Go...|[[token, 0, 5, Go...|[[token, 0, 5, go...|[[pos, 0, 5, NNP,...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 5, Go...|
|  2|The Paris metro w...|[[document, 0, 11...|[[token, 0, 2, Th...|[[document, 0, 11...|[[token, 0, 2, Th...|[[token, 0, 2, Th...|[[token, 0, 2, th...|[[pos, 0, 2, DT, ...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 4, 8, Pa...|
+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+
*/

annotation.select(""entities.result"").show(false)

/*
+----------------------------------+
|result                            |
+----------------------------------+
|[Google, TensorFlow]              |
|[Donald John Trump, United States]|
+----------------------------------+
*/
Please check out our Models Hub for the full list of pre-trained pipelines with examples, demos, benchmarks, and more
Models
Spark NLP offers more than 710+ pre-trained models in 192 languages.
Some of the selected languages: Afrikaans, Arabic, Armenian, Basque, Bengali, Breton, Bulgarian, Catalan, Czech, Dutch, English, Esperanto, Finnish, French, Galician, German, Greek, Hausa, Hebrew, Hindi, Hungarian, Indonesian, Irish, Italian, Japanese, Latin, Latvian, Marathi, Norwegian, Persian, Polish, Portuguese, Romanian, Russian, Slovak, Slovenian, Somali, Southern Sotho, Spanish, Swahili, Swedish, Tswana, Turkish, Ukrainian, Zulu
Quick online example:
# load NER model trained by deep learning approach and GloVe word embeddings
ner_dl = NerDLModel.pretrained('ner_dl')
# load NER model trained by deep learning approach and BERT word embeddings
ner_bert = NerDLModel.pretrained('ner_dl_bert')
// load French POS tagger model trained by Universal Dependencies
val french_pos = PerceptronModel.pretrained(""pos_ud_gsd"", lang=""fr"")
// load Italain LemmatizerModel
val italian_lemma = LemmatizerModel.pretrained(""lemma_dxc"", lang=""it"")
Quick offline example:

Loading PerceptronModel annotator model inside Spark NLP Pipeline

val french_pos = PerceptronModel.load(""/tmp/pos_ud_gsd_fr_2.0.2_2.4_1556531457346/"")
      .setInputCols(""document"", ""token"")
      .setOutputCol(""pos"")
Please check out our Models Hub for the full list of pre-trained models with examples, demo, benchmark, and more
Offline
Spark NLP library and all the pre-trained models/pipelines can be used entirely offline with no access to the Internet. If you are behind a proxy or a firewall with no access to the Maven repository (to download packages) or/and no access to S3 (to automatically download models and pipelines), you can simply follow the instructions to have Spark NLP without any limitations offline:

Instead of using the Maven package, you need to load our Fat JAR
Instead of using PretrainedPipeline for pretrained pipelines or the .pretrained() function to download pretrained models, you will need to manually download your pipeline/model from Models Hub, extract it, and load it.

Example of SparkSession with Fat JAR to have Spark NLP offline:
spark = SparkSession.builder \
    .appName(""Spark NLP"")\
    .master(""local[*]"")\
    .config(""spark.driver.memory"",""16G"")\
    .config(""spark.driver.maxResultSize"", ""0"") \    
    .config(""spark.kryoserializer.buffer.max"", ""2000M"")\
    .config(""spark.jars"", ""/tmp/spark-nlp-assembly-3.1.0.jar"")\
    .getOrCreate()

You can download provided Fat JARs from each release notes, please pay attention to pick the one that suits your environment depending on the device (CPU/GPU) and Apache Spark version (2.3.x, 2.4.x, and 3.x)
If you are local, you can load the Fat JAR from your local FileSystem, however, if you are in a cluster setup you need to put the Fat JAR on a distributed FileSystem such as HDFS, DBFS, S3, etc. (i.e., hdfs:///tmp/spark-nlp-assembly-3.1.0.jar)

Example of using pretrained Models and Pipelines in offline:
# instead of using pretrained() for online:
# french_pos = PerceptronModel.pretrained(""pos_ud_gsd"", lang=""fr"")
# you download this model, extract it, and use .load
french_pos = PerceptronModel.load(""/tmp/pos_ud_gsd_fr_2.0.2_2.4_1556531457346/"")\
      .setInputCols(""document"", ""token"")\
      .setOutputCol(""pos"")

# example for pipelines
# instead of using PretrainedPipeline
# pipeline = PretrainedPipeline('explain_document_dl', lang='en')
# you download this pipeline, extract it, and use PipelineModel
PipelineModel.load(""/tmp/explain_document_dl_en_2.0.2_2.4_1556530585689/"")

Since you are downloading and loading models/pipelines manually, this means Spark NLP is not downloading the most recent and compatible models/pipelines for you. Choosing the right model/pipeline is on you
If you are local, you can load the model/pipeline from your local FileSystem, however, if you are in a cluster setup you need to put the model/pipeline on a distributed FileSystem such as HDFS, DBFS, S3, etc. (i.e., hdfs:///tmp/explain_document_dl_en_2.0.2_2.4_1556530585689/)

Examples
Need more examples? Check out our dedicated Spark NLP Showcase repository to showcase all Spark NLP use cases!
Also, don't forget to check Spark NLP in Action built by Streamlit.
All examples: spark-nlp-workshop
FAQ
Check our Articles and Videos page here
Citation
We have published a paper that you can cite for the Spark NLP library:
@article{KOCAMAN2021100058,
    title = {Spark NLP: Natural language understanding at scale},
    journal = {Software Impacts},
    pages = {100058},
    year = {2021},
    issn = {2665-9638},
    doi = {https://doi.org/10.1016/j.simpa.2021.100058},
    url = {https://www.sciencedirect.com/science/article/pii/S2665963821000063},
    author = {Veysel Kocaman and David Talby},
    keywords = {Spark, Natural language processing, Deep learning, Tensorflow, Cluster},
    abstract = {Spark NLP is a Natural Language Processing (NLP) library built on top of Apache Spark ML. It provides simple, performant & accurate NLP annotations for machine learning pipelines that can scale easily in a distributed environment. Spark NLP comes with 1100+ pretrained pipelines and models in more than 192+ languages. It supports nearly all the NLP tasks and modules that can be used seamlessly in a cluster. Downloaded more than 2.7 million times and experiencing 9x growth since January 2020, Spark NLP is used by 54% of healthcare organizations as the world’s most widely used NLP library in the enterprise.}
    }
}
Contributing
We appreciate any sort of contributions:

ideas
feedback
documentation
bug reports
NLP training and testing corpora
Development and testing

Clone the repo and submit your pull-requests! Or directly create issues in this repo.
Contact
nlp@johnsnowlabs.com
John Snow Labs
http://johnsnowlabs.com
",batch2,8:09:16,Done
139,CelestiaProject/CelestiaContent,,batch2,8:09:15,Done
140,tarunprabhu/DragonEgg,,batch1,16:58:59,Done
141,d12frosted/emacs-plus-basis,,batch1,16:58:59,Done
142,luispuerto/luispuerto.net,"Luis Puerto Source Code









This is the source code of my personal site luispuerto.net. It's built using the Jekyll and Netlify. I use the Minimal Mistakes template from Michael Rose as a base and I've added some changes from my own vintage. Please feel free to fork my work since I would really happy to see that it helps others to build also their sites.
NB: I've archived my work in this repo previous to October 27, 2018 here. You can know more about the reasons why I've done that in this blog post.
Table of Contents

Getting Started

Prerequisites


Built With

Noteworthy Jekyll Plugins - Gems 💎
Icons + Demo Images:
Other:


Issues
Contributing
Versioning
Authors
License
Acknowledgments

Getting Started
You can get a copy of this repo running the following in your terminal:
$ git clone https://github.com/luispuerto/luispuerto.net.git
Prerequisites
The only prerequisite to run this blog locally is to have Jekyll on your machine, and it's recommended to have also Bundler.
$ gem install jekyll bundler
Of course to be able to run Jekyll you need to have several other things on your system, like Ruby 💎. Please check the Jekyll docs for more info about prerequisites.
Built With
This site is build using the following software and services.

Jekyll - The static website generator I'm using to build my side.
Bundler - Bundler manages an application's dependencies through its entire life, across many machines, systematically and repeatably.
Minimal Mistakes - The Jekyll template I'm using.
Netlify - Where I'm building and hosting my site.
TravisCI - I'm performing some continuous integration tasks over here, like for example index the site for Algolia when I build it from master.

Noteworthy Jekyll Plugins - Gems 💎
Since I've not publishing the site using GitHub pages and I'm building this page by other means —Netlify— I have the freedom of using additional plugins and more granular control over what plugins I use on this site. These are the most noteworthy gems I'm using over the current Jekyll 4 dependencies, but you can check the complete list on my [Gemfile][].

jekyll-algolia - I'm using Algolia as a search engine on the site, so this gem comes in handy to index the site every time I push a new version to master.
jekyll-archives - I use this gem to create the archives of my site on a more compressive way.
jekyll-include-cache - To be able to cache part some includes that repeat themselves in most of the pages.
jemoji - The World without emojies is much more boring 🤷‍♂️.

Icons + Demo Images:
Some of the icons and images come from the following sites:

The Noun Project - Icons for everything.
Font Awesome - Vector icons and social logos.
Unsplash - Free images.

Other:
I use the additional services, snippets and JavaScript in this site.

Algolia - I use their service on this site for searching and indexing the site.
Jekyl Codex - I've used some snippets of code from this site for some solutions. Like for example open in a new window when you click in a link to a outside page.
jQuery - jQuery is a JavaScript library designed to simplify HTML DOM tree traversal and manipulation, as well as event handling, CSS animation, and Ajax.
Susy - Susy is a lightweight grid-layout engine for Sass.
Breakpoint - Breakpoint makes writing media queries in Sass simpler.
Magnific Popup - Responsive lightbox for images.
FitVids.JS - Plugin for fluid width video embeds.
GreedyNav.js - JS to handle the burger 🍔 menu.
Smooth Scroll - A lightweight script to animate scrolling to anchor links.
Gumshoe - A simple vanilla JS scrollspy script.
jQuery throttle / debounce - Allows you to rate-limit your functions in multiple useful ways.
Lunr - In site search engine
Bigfootjs - A better way to handle footnotes.

Issues
Please submit any on [issues][], but please read first the CONTRIBUTING.md file.
Contributing

Please read CONTRIBUTING.md for details about how to contribute and the process of submitting a pull request to this repo. Note that we have subscribe to the Contributor Covenant as our code of conduct while participating in this community. Please read CODE_OF_CONDUCT.md if you don't know about it.
I more than happy to accept contributions to the code and even more to the content —like for example if you encounter a typo or something like that or you are a native English speaker and something you're reading isn't quite right. Even more if you think there is a mistake somewhere.
Please contribute.
Versioning
I don't use any versioning system —and I don't know if it's appropriate for a project like a blog— and in general I follow the versioning system used for the Minimal Mistakes template, which is SemVer. You can check the available versions taking a look to the tags on this repository.
Authors

Michael Rose - Initial work and template creator - @mmistakes
Luis Puerto - Maintainer and author of this fork and the blog that it hosts - @luispuerto

See also the list of contributors who participated on this project either on my side or in mmistakes' ones.
License
Following Michael Rose, I've continued to license the code of this project under the MIT License —see the LICENSE file for details. However, the content of the website is license under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International license ——see the LICENSE4CONTENT file for details.
Minimal Mistakes template uses also a series of additional software and plugins, and I've also added some to this fork. You can see the licenses of those in the LICENSE4VENDORS file.
Acknowledgments
I would like to specially acknowledge Michael Rose work creating the template I'm using that it's awesome. And of course all the contributors to Minimal Mistakes template.
",batch2,8:10:17,Done
143,edsolis/homebrew-core-mirror,"Homebrew Core
Core formulae for the Homebrew package manager.
Core formulae for the Linuxbrew package manager (Homebrew on Linux or Windows 10 Subsystem for Linux) are in Homebrew/linuxbrew-core.

How do I install these formulae?
Just brew install <formula>. This is the default tap for Homebrew and is installed by default.
More Documentation, Troubleshooting, Contributing, Security, Community, Donations, License and Sponsors
See these sections in Homebrew/brew's README.
",batch1,16:58:59,Done
144,coldnew-examples/emacs-nativecomp-mirror,,batch1,16:59:00,Done
145,dhkim717/fabric," 
Incubation Notice
This project is a Hyperledger project in Incubation. It was proposed to the community and documented here. Information on what Incubation entails can be found in the Hyperledger Project Lifecycle document.
Hyperledger Fabric
The fabric is an implementation of blockchain technology, leveraging familiar and proven technologies. It is a modular architecture allowing pluggable implementations of various function. It features powerful container technology to host any mainstream language for smart contracts development.
Contributing to the project
We welcome contributions to the Hyperledger Project in many forms. There's always plenty to do! Full details of how to contribute to this project are documented in the CONTRIBUTING.md file.
Maintainers
The project's maintainers: are responsible for reviewing and merging all pull requests and they guide the over-all technical direction of the project within the guidelines established by the Hyperledger Project's Technical Steering Committee (TSC).
Communication 
We use Hyperledger Slack for communication and Google Hangouts™ for screen sharing between developers.
Installing the fabric
Installation: Describes how to install the blockchain fabric and use project tools.
Documentation
README: Project documentation.
Still Have Questions?
For general purpose questions, please use StackOverflow.
License 
The Hyperledger Project uses the Apache License Version 2.0 software license.
Related information
If you are new to the project, you can begin by reviewing the following documents:

Whitepaper WG
Requirements WG
Protocol Specification

",batch2,8:10:17,Done
146,rockbaaska/wot-xvm,,batch2,8:10:18,Done
147,alaha999/FastSimPhaseII,,batch1,16:57:58,Done
148,0xAX/emacs,,batch1,16:59:00,Done
149,bazurbat/jagen,"Jagen

Jagen is a utility tool that saves software engineers time. It combines selected features of
workspace and package managers to automate common day-to-day development and integration tasks such
as setting up the environment, downloading distribution archives and toolchains, managing patches,
keeping source repositories up to date and rebuilding parts of the project as needed.
It is designed to be a very lightweight alternative to
Repo/GClient
and Yocto.
Introduction

Features
Getting Started
Package Rules
Building

Reference

Initialization
Build
Clean
Update
List Information
Managing Sources
Filesystem images
Rust Support
Rules
Build System
Bash completions

Features

Simple declarative rules to define packages.
Initialize a build root using a single command (fast onboarding).
Reproduce the same environment on CI and development systems.
Build or rebuild any stage of any package respecting dependencies.
Complex projects can consist of layers (similar to OE/Yocto but much more straightforward).
Out of the box support for common build systems: CMake, Autotools, Android Gradle. In many cases
can handle packages using bare Make as well without additional configuration.
Built-in source management facilities (similar to Repo and GClient) supporting
Git and Mercurial (Hg).
Automatic downloading of distribution archives (from everything curl supports + special handling
of Google Drive).
Designed for cross-compilation from the start.
Easy to add custom pre-built toolchains.
Supports using of multiple toolchains in the same build root.
Packages can dynamically export the environment or settings for other packages.
First-class Rust language support.
Fully assisted Bash completion (fast and offers items relevant to the current project).
An extensive set of facilities to accommodate packages with custom build systems or special
needs.

Getting Started
A directory which groups related source code, distribution archives, build artifacts and other
auxiliary files, is called a ""workspace"", a ""root directory"" or a ""build root"" interchangeably.
Build roots can consist of multiple layers defining a project's environment and components which are
merged together according to rules to form a complete build system.
To start using Jagen create a directory for the workspace and make it current:
mkdir jagen-root && cd jagen-root

Initialize this directory by piping Jagen's init script into the shell:
curl -fsSL https://git.io/fhyEM | sh

Create a rules.lua file and add your rules there. Use the generated ./jagen script to run the
build system.
If you already have some layers prepared pass their URLs to the shell after the --. For example,
to create a workspace preconfigured with a tutorial layer
located at https://github.com/bazurbat/jagen-start use the command:
curl -fsSL https://git.io/fhyEM | sh -s -- https://github.com/bazurbat/jagen-start

See a list of defined packages:
./jagen list packages

Run the build system:
./jagen build

During the build, Jagen will download/clone sources and run packages configure/compile/install
stages in order as needed until everything is done. You can find the results in a host
subdirectory which is used as a default staging directory for packages using the system's native
toolchain.
To modify the list of layers or flags or refresh the generated environment for an existing workspace
run the init script again with the appropriate arguments inside the workspace's directory. To
guard against unexpected changes to the script in the master branch rerun it from the local copy
which was cloned during the workspace creation:
./.jagen/init [OPTIONS...]

For more complex configuration changes than just setting flags or layers edit the config.sh file
manually. Do not rerun the init script in this case because it generates the default config.sh.
To download rules from the tutorial layer for experiments:
curl -fsSL https://raw.githubusercontent.com/bazurbat/jagen-start/master/rules.lua > rules.lua

Edit the rules.lua file and rerun the ./jagen build command to bring the workspace up to date.
Package Rules
Jagen generates a build system from declarative rules found in rules.lua files across the
workspace's layers. An order is significant with the later rules appending and overriding preceding
definitions. The rules.lua file in the workspace itself is processed last, so it is a usual place
to define package exclusions and build type (release/debug) specific for the build root. In the
simplest case, the workspace's rules.lua can define all the packages as well but for easier
sharing of the resulting environment, it is recommended to put it into a separate Git repository and
use init commands outlined above once the ruleset stabilizes.
For an example, here are the contents of the rules.lua from the tutorial layer:
package { 'nanomsg',
    source = {
        location = 'https://github.com/nanomsg/nanomsg.git',
        tag = '1.1.4'
    },
    build = 'cmake'
}

package { 'googletest',
    source = {
        location = 'https://github.com/google/googletest.git',
        branch = 'v1.8.x'
    },
    build = 'cmake'
}

package { 'hello-nanomsg',
    source = 'https://github.com/bazurbat/hello-nanomsg.git',
    build = 'cmake',
    requires = { 'nanomsg', 'googletest' }
}

package { 'sqlite',
    source = {
        location = 'https://www.sqlite.org/2018/sqlite-autoconf-3250200.tar.gz',
        sha1sum = 'aedfbdc14eb700099434d6a743135743cff47393'
    },
    build = {
        type = 'gnu',
        options = {
            '--disable-editline',
            '--disable-threadsafe',
            '--disable-dynamic-extensions',
            '--disable-fts4',
            '--disable-fts5',
            '--disable-json1',
            '--disable-rtree',
            '--disable-static-shell'
        }
    }
}

package { 'hello-sqlite',
    source = 'https://github.com/bazurbat/hello-sqlite.git',
    build = 'cmake',
    requires = 'sqlite'
}
The simplest rule has the form:
package { 'mypackage' }
Which defines an empty package named ""mypackage"". To enable Jagen's source management facilities you
need to specify the source location of the package by setting the source property, for example:
package { 'mypackage',
    source = 'https://github.com/nanomsg/nanomsg.git'
}
Run the jagen source update mypackage command and find the nanomsg sources in the src/mypackage
subdirectory of the build root. So, to use Jagen as a workspace manager it is enough to define a few
packages this way and issue jagen source update command periodically to update them. If no other
source properties are set Jagen will clone the default branch (""master"" most of the time in the case
of Git). Perhaps more common case, especially for dependent repositories, is a need to clone a
specific tag. This can be done by setting the tag property of the source:
package { 'nanomsg',
    source = { 'https://github.com/nanomsg/nanomsg.git',
        tag = '1.1.4'
    }
}
or, alternatively, using the non-shorthand syntax:
package { 'nanomsg',
    source = {
        location = 'https://github.com/nanomsg/nanomsg.git',
        tag = '1.1.4'
    }
}
When a tag is set Jagen will not update the source after the initial clone because tags are not
supposed to change upstream. You can also set a specific revision in a similar manner:
package { 'nanomsg',
    source = {
        location = 'https://github.com/nanomsg/nanomsg.git',
        rev = 'ef4123ff70c74b47b66ef066ecf88d1ed3750dc3'
    }
}
Note that you need to specify a full hash of the commit. This is also true for Mercurial sources
because revision numbers are repository-specific.
Instead of fixing the source to a single revision you can set a branch:
package { 'nanomsg',
    source = {
        location = 'https://github.com/nanomsg/nanomsg.git',
        branch = 'ws'
    }
}
This way the jagen source update command will try to bring the specified branch up to date. This
form is useful for tracking upstream development.
Building
In addition to the source management, Jagen includes extensive facilities to orchestrate the
building of software. Its role is to prepare an environment, compile and install dependencies and
call a package's own build system with the correct parameters. To enable the build support you need
to specify the type of the build system of the package, like so:
package { 'nanomsg',
    source = {
        location = 'https://github.com/nanomsg/nanomsg.git',
        tag = '1.1.4'
    },
    build = 'cmake'
}
Run jagen build nanomsg command to build it. By default, the building of the package also implies
that it needs to be installed to be found by dependent packages. Given the rule above the nanomsg
will be installed in the host subdirectory of the build root.
Package dependencies can be specified with requires property:
package { 'nanomsg',
    source = {
        location = 'https://github.com/nanomsg/nanomsg.git',
        tag = '1.1.4'
    },
    build = 'cmake'
}

package { 'googletest',
    source = {
        location = 'https://github.com/google/googletest.git',
        branch = 'v1.8.x'
    },
    build = 'cmake'
}

package { 'hello-nanomsg',
    source = 'https://github.com/bazurbat/hello-nanomsg.git',
    build = 'cmake',
    requires = { 'nanomsg', 'googletest' }
}
Just run jagen build. Jagen will clone/update sources and build everything in order as needed.
",batch2,8:09:16,Done
150,heyitsmdr/arcadiamud,"
ranvier
Node.js-based MUD engine
Ranvier is a MUD game engine whose goal is to be a simple but powerful way to build whatever MUD you want with special care given to extensibility. The core code strives to be completely unopinionated toward any specific style of game while using the bundle system to build the game you want without having to dig through the engine's code.
Special Features

Robust bundle system: Nearly every aspect of the game can be modified without changing the core and allows for easy
packaging and sharing of commands/areas/items/npcs/channels/behaviors
Unopinionated network layer: easily swap out telnet for any network layer you like. No need to gut the whole codebase
just to support a different transport type, just drop in a file.
Customizable data layer: You are not tied to saving in any particular database or file storage sytem
Optional coordinate based room system allowing for the flexibilty of a standard MUD world with the easy mappability of
a strict 3D world.
Scripting for all entities in the game for any event along with behaviors to create shared, composable scripts
Skill system with passive/active skills
Effects e.g., buffs/debuffs
Quest system allowing for starting/progress/completion from any event in the game
Communication channels with custom audiences

Documentation
Ranvier prides itself on having thorough documentation which is available on our website: ranviermud.com
Slack
We have a Slack channel you can use to ask questions, suggest features, or just keep up to date with the project: https://ranviermud.slack.com
Get an invite
Requirements

Node.js >= v10.12.0

Demo
Point your favorite client or telnet to ranviermud.com port 4000. This demo server is wiped and updated from the master branch every hour.
",batch2,8:10:17,Done
151,AmesianX/llvm-project-v2,"Disclaimer
The llvm-monorepo-root,
llvm-project-v2,
llvm-project-v2-split,
llvm-project-v1, and
llvm-project-v1-split repositories are
WIP repositories that are used for development and prototyping of an llvm-project monorepo
that will be used for Apple's compiler releases and for the open source Swift project.
Please see this forum post
for more details.
Original readme follows:
The LLVM Compiler Infrastructure
This directory and its subdirectories contain source code for LLVM,
a toolkit for the construction of highly optimized compilers,
optimizers, and runtime environments.
",batch1,16:57:58,Done
152,craigwmcclellan/craigwmcclellan.github.io,"Neo Cactus for Jekyll
Demo: https://mmarfil.com/
Screenshot

This Jekyll theme started as a port of Cactus to my own needs, but I ended up performing a lot more modifications than expected. Some people reached me out and asked if I could share it, so here we are.
Disclaimer: I'm only a designer, so please don't expect the code to be pretty.
Usage
To start your project, fork this respository, put in your content, and go!
",batch1,16:57:58,Done
153,stinsonga/emacs-mirror,,batch1,16:58:59,Done
154,christopherfujino/homebrew-flutter,"Flutter Homebrew Tap
These are custom Homebrew formulae used for building packages to be distributed with the Flutter application framework.
How do I install these formulae?
Just brew install <formula>. This is the default tap for Homebrew and is installed by default.
More Documentation, Troubleshooting, Contributing, Security, Community, Donations, License and Sponsors
See these sections in Homebrew/brew's README.
",batch1,16:58:59,Done
155,sx-aurora-dev/llvm,"LLVM for NEC SX-Aurora VE
This repository is a clone of public LLVM repository (http://llvm.org), plus an
experimental modifications which provides support for the NEC SX-Aurora TSUBASA
Vector Engine (VE). See README_MORE.rst.
You can start with the PRM package.
% yum install \
  https://sx-aurora.com/repos/veos/ef_extra/x86_64/llvm-ve-1.3.0-1.3.0-1.x86_64.rpm \
  https://sx-aurora.com/repos/veos/ef_extra/x86_64/llvm-ve-link-1.3.0-1.x86_64.rpm

Then use clang like below.  Clang++ is also available.
$ /opt/nec/nosupport/llvm-ve/clang -target ve-linux -O3 ...


If you are interested in intrinsic functions for vector instructions, see
the manual.
If you are interested in the guided vectorization, or region vectorizer, see
RV.
If you want to build the llvm-ve, see
llvm-dev and
Compile.rst.
Automatic vectorization is not supported.

",batch1,16:57:58,Done
156,sailfishos-mirror/emacs,,batch1,16:59:00,Done
157,nelsonsbrian/ranviermud,"
ranvier
Node.js-based MUD engine
Ranvier is a MUD game engine whose goal is to be a simple but powerful way to build whatever MUD you want with special care given to extensibility. The core code strives to be completely unopinionated toward any specific style of game while using the bundle system to build the game you want without having to dig through the engine's code.
Special Features

Robust bundle system: Nearly every aspect of the game can be modified without changing the core and allows for easy
packaging and sharing of commands/areas/items/npcs/channels/behaviors
Unopinionated network layer: easily swap out telnet for any network layer you like. No need to gut the whole codebase
just to support a different transport type, just drop in a file.
Customizable data layer: You are not tied to saving in any particular database or file storage sytem
Optional coordinate based room system allowing for the flexibilty of a standard MUD world with the easy mappability of
a strict 3D world.
Scripting for all entities in the game for any event along with behaviors to create shared, composable scripts
Skill system with passive/active skills
Effects e.g., buffs/debuffs
Quest system allowing for starting/progress/completion from any event in the game
Communication channels with custom audiences

Documentation
Ranvier prides itself on having thorough documentation which is available on our website: ranviermud.com
Slack
We have a Slack channel you can use to ask questions, suggest features, or just keep up to date with the project: https://ranviermud.slack.com
Get an invite
Requirements

Node.js >= v10.12.0

Demo
Point your favorite client or telnet to ranviermud.com port 4000. This demo server is wiped and updated from the master branch every hour.
",batch2,8:10:17,Done
158,piskvorky/pazpar2,,batch2,8:09:16,Done
159,jadonk/cape-firmware,"This tree is now deprecated
Please see https://github.com/beagleboard/linux for the latest sources.
devicetree-source
The soon-to-be definitive source for all the devicetree bits for all of the BeagleBoard.org boards and capes.
",batch1,16:57:57,Done
160,google/llvm-propeller,"Propeller: Profile Guided Optimizing Large Scale LLVM-based Relinker
Background
We recently evaluated Facebook’s BOLT, a Post Link Optimizer
framework, on large google benchmarks and noticed that it improves key
performance metrics of these benchmarks by 2% to 6%, which is pretty
impressive as this is over and above a baseline binary already heavily
optimized with ThinLTO + PGO. Furthermore, BOLT is also able to
improve the performance of binaries optimized via Context-Sensitive
PGO. While ThinLTO + PGO is also profile guided and does very
aggressive performance optimizations, there is more room for
performance improvements due to profile approximations while applying
the transformations. BOLT uses exact profiles from the final binary
and is able to fill the gaps left by ThinLTO + PGO. The performance
improvements due to BOLT come from basic block layout, function
reordering and function splitting.
While BOLT does an excellent job of squeezing extra performance from highly optimized
binaries with optimizations such as code layout, it has these major issues:


It does not take advantage of distributed build systems.


It has scalability issues and to rewrite a binary with a ~300M text segment size:


Memory foot-print is 70G.


It takes more than 10 minutes to rewrite the binary.




Similar to Full LTO, BOLT’s design is monolithic as it disassembles
the original binary, optimizes and rewrites the final binary in one
process. This limits the scalability of BOLT and the memory and time
overhead shoots up quickly for large binaries.
Inspired by the performance gains and to address the scalability issue
of BOLT, we went about designing a scalable infrastructure that can
perform BOLT-like post-link optimizations. In this RFC, we discuss our
system, “Propeller”, which can perform profile guided link time binary
optimizations in a scalable way and is friendly to distributed build
systems. Our system leverages the existing capabilities of the
compiler tool-chain and is not a stand alone tool. Like BOLT, our
system boosts the performance of optimized binaries via link-time
optimizations using accurate profiles of the binary. We discuss the
Propeller system and show how to do the whole program basic block
layout using Propeller.
Propeller does whole program basic block layout at link time via basic
block sections. We have added support for having each basic block in
its own section which allows the linker to do arbitrary reorderings of
basic blocks to achieve any desired fine-grain code layout which
includes block layout, function splitting and function reordering.
Our experiments on large real-world applications and SPEC with code
layout show that Propeller can optimize as effectively as BOLT, with
just 20% of its memory footprint and time overhead.
An LLVM branch with propeller patches is available in the git
repository here: https://github.com/google/llvm-propeller/ We will
upload patches for review for the various elements
This directory and its sub-directories contain source code for LLVM,
a toolkit for the construction of highly optimized compilers,
optimizers, and run-time environments.
The README briefly describes how to get started with building LLVM.
For more information on how to contribute to the LLVM project, please
take a look at the
Contributing to LLVM guide.
Getting Started with the LLVM System
Taken from https://llvm.org/docs/GettingStarted.html.
Overview
Welcome to the LLVM project!
The LLVM project has multiple components. The core of the project is
itself called ""LLVM"". This contains all of the tools, libraries, and header
files needed to process intermediate representations and converts it into
object files.  Tools include an assembler, disassembler, bitcode analyzer, and
bitcode optimizer.  It also contains basic regression tests.
C-like languages use the Clang front end.  This
component compiles C, C++, Objective-C, and Objective-C++ code into LLVM bitcode
-- and from there into object files, using LLVM.
Other components include:
the libc++ C++ standard library,
the LLD linker, and more.
Getting the Source Code and Building LLVM
The LLVM Getting Started documentation may be out of date.  The Clang
Getting Started page might have more
accurate information.
This is an example work-flow and configuration to get and build the LLVM source:


Checkout LLVM (including related sub-projects like Clang):


git clone https://github.com/llvm/llvm-project.git


Or, on windows, git clone --config core.autocrlf=false  https://github.com/llvm/llvm-project.git




Configure and build LLVM and Clang:


cd llvm-project


mkdir build


cd build


cmake -G <generator> [options] ../llvm
Some common build system generators are:

Ninja --- for generating Ninja
build files. Most llvm developers use Ninja.
Unix Makefiles --- for generating make-compatible parallel makefiles.
Visual Studio --- for generating Visual Studio projects and
solutions.
Xcode --- for generating Xcode projects.

Some Common options:


-DLLVM_ENABLE_PROJECTS='...' --- semicolon-separated list of the LLVM
sub-projects you'd like to additionally build. Can include any of: clang,
clang-tools-extra, libcxx, libcxxabi, libunwind, lldb, compiler-rt, lld,
polly, or debuginfo-tests.
For example, to build LLVM, Clang, libcxx, and libcxxabi, use
-DLLVM_ENABLE_PROJECTS=""clang;libcxx;libcxxabi"".


-DCMAKE_INSTALL_PREFIX=directory --- Specify for directory the full
path name of where you want the LLVM tools and libraries to be installed
(default /usr/local).


-DCMAKE_BUILD_TYPE=type --- Valid options for type are Debug,
Release, RelWithDebInfo, and MinSizeRel. Default is Debug.


-DLLVM_ENABLE_ASSERTIONS=On --- Compile with assertion checks enabled
(default is Yes for Debug builds, No for all other build types).




cmake --build . [-- [options] <target>] or your build system specified above
directly.


The default target (i.e. ninja or make) will build all of LLVM.


The check-all target (i.e. ninja check-all) will run the
regression tests to ensure everything is in working order.


CMake will generate targets for each tool and library, and most
LLVM sub-projects generate their own check-<project> target.


Running a serial build will be slow.  To improve speed, try running a
parallel build.  That's done by default in Ninja; for make, use the option
-j NNN, where NNN is the number of parallel jobs, e.g. the number of
CPUs you have.




For more information see CMake




Consult the
Getting Started with LLVM
page for detailed information on configuring and compiling LLVM. You can visit
Directory Layout
to learn about the layout of the source code tree.
",batch1,16:57:57,Done
161,erjoalgo/emacs,,batch1,16:59:00,Done
162,kostasziridis/Homebrew,"Homebrew Core
Core formulae for the Homebrew package manager.
Core formulae for the Linuxbrew package manager (Homebrew on Linux or Windows 10 Subsystem for Linux) are in Homebrew/linuxbrew-core.

How do I install these formulae?
Just brew install <formula>. This is the default tap for Homebrew and is installed by default.
More Documentation, Troubleshooting, Contributing, Security, Community, Donations, License and Sponsors
See these sections in Homebrew/brew's README.
",batch1,16:57:59,Done
163,dandavison/emacs-mac,,batch1,16:59:00,Done
164,kolyshkin/vzctl,,batch2,8:10:17,Done
165,archlinux/archweb,"Archweb README

To get a pretty version of this document, run
$ markdown README > README.html

License
See LICENSE file.
Authors
See AUTHORS file.
Dependencies

python
rsync (optional for mirrorcheck with rsync mirrors)

Python dependencies
More detail in requirements.txt and requirements_prod.txt; it is best to
use virtualenv and pip to handle these. But if you insist on (Arch Linux)
packages, you will probably want the following:

python-django
python-psycopg2
python-markdown
python-memcached

Testing Installation


Run python -m venv env.
 cd /path/to/archweb && python -m venv ./env/



Activate the virtualenv.
 source ./env/bin/activate



Install dependencies through pip.
 pip install -r requirements.txt



Copy local_settings.py.example to local_settings.py and modify.
Make sure to uncomment the appropriate database section (either sqlite or
PostgreSQL).


Migrate changes.
 ./manage.py migrate



Load the fixtures to pre populate some data. If you don't want some of the
provided data, adjust the file glob accordingly.
 ./manage.py loaddata main/fixtures/*.json
 ./manage.py loaddata devel/fixtures/*.json
 ./manage.py loaddata mirrors/fixtures/*.json
 ./manage.py loaddata releng/fixtures/*.json



Use the following commands to start a service instance
 ./manage.py runserver



To optionally populate the database with real data:
 wget http://mirrors.kernel.org/archlinux/core/os/x86_64/core.db.tar.gz
 ./manage.py reporead x86_64 core.db.tar.gz
 # Package file listing
 wget http://mirrors.kernel.org/archlinux/core/os/x86_64/core.files.tar.gz
 ./manage.py reporead --filesonly x86_64 core.files.tar.gz



Alter architecture and repo to get x86_64 and packages from other repos if
needed.


Database Updates for Added/Removed packages
 sqlite3 archweb.db < packages/sql/update.sqlite3.sql



For PostgreSQL use packages/sql/update.postgresql_psycopg2.sql
Testing SMTP server
To be able to create an account on your test environment an SMTP server is
required. A simple debugging SMTP server can be setup using Python.
    python -m smtpd -n -c DebuggingServer localhost:1025

In local_settings.py add entries to set EMAIL_HOST to 'localhost' and EMAIL_PORT to
1025.
Running tests and coverage
To the unittests execute the following commands:
    ./manage.py collectstatic --noinput
    ./manage.py test

Running coverage:
    pip install coverage
    coverage run --omit='env*' --source='.' manage.py test
    coverage report

Django Debug toolbar
To use the Django Debug toolbar install django-debug-toolbar and in local_settings.py
set DEBUG_TOOLBAR to True.
Management commands
Archweb provides multiple management commands for importing various sorts of data. An overview of commands:

generate_keyring - Assemble a GPG keyring with all known developer keys.
pgp_import - Import keys and signatures from a given GPG keyring.
read_rebuilderd_status - Import rebuilderd status into Archweb.
rematch_developers - Rematch flag requests and packages where user_id/packager_id is NULL to a Developer.
reporead - Parses a repo.db.tar.gz, repo.files.tar.gz file and updates the Arch database with the relevant changes.
reporead_inotify - Watches a templated patch for updates of *.files.tar.gz to update Arch databases with.
donor_import - Import a single donator from a mail passed to stdin
mirrorcheck - Poll every active mirror URLs to store the lastsnyc time and record network timing details.
mirrorresolv - Poll every active mirror URLs and determine wheteher they have IP4 and/or IPv6 addresses.
populate_signoffs - retrieves the latest commit message of a signoff-eligible package.
update_planet - Import all feeds for users who have a valid website and website_rss in their user profile.
read_links - Reads a repo.links.db.tar.gz file and updates the Soname model.
read_links_inotify - Watches a templated patch for updates of *.links.tar.gz to update Arch databases with.

Updating iPXE image
The netboot image can be updated by building the AUR
package (note that it builds
from git master) and copying the resulting ipxe.pxe, ipxe.lkrn and ipxe.efi to
sitestatic/netboot. Then as Arch Linux Developer sign them with your PGP key
gpg --output ipxe.efi.sig --detach-sig ipxe.efi.
Testing a build iPXE image requires the 'qemu' package and running the
following command:
    qemu-system-x86_64 -kernel ipxe.lkrn -m 2G

Production Installation
Arch Linux has an Ansible role for Archweb in their infrastructure repo.
vim: set syntax=markdown et:
",batch2,8:10:17,Done
166,jandockx/ppwcode-recovered-from-google-code,,batch2,8:09:15,Done
167,rjdp/EE-dbmigrate,"[] (https://travis-ci.org/rtCamp/easyengine)


EasyEngine (ee) is a python tool, which makes it easy to manage your wordpress sites running on nginx web-server.
refered http://alembic.readthedocs.org/en/latest/tutorial.html#the-migration-environment for site migration
requirements = alembic
use pip3 install alembic ,, since apt-get install alembic --> adds alembic to python2 version
EasyEngine currently supports:

Ubuntu 12.04 & 14.04
Debian 7 & 8

Port Requirements:

22/TCP (Inbound/Outbound) : Standard SSH port
80/TCP (Inbound/Outbound) : Standard HTTP port
443/TCP(Inbound/Outbound) : Standard HTTPS port
22222/TCP (Inbound)       : To access EasyEngine admin tools
11371/TCP (Outbound)      : To connect to GPG Key Server

Quick Start
wget -qO ee rt.cx/ee && sudo bash ee     # Install easyengine 3
sudo ee site create example.com --wp     # Install required packages & setup WordPress on example.com
Update EasyEngine
Update procedure for EasyEngine to latest version
For current installed version prior to 3.0.6
wget -qO ee rt.cx/ee && sudo bash ee

If current version is after than 3.0.6
ee update

More Site Creation Commands
Standard WordPress Sites
ee site create example.com --wp                  # install wordpress without any page caching
ee site create example.com --w3tc                # install wordpress with w3-total-cache plugin
ee site create example.com --wpsc                # install wordpress with wp-super-cache plugin
ee site create example.com --wpfc                # install wordpress + nginx fastcgi_cache
WordPress Multsite with subdirectory
ee site create example.com --wpsubdir            # install wpmu-subdirectory without any page caching
ee site create example.com --wpsubdir --w3tc     # install wpmu-subdirectory with w3-total-cache plugin
ee site create example.com --wpsubdir --wpsc     # install wpmu-subdirectory with wp-super-cache plugin
ee site create example.com --wpsubdir --wpfc     # install wpmu-subdirectory + nginx fastcgi_cache
WordPress Multsite with subdomain
ee site create example.com --wpsubdomin            # install wpmu-subdomain without any page caching
ee site create example.com --wpsubdomain --w3tc     # install wpmu-subdomain with w3-total-cache plugin
ee site create example.com --wpsubdomain --wpsc     # install wpmu-subdomain with wp-super-cache plugin
ee site create example.com --wpsubdomain --wpfc     # install wpmu-subdomain + nginx fastcgi_cache
Non-WordPress Sites
ee site create example.com --html     # create example.com for static/html sites
ee site create example.com --php      # create example.com with php support
ee site create example.com --mysql    # create example.com with php & mysql support
HHVM Enabled Sites
ee site create example.com --wp --hhvm           # create example.com WordPress site with HHVM support
ee site create example.com --php --hhvm          # create example.com php site with HHVM support
PageSpeed Enabled Sites
ee site create example.com --wp --pagespeed      # create example.com WordPress site with PageSpeed support
ee site create example.com --php --pagespeed     # create example.com php site with PageSpeed support
Cheatsheet - Site creation




Single Site
Multisite w/ Subdir
Multisite w/ Subdom




NO Cache
--wp
--wpsubdir
--wpsubdomain


WP Super Cache
--wpsc
--wpsubdir --wpsc
--wpsubdomain --wpsc


W3 Total Cache
--w3tc
--wpsubdir --w3tc
--wpsubdomain --w3tc


Nginx cache
--wpfc
--wpsubdir --wpfc
--wpsubdomain --wpfc



Useful Links

[Documentation] (http://docs.rtcamp.com/easyengine/)
[FAQ] (http://docs.rtcamp.com/easyengine/faq.html)
[Conventions used] (http://rtcamp.com/wordpress-nginx/tutorials/conventions/)
[EasyEngine Premium Support] (https://rtcamp.com/products/easyengine-premium-support/)

Donations
[]  (https://rtcamp.com/donate/?project=easyengine)
Careers
We are looking for [Python Developers] (https://rtcamp.com/careers/python-developer/) to join our team.
We offer work from home, so you can join EasyEngine team anywhere! [Why Python?] (https://rtcamp.com/blog/easyengine-3-roadmap/#whypython)

License
[MIT] (http://opensource.org/licenses/MIT)
",batch2,8:09:16,Done
168,dinojr/dinojr-emacs,,batch1,16:59:00,Done
169,Homebrew/homebrew-core,"Homebrew Core
Core formulae for the Homebrew package manager.
Core formulae for the Linuxbrew package manager (Homebrew on Linux or Windows 10 Subsystem for Linux) are in Homebrew/linuxbrew-core.
Homebrew/discussions (forum)
How do I install these formulae?
Just brew install <formula>. This is the default tap for Homebrew and is installed by default.
More Documentation, Troubleshooting, Contributing, Security, Community, Donations, License and Sponsors
See these sections in Homebrew/brew's README.
",batch1,16:58:59,Done
170,nwjs/chromium.src," Chromium
Chromium is an open-source browser project that aims to build a safer, faster,
and more stable way for all users to experience the web.
The project's web site is https://www.chromium.org.
To check out the source code locally, don't use git clone! Instead,
follow the instructions on how to get the code.
Documentation in the source is rooted in docs/README.md.
Learn how to Get Around the Chromium Source Code Directory Structure
.
For historical reasons, there are some small top level directories. Now the
guidance is that new top level directories are for product (e.g. Chrome,
Android WebView, Ash). Even if these products have multiple executables, the
code should be in subdirectories of the product.
",batch1,16:57:57,Done
171,dirkwhoffmann/virtualc64,"


",batch2,8:09:16,Done
172,dongruibing1614/lombok,,batch2,8:10:17,Done
173,galaxiegaming/discord-bot-1,"LenoxBot

LenoxBot is a Discord bot that offers many cool new features to your Discord server!






Table of Content

Installation
Support
License

Installation
You can find the whole instructions to download and how to configure the bot in our documentation here.
Support
Twitter
Discord Server
Documentation
License
MIT
",batch2,8:09:16,Done
174,cms-sw/cmssw,,batch1,16:57:58,Done
175,NorinAB/saiku,"





    When something doesn't work as expected, then please subscribe to the
    Saiku User Group list
    and send your doubt. If that doesn't solve your problem, then please ask for help on
    Saiku Dev Group.
  


Saiku Analytics
The world's greatest open source OLAP browser



Homepage |
  Saiku License |
  Wiki |
  Community |
  Mailing List |
  Chat |
  News



  Saiku allows business users to explore complex data sources,
  using a familiar drag and drop interface and easy to understand
  business terminology, all within a browser. Select the data you
  are interested in, look at it from different perspectives,
  drill into the detail. Once you have your answer, save your results,
  share them, export them to Excel or PDF, all straight from the browser.
  (more)




    Please consider supporting development by making a
    donation.
  


Setup
Build Instructions
mvn clean install -DskipTests

mvn clean clover2:setup test clover2:aggregate clover2:clover
Update project version
To update the pom versions run:
mvn versions:set -DnewVersion=3.x.x
Then remove the backups with:
find . -name ""*.versionsBackup"" -type f -delete
Get Saiku License
Saiku is open source and free to use. Our default server does ship with a license server installed. To get a license you can visit http://licensing.meteorite.bi and get a FREE license which is pinned to the major release of the server, but does not expire at any time. This helps us with a more accurate picture of installation numbers and deployments.
Wiki

Saiku Wiki

Community

Saiku Community

Bugs and Feature Requests

GitHub Issues

Discussion List

Saiku Dev Group
Saiku User Group
Stack Overflow
Freenode IRC - Channel: ##saiku

Browser Support
We do care about it.












Latest ✔
Latest ✔
Latest ✔
Latest ✔
Latest ✔



Team
Saiku is maintained by these people and a bunch of awesome contributors.













Breno Polanski
Bruno Catão
Luis Garcia
Mark Cahill
Paul Stoellberger
Tom Barber



Contributing
Check CONTRIBUTING.md for more details. Some important information:


To get started, sign the Contributor License Agreement.


If you find a bug then please create an issue here.


If you have a feature request, then please get in touch. We'd love to hear from you! Send a email for: info@meteorite.bi


History
For detailed changelog, check Releases.
License
Saiku and the Saiku UI are free software. The UI, contained in this repository, is available under the terms of the Apache License Version 2. A copy is attached for your convenience.
⬆ back to top
",batch2,8:09:16,Done
176,vigour-io/vjs,"


vigour-js
",batch2,8:10:17,Done
177,mrthuanvn/wot-xvm,,batch2,8:10:18,Done
178,ygelfand/errbit,"Errbit    
The open source, self-hosted error catcher
Errbit is a tool for collecting and managing errors from other applications.
It is Airbrake API compliant, so if you are already using
Airbrake, you can just point the airbrake gem to your Errbit server.







Apps






Errors






Error Summary






Error Backtraces



Mailing List
Join the Google Group at https://groups.google.com/group/errbit to receive
updates and notifications.
Requirements
The list of requirements to install Errbit are:

Ruby >= 2.3.x
MongoDB 3.4.x-4.0.x

Installation
Note: This app is intended for people with experience deploying and maintaining
Rails applications.

Install MongoDB
git clone https://github.com/errbit/errbit.git
bundle install
bundle exec rake errbit:bootstrap
bundle exec rails server

Configuration
Errbit configuration is done entirely through environment variables. See
configuration
Deployment
See notes on deployment
Notice Grouping
The way Errbit arranges notices into error groups is configurable. By default,
Errbit uses the notice's error class, error message, complete backtrace,
component (or controller), action and environment name to generate a unique
fingerprint for every notice. Notices with identical fingerprints appear in the
UI as different occurences of the same error and notices with differing
fingerprints are displayed as separate errors.
Changing the fingerprinter (under the ""Config"" menu) applies to all apps and
the change affects only notices that arrive after the change. If you want to
refingerprint old notices, you can run rake errbit:notice_refingerprint.
Since version 0.7.0, the notice grouping can be separately configured for each
app (under the ""edit"" menu).
Managing apps
An Errbit app is a place to collect error notifications from your external
application deployments.
See apps
Authentication
Configuring GitHub authentication:

Set GITHUB_AUTHENTICATION=true
Register your instance of Errbit at https://github.com/settings/applications/new

If you host Errbit at errbit.example.com, you would fill in:

URL
http://errbit.example.com
Callback URL
http://errbit.example.com/users/auth/github/callback


After you have registered your app, set GITHUB_CLIENT_ID and GITHUB_SECRET
with your app's Client ID and Secret key.

When you start your application, you should see the option to Sign in with
GitHub on the Login page. You will also be able to link your GitHub profile
to your user account on your Edit profile page.
If you have signed in with GitHub, or linked your GitHub profile, and you're
working with an App that has a GitHub repo configured, then you will be able to
create issues on GitHub. If you use another issue tracker, see Issue
Trackers.
You can change the OAuth scope Errbit requests from GitHub by setting
GITHUB_ACCESS_SCOPE. The default ['repo'] is very permissive, but there are a
few others that could make sense for your needs:

GITHUB_ACCESS_SCOPE=""['repo']""
Allow creating issues for public and private repos
GITHUB_ACCESS_SCOPE=""['public_repo']""
Allow creating issues for public repos only
GITHUB_ACCESS_SCOPE=""[]""
No permissions at all, but allows errbit login through github


GITHUB_ORG_ID is an optional environment variable you can set to your own
github organization id. If set, only users of the specified GitHub
organization can log in to Errbit through GitHub. Errbit will provision
accounts for new users.

Configuring Google authentication:

Set GOOGLE_AUTHENTICATION=true
Register your instance of Errbit at https://console.developers.google.com/apis/api/plus/overview

If you host Errbit at errbit.example.com, you would fill in:

URL
http://errbit.example.com
Callback URL
http://errbit.example.com/users/auth/google_oauth2/callback


After you have registered your app, set GOOGLE_CLIENT_ID and GOOGLE_SECRET
with your app's Client ID and Secret key.

When you start your application, you should see the option to Sign in with
Google on the Login page. You will also be able to link your Google profile
to your user account on your Edit profile page.
Configuring LDAP authentication:

Set ERRBIT_USER_HAS_USERNAME=true
Follow the devise_ldap_authenticatable setup instructions.
Set config.ldap_create_user = true in config/initializers/devise.rb, this enables creating the users from LDAP, otherwhise login will not work.
Create a new initializer (e.g. config/initializers/devise_ldap.rb) and add the following code to enable ldap authentication in the User-model:

Errbit::Config.devise_modules << :ldap_authenticatable

If you are authenticating by username, you will need to set the user's email manually
before authentication. You must add the following lines to app/models/user.rb:

  def ldap_before_save
    name = Devise::LDAP::Adapter.get_ldap_param(self.username, ""givenName"")
    surname = Devise::LDAP::Adapter.get_ldap_param(self.username, ""sn"")
    mail = Devise::LDAP::Adapter.get_ldap_param(self.username, ""mail"")

    self.name = (name + surname).join ' '
    self.email = mail.first
  end

Now login with your user from LDAP, this will create a user in the database
Open a rails console and set the admin flag for your user:

user = User.first
user.admin = true
user.save!
Upgrading
When upgrading Errbit, please run:
git pull origin master # assuming origin is the github.com/errbit/errbit repo
bundle install
rake db:migrate
rake db:mongoid:remove_undefined_indexes
rake db:mongoid:create_indexes
rake assets:precompile
This will ensure that your application stays up to date with any schema changes.
There are additional steps if you are upgrading from a version prior to v0.4.0.
User information in error reports
Errbit can now display information about the user who experienced an error.
This gives you the ability to ask the user for more information,
and let them know when you've fixed the bug.
The Airbrake gem will look for current_user or current_member. By default it will only send the id of the user, to specify other attributes you can set config.user_attributes. See the Airbrake wiki for more information.
If user information is received with an error report,
it will be displayed under the User Details tab:

This tab will be hidden if no user information is available.
Javascript error notifications
You can log javascript errors that occur in your application by including the
airbrake-js javascript library.
Install airbrake-js according to the docs at and set your project and host as
soon as you want to start reporting errors. Then follow along with the
documentation at https://github.com/airbrake/airbrake-js/blob/master/README.md
var airbrake = new airbrakeJs.Client({
  projectId: 'ERRBIT API KEY',
  projectKey: 'ERRBIT API KEY (again)',
  reporter: 'xhr',
  host: 'https://myerrbit.com'
});
Plugins and Integrations
You can extend Errbit by adding Ruby gems and plugins which are typically gems.
It's nice to keep track of which gems are core Errbit dependencies and which
gems are your own dependencies. If you want to add gems to your own Errbit,
place them in a new file called UserGemfile and Errbit will treat that file
as an additional Gemfile. If you want to use a file with a different name, you
can pass the name of that file in an environment variable named USER_GEMFILE.
If you want to use errbit_jira_plugin, just add it to UserGemfile:
echo ""gem 'errbit_jira_plugin'"" > UserGemfile
bundle install
Issue Trackers
Each issue tracker integration is implemented as a gem that depends on
errbit_plugin. The only officially
supported issue tracker plugin is
errbit_github_plugin.
If you want to implement your own issue tracker plugin, read the README.md file
at errbit_plugin.
What if Errbit has an error?
Errbit will log it's own errors to an internal app named Self.Errbit.  The
Self.Errbit app is automatically created when the first error happens.
If your Errbit instance has logged an error, we would appreciate a bug report
on GitHub Issues. You can post this manually at
https://github.com/errbit/errbit/issues,
or you can set up the GitHub Issues tracker for your Self.Errbit app:

Go to the Self.Errbit app's edit page. If that app does not exist yet,
go to the apps page and click Add a new App to create it. (You can also
create it by running rake airbrake:test.)
In the Issue Tracker section, click GitHub Issues.
Fill in the Account/Repository field with errbit/errbit.
Fill in the Username field with your github username.
If you are logged in on GitHub, you can find your
API Token on this page:
https://github.com/account/admin.
Save the settings by clicking Update App (or Add App)
You can now easily post bug reports to GitHub Issues by clicking the
Create Issue button on a Self.Errbit error.

Getting Help
If you need help, try asking your question on StackOverflow using the
tag errbit:
https://stackoverflow.com/questions/tagged/errbit
Use Errbit with applications written in other languages
In theory, any Airbrake-compatible error catcher for other languages should work with Errbit.
Solutions known to work are listed below:



Language
Project




PHP (>= 5.3)
flippa/errbit-php


OOP PHP (>= 5.3)
emgiezet/errbitPHP


Python
mkorenkov/errbit.py , pulseenergy/airbrakepy



People using Errbit
See our wiki page for a list of people and companies around the world who use
Errbit. You may
edit this
page, and add
your name and country to the list if you are using Errbit.
Special Thanks

Michael Parenteau - For rocking the Errbit design and providing a great user experience.
Nick Recobra (@oruen) - Nick is Errbit's first core contributor. He's been working hard at making Errbit more awesome.
Nathan Broadbent (@ndbroadbent) - Maintaining Errbit and contributing many features
Vasiliy Ermolovich (@nashby) - Contributing and helping to resolve issues and pull requests
Marcin Ciunelis (@martinciu) - Helping to improve Errbit's architecture
Cyril Mougel (@shingara) - Maintaining Errbit and contributing many features
Relevance - For giving me Open-source Fridays to work on Errbit and all my awesome co-workers for giving feedback and inspiration.
Thoughtbot - For being great open-source advocates and setting the bar with Airbrake.

See the contributors graph for more details.
Contributing to Errbit
See the contribution guidelines
Running tests
Check the .travis.yml file to see how tests are run
Copyright
Copyright (c) 2010-2015 Errbit Team
",batch2,8:10:18,Done
179,mtezych/llvm,,batch1,16:57:58,Done
180,aTonyXiao/packer-templates,"packer-templates 
Collection of Packer templates used for various infrastructure layers.
How to build stuff
To build a given template, one may use the make implicit builder, like the
following for ci-stevonnie:
make ci-stevonnie
or, with a specific builder:
make ci-stevonnie BUILDER=docker
or forget about the Makefile and run with packer directly:
packer build -only=docker <(bin/yml2json < ci-stevonnie.yml)
env config bits
Most of the templates in here require some env vars.  Take a look at
.example.env for an example.  Use of
autoenv is encouraged but not
required.
packer template types
There are two primary types of templates present at the top level: those
intended for use as execution environment for jobs flowing through Travis CI,
and those used for various backend fun in the Travis CI infrastructure.  The
former type all have the prefix ci-, described in more detail below:
stacks
There are two primary types of stacks: those targeting Ubuntu 14.04 (trusty),
and those targeting Ubuntu 16.04 (xenial) that run on GCE and Docker.
Take a peek at what's what:
make stacks-trusty
make stacks-xenial
There may be some subtle variations, but for the most part each stack is built
via the following steps.
git metadata file input
The generated files in ./tmp/git-meta/ are copied onto the provisioned machine
at /var/tmp/git-meta/ for later use by the ./packer-scripts/packer-env-dump
script.
purge file input
A git-tracked file in ./packer-assets is copied onto the provisioned machine
at /var/tmp/purge.txt for later use by the ./packer-scripts/purge script.
packages file input
A git-tracked file in ./packer-assets is copied onto the provisioned machine
at /var/tmp/packages.txt for later use by both the
travis_packer_templates::default recipe and the serverspec suites via
./cookbooks/lib/support.rb.
write packer and travis env vars
The script at ./packer-scripts/packer-env-dump creates a directory on the
provisioned machine at /.packer-env which is intended to be in the envdir
format.  Any environment variables
that match ^(PACKER|TRAVIS), and (if present) the files previously written to
/var/tmp/git-meta/ are copied or written into /.packer-env/.
remove default users
The script at ./packer-scripts/remove-default-users will perform a best-effort
removal of users defined in ${DEFAULT_USERS} (default vagrant ubuntu).  The
primary reasons for this are general tidyness and to try to free up uid 2000.
pre-chef bootstrapping
The script at ./packer-scripts/pre-chef-bootstrap is responsible for ensuring
the provisioned machine has all necessary packages and users for the Chef
provisioning process.  The steps executed include:

remove the ""partner"" APT source list file
remove all cached APT list files
install APT packages needed by Chef
ensure /var/run/sshd dir exists
ensure sshd: ALL: ALLOW exists in /etc/hosts.allow
ensure there is a travis user
change the travis user password to travis
ensure #includedir /etc/sudoers.d exists in /etc/sudoers
ensure the /etc/sudoers.d dir exists
ensure the /etc/sudoers.d/travis file exists with specific permissions
ensure the /home/travis/.ssh dir exists
ensure the /home/travis/.ssh/authorized_keys file exists
add /var/tmp/*_rsa.pub to /home/travis/.ssh/authorized_keys
ensure /home/travis/.ssh/authorized_keys perms are 0600
ensure the /home/travis/bin dir exists

cloning travis-cookbooks
The script at ./packer-scripts/clone-travis-cookbooks is responsible for git clone'ing travis-cookbooks
into /tmp/chef-stuff on the provisioned machine.  Optional env vars supported
by this script are:

TRAVIS_COOKBOOKS_BRANCH - the branch specified during git clone
TRAVIS_COOKBOOKS_EDGE_BRANCH - the default branch used if
TRAVIS_COOKBOOKS_BRANCH is not defined
TRAVIS_COOKBOOKS_URL - the git clone remote (default
https://github.com/travis-ci/travis-cookbooks.git)
TRAVIS_COOKBOOKS_SHA - a git tree-ish to which the clone will be checked
out if defined (default not set)

Once the clone is complete, the clone directory is written to
/.packer-env/TRAVIS_COOKBOOKS_DIR and the head sha is written to
/.packer-env/TRAVIS_COOKBOOKS_SHA.
chef provisioning
The chef-solo provisioner will typically have no json data, but instead will
leave all attribute and effective run list definition to a single wrapper
cookbook located in ./cookbooks/.
chef wrapper cookbook layout
Each wrapper cookbook must contain at least a metadata.rb and a
recipes/default.rb.  Typically, the attributes/default.rb is defined and
contains all override attribute settings.  The earliest version of Chef used by
either trusty or xenial stacks is 12.9, which means that all cookbook
dependencies must be declared in metadata.rb, a requirement that is also
enforced by the foodcritic checks.
For example, the minimal trusty image ""ci-stevonnie"" has a wrapper cookbook at
./cookbooks/travis-ci_stevonnie that looks like this:
cookbooks/travis_ci_stevonnie
├── README.md
├── attributes
│   └── default.rb
├── metadata.rb
├── recipes
│   └── default.rb
└── spec
    ├── ...

travis user double check
The script at ./packer-scripts/ensure-travis-user is responsible for ensuring
the existence of the travis user and its home directory permissions,
optionally setting the password to a random string.  The list of operations is:

ensure the travis user exists
set the travis user password
ensure /home/travis exists
ensure /home/travis/.ssh/authorized_keys and /home/travis/.ssh/known_hosts
both exist and have permissions of 0600
blank out /home/travis/.ssh/authorized_keys
ensure /home/travis is fully owned by travis:travis

Optional env vars supported by this script are:

TRAVIS_USER_PASSWORD - a string (default ""travis"")
TRAVIS_OBFUSCATE_PASSWORD - if non-empty, causes
TRAVIS_USER_PASSWORD to be set to a random string

purging undesirable packages
The script at ./packer-scripts/purge is responsible for purging packages that
are not desirable for the CI environment, such as the Chef that was installed
prior for the Chef provisioner.  Additionally, any package names present in
/var/tmp/purge.txt will be purged.  Optional env vars supported by this script
are:

APT_GET_UPGRADE_DURING_CLEANUP - if non-empty, triggers an apt-get -y upgrade prior to package purging.
CLEAN_DEV_PACKAGES - if non-empty, purges any packages matching -dev$

disabling apparmor
The script at ./packer-scripts/disable-apparmor is responsible for disabling
apparmor if detected.  This is done primarily so that services such as
PostgreSQL and Docker may be used in the CI environment without first updating
apparmor configuration and restaring said services.
running server specs
The script at ./packer-scripts/run-serverspecs is responsible for running the
serverspec suites via the rspec executable that is part of the chefdk package.
The list of operations is:

install the chefdk package
create a sudo-bash wrapper for use in some specs
ensure all spec files are owned by travis:travis
run each suite defined in ${SPEC_SUITES}
optionally remove the chefdk package

Optional env vars supported by this script are:

PACKER_CHEF_PREFIX - directory in which to find packer chef stuff (default
/tmp)
SPEC_RUNNER - string used to wrap execution of rspec (default sudo -u travis HOME=/home/travis -- bash -lc)
SPEC_SUITES - comma-delimited string of spec suites to run (default not set)
SKIP_CHEFDK_REMOVAL - if non-empty do not remove the chefdk package and
APT source

removing undesirable files
The script at ./packer-scripts/cleanup is responsible for removing files and
directories that are unnecessary for the CI environment or otherwise add
unnecessary mass to the mastered image.  The list of operations is:

recursively remove a bunch of files and directories
conditionally remove /var/lib/apt/lists/*
conditionally remove /var/lib/man-db
conditionally remove /home/travis/linux.iso and /home/travis/shutdown.sh
empty all files in /var/log

Optional env vars supported by this script are:

CLEANUP_APT_LISTS - if non-empty, trigger removal of /var/lib/apt/lists/*
CLEANUP_MAN_DB - if non-empty, trigger removal of /var/lib/man-db

minimizing image size
The script at ./packer-scripts/minimize is responsible for reducing the size
of the provisioned image by squeezing out all of the empty space into a
contiguous area using the same method as
bento.
The list of operations is:

exit 0 if $PACKER_BUILDER_TYPE is either googlecompute or amazon-ebs,
as minimizing like this is superfluous on those builders
if $PACKER_BUILDER_TYPE is not docker, turn off swap and zero out the swap
partition if available.
write zeros to /EMPTY until the disk is out of space
remove /EMPTY and run sync
if the vmware-toolbox-cmd is available, run disk shrink operations for both
/ /boot paths.

registering the image with job-board
The script at ./bin/job-board-register is responsible for ""registering"" the
mastered image in a post-processing step by making an HTTP request to the
job-board images API.  The list of
operations is:

source any available env vars exported from the provisioned VM
dump any env vars with prefixes ^(PACKER|TRAVIS|TAGS|IMAGE_NAME)
define a TAGS env var that will be used as the tags HTTP request param.
define a URI-escaped query string from several env vars
perform the HTTP request to job-board with curl and pipe the response
through jq

Required env vars for this script are:

JOB_BOARD_IMAGES_URL - the URL including PATH_INFO prefix to job-board
IMAGE_NAME - the name of the image, typically the same as that used by the
target infrastructure

Optional env vars supported by this script are:

PACKER_ENV_DIR - path to the envdir containing packer-specific env vars,
default /.packer-env
TAGS - initial value for tags set during job-board registration
GROUP - value used in group tag, default edge if edge conditions match,
else dev
DIST - value used in dist tag, default either Linux release codename or OS
X product version
OS - value used in os tag, default lowercase value of uname, mapped to
osx on Darwin

For more info on the relationship between a given packer build artifact and
job-board, see job-board details below.
job-board details
The job-board application is
responsible for tracking stack image metadata and presenting a queryable API
that is used by the travis-worker API image
selector.
As described above, each stack image is registered with job-board along with a
group, os, dist, and map of tags.  When travis-worker requests a stack
image identifier, it performs a series of queries with progressively lower
specificity.
Of the values assigned to each stack image, the tags map is perhaps most
mysterious, in part because it is so loosely defined.  This is intentional, as
the number of values that could be considered ""tags"" varies enough that
maintaining them all as individual columns would result in (opinions!) too much
overhead in the form of schema management and query complexity.
The implementation of the job-board-register
script includes a process that converts the
languages and features arrays present in /.job-board-register.yml, written
from the values present in chef attributes at
travis_packer_templates.job_board.{features,languages}, into ""sets""
represented as {key} => true.  For example, if a given wrapper cookbook
contains attributes like this:
override['travis_packer_templates']['job_board']['languages'] = %w(
  fribble
  snurp
  zzz
)
then the tags generated for registration with job-board would be equivalent to:
{
  ""language_fribble"": true,
  ""language_snurp"": true,
  ""language_zzz"": true
}
job-board tagsets
A ""tagset"" is the ""set"" (as in the type) of the ""tags"" applied during job-board
registration of a particular stack image, including languages and features.
At the time of this writing, both tagsets are used during serverspec runs, and
only the languages tagset is considered during selection via the job-board
API.
tagset relationships
Because the travis-worker API image
selector
is querying job-board for stack images that match a particular language, it is
important for us to ensure reasonably consistent image selection by way of
asserting the languages values do not overlap between certain stacks (an
""exclusive"" relationship).  Additionally, it is important that we ensure certain
stack features are subsets of others (an ""inclusive"" relationship).
Part of the CI process for this repository makes assertions about such
exclusive and inclusive relationships by way of the
check-job-board-tags script.  The exact
relationships being enforced may be viewed like so:
./bin/check-job-board-tags --list-only
exclusive relationships
An exclusive tagset relationship is equivalent to asserting that the set
intersection is the empty set, e.g.:
tagset_a = %w(a b c)
tagset_b = %w(d e f)
assert (tagset_a & tagset_b).empty?
inclusive relationships
An inclusive tagset relationship is equivalent to asserting that all members
of one tagset are present in another, or that a tagset's intersection with its
superset is equivalent to itself, e.g.:
tagset_a = %w(a b c d e f)
tagset_b = %w(f d b)
assert (tagset_a & tagset_b).sort == tagset_b.sort
Testing cookbook changes
When submitting changes to this repository, please be aware that
the top level-specs are shallow and don't include logic changes in the cookbooks.
Any cookbook specs are ran as part of the actual image building
process, which is triggered when any of the ci-<image-name>.yml
templates are modified.
The image build is ran as part of the
packer-build
repo on the branch corresponding to each template and is triggered by
travis-packer-build.
This can be installed and invoked locally by running bundle install
and then bundle exec travis-packer-build [options].
Example:
bundle exec travis-packer-build \
	-I ci-sardonyx.yml \
	--target-repo-slug=""travis-infrastructure/packer-build"" \
	--github-api-token=""<your-token-here>"" \
	--body-tmpl="".packer-build-pull-request-false-tmpl.yml""

You can specify the branch using -B (if you don't want to build from master).
The file .packer-build-pull-request-false-tmpl.yml here is just an
example, but you can also create a different template that specifies
other travis-cookbooks or packer-template branches.
Additionaly, if you just want to test a change in
travis-cookbooks, you
can use the shortcut script in ./bin/packer-build-cookbooks-branch:
./bin/packer-build-cookbooks-branch <travis-cookbooks-branch-name> <template-name>

Note: The above script expects the GITHUB_API_TOKEN
environment variable to be set.
Once created, the images will be registered in job-board under the
group: dev tag.
",batch2,8:09:16,Done
181,nanosoftsystem/ai,"Mycroft  

Mycroft is a hackable open source voice assistant.
Table of Contents

Table of Contents
Getting Started
Running Mycroft
Using Mycroft

Home Device and Account Manager
Skills


Behind the scenes

Pairing Information
Configuration
Using Mycroft Without Home
API Key Services
Using Mycroft behind a proxy

Using Mycroft behind a proxy without authentication
Using Mycroft behind an authenticated proxy




Getting Involved
Links

Getting Started
First, get the code on your system!  The simplest method is via git (git installation instructions):

cd ~/
git clone --depth=1 https://github.com/MycroftAI/mycroft-core.git
cd mycroft-core
bash dev_setup.sh

This script sets up dependencies and a virtualenv.  If running in an environment besides Ubuntu/Debian, Arch or Fedora you may need to manually install packages as instructed by dev_setup.sh.
NOTE: The default branch for this repository is 'dev', which should be considered a work-in-progress. If you want to clone a more stable version, switch over to the 'master' branch.
NOTE: If you are willing to contribute to this project, clone the entire repository by

git clone  https://github.com/MycroftAI/mycroft-core.git
instead of
git clone --depth=1 https://github.com/MycroftAI/mycroft-core.git
which is said above.

Running Mycroft
Mycroft provides start-mycroft.sh to perform common tasks. This script uses a virtualenv created by dev_setup.sh.  Assuming you installed mycroft-core in your home directory run:

cd ~/mycroft-core
./start-mycroft.sh debug

The ""debug"" command will start the background services (microphone listener, skill, messagebus, and audio subsystems) as well as bringing up a text-based Command Line Interface (CLI) you can use to interact with Mycroft and see the contents of the various logs. Alternatively you can run ./start-mycroft.sh all to begin the services without the command line interface.  Later you can bring up the CLI using ./start-mycroft.sh cli.
The background services can be stopped as a group with:

./stop-mycroft.sh

Using Mycroft
Home Device and Account Manager
Mycroft AI, Inc. maintains a device and account management system known as Mycroft Home. Developers may sign up at: https://home.mycroft.ai
By default, mycroft-core  is configured to use Home. By saying ""Hey Mycroft, pair my device"" (or any other request verbal request) you will be informed that your device needs to be paired. Mycroft will speak a 6-digit code which you can entered into the pairing page within the Mycroft Home site.
Once paired, your unit will use Mycroft API keys for services such as Speech-to-Text (STT), weather and various other skills.
Skills
Mycroft is nothing without skills.  There are a handful of default skills that are downloaded automatically to your /opt/mycroft/skills directory, but most need to be installed explicitly.  See the Skill Repo to discover skills made by others.  And please share your own interesting work!
Behind the scenes
Pairing Information
Pairing information generated by registering with Home is stored in:
~/.mycroft/identity/identity2.json <-- DO NOT SHARE THIS WITH OTHERS!
Configuration
Mycroft configuration consists of 4 possible locations:

mycroft-core/mycroft/configuration/mycroft.conf(Defaults)
Mycroft Home (Remote)
/etc/mycroft/mycroft.conf(Machine)
$HOME/.mycroft/mycroft.conf(User)

When the configuration loader starts, it looks in these locations in this order, and loads ALL configurations. Keys that exist in multiple configuration files will be overridden by the last file to contain the value. This process results in a minimal amount being written for a specific device and user, without modifying default distribution files.
Using Mycroft Without Home
If you do not wish to use the Mycroft Home service, you may insert your own API keys into the configuration files listed below in configuration.
The place to insert the API key looks like the following:
[WeatherSkill]
api_key = """"
Put a relevant key inside the quotes and mycroft-core should begin to use the key immediately.
API Key Services
These are the keys currently used in Mycroft Core:

STT API, Google STT, Google Cloud Speech
Weather Skill API, OpenWeatherMap
Wolfram-Alpha Skill

Using Mycroft behind a proxy
Many schools, universities and workplaces run a proxy on their network. If you need to type in a username and password to access the external internet, then you are likely behind a proxy.
If you plan to use Mycroft behind a proxy, then you will need to do an additional configuration step.
NOTE: In order to complete this step, you will need to know the hostname and port for the proxy server. Your network administrator will be able to provide these details. Your network administrator may want information on what type of traffic Mycroft will be using. We use https traffic on port 443, primarily for accessing ReST-based APIs.
Using Mycroft behind a proxy without authentication
If you are using Mycroft behind a proxy without authentication, add the following environment variables, changing the proxy_hostname.com and proxy_port for the values for your network. These commands are executed from the Linux command line interface (CLI).
$ export http_proxy=http://proxy_hostname.com:proxy_port
$ export https_port=http://proxy_hostname.com:proxy_port
$ export no_proxy=""localhost,127.0.0.1,localaddress,.localdomain.com,0.0.0.0,::1""
Using Mycroft behind an authenticated proxy
If  you are behind a proxy which requires authentication, add the following environment variables, changing the proxy_hostname.com and proxy_port for the values for your network. These commands are executed from the Linux command line interface (CLI).
$ export http_proxy=http://user:password@proxy_hostname.com:proxy_port
$ export https_port=http://user:password@proxy_hostname.com:proxy_port
$ export no_proxy=""localhost,127.0.0.1,localaddress,.localdomain.com,0.0.0.0,::1""
Getting Involved
This is an open source project and we would love your help. We have prepared a contributing guide to help you get started.
If this is your first PR or you're not sure where to get started,
say hi in Mycroft Chat and a team member would be happy to mentor you.
Join the Mycroft Forum for questions and answers.
Links

Creating a Skill
Documentation
Release Notes
Mycroft Chat
Mycroft Forum
Mycroft Blog

",batch1,16:57:57,Done
182,h2oota/emacs-win64-msvc,,batch1,16:59:01,Done
183,aki237/Emacs--scamE,,batch1,16:59:00,Done
184,johndebord/emacs-fork,,batch1,16:58:59,Done
185,msart/gitg,,batch2,8:10:17,Done
186,dr3mro/plowshare,,batch2,8:10:18,Done
187,t-suwa-forks/emacs,,batch1,16:59:00,Done
188,YorkZ/emacs,,batch1,16:59:01,Done
189,curtclifton/curtclifton.github.io,"Kiko
Kiko is a theme for Jekyll, the static site generator. It's designed and developed by @gfjaru.
See it live
Get Started

Fork this repository
Clone the repository to your computer.git clone https://github.com/YOURUSERNAME/Kiko
Run it.jekyll serve
Go to http://127.0.0.1:4000.

Make it yours

Change name and add/remove nav at _config.yml.
Change about.md.

License
This theme is released under MIT License.
",batch1,16:57:57,Done
190,tupeloTS/telescopety,"

Telescope is an open-source, real-time social news site built with Meteor
Note: Telescope is beta software. Most of it should work but it's still a little unpolished and you'll probably find some bugs. Use at your own risk :)
Note that Telescope is distributed under the MIT License
Getting Started
Note that while simply cloning this repository will work, it is recommended you clone the sample project repository instead for a simpler workflow.
Please refer to the documentation for more instructions on installing Telescope.
Learn More

Homepage
Demo
Sample Project
Documentation
Roadmap
Slack
Meta – Discussions about Telescope

",batch2,8:09:16,Done
191,srivasta/debian.packaging.make-doc,,batch2,8:10:18,Done
192,SwiftLawnGnome/emacs-gsoc,,batch1,16:58:59,Done
193,DeMoorJasper/Maltrail,"Maltrail, modified for easy extendability
This repo is a fork of Maltrail, this fork aims to create a more extendable, simplified, cleaner and advanced version of Maltrail.
This fork also includes a cleaner and more modern React/Node.js based web-api/dashboard for going through the logs.
Getting started
Installing Maltrail
git clone https://github.com/DeMoorJasper/maltrail.git
cd maltrail
Setup sensor
Installing dependencies
sudo apt-get install git python-pcapy
curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py && python get-pip.py
pip install impacket requests
Running sensor
This command assumes you're inside the maltrail folder.
sudo python sensor.py
Setup webserver
Installing Node.js
First Install nvm.
Once that's finished install node 8 using nvm install 8.
Install/Build webserver
This command assumes you're inside the maltrail folder and have node installed.
make build-webserver
Running the webserver
This command assumes you're inside the maltrail folder and have node installed.
make run-webserver
Documentation
For the basic functionality you can find some documentation in the original repo: Maltrail. However this is slightly outdated and this fork is lacking features that the original project had and vice versa.
For the plugin/trigger functionality there is no documentation at the moment other than the existing (example) plugins and triggers.
Contributing
We welcome any contributor, especially on the plugin side.
The goal of this project is to be a powerful IDS out of the box that is super extendable so it can be used in more extensive research and practise.
If you're intrested known work and bugs are listed in the issues section. Feel free to check it out, ask questions and hopefully try to implement/fix it with a PR.
License
This project is licensed under MIT.
Original Maltrail was written by @stamparm
This fork is written/maintained by @DeMoorJasper
",batch2,8:09:16,Done
194,1ucian0/qiskit-terra-fuzz,"Qiskit Terra

Qiskit is an open-source framework for working with Noisy Intermediate-Scale Quantum (NISQ) computers at the level of pulses, circuits, and algorithms.
Qiskit is made up of elements that work together to enable quantum computing. This element is Terra and is the foundation on which the rest of Qiskit is built.
Installation
We encourage installing Qiskit via the pip tool (a python package manager), which installs all Qiskit elements, including Terra.
pip install qiskit
PIP will handle all dependencies automatically and you will always install the latest (and well-tested) version.
To install from source, follow the instructions in the contribution guidelines.
Creating Your First Quantum Program in Qiskit Terra
Now that Qiskit is installed, it's time to begin working with Terra.
We are ready to try out a quantum circuit example, which is simulated locally using
the Qiskit BasicAer element. This is a simple example that makes an entangled state.
$ python

>>> from qiskit import *
>>> q = QuantumRegister(2)
>>> c = ClassicalRegister(2)
>>> qc = QuantumCircuit(q, c)
>>> qc.h(q[0])
>>> qc.cx(q[0], q[1])
>>> qc.measure(q, c)
>>> backend_sim = BasicAer.get_backend('qasm_simulator')
>>> result = execute(qc, backend_sim).result()
>>> print(result.get_counts(qc))
In this case, the output will be:
{'00': 513, '11': 511}
A script is available here, where we also show how to
run the same program on a real quantum computer via IBMQ.
Executing your code on a real quantum chip
You can also use Qiskit to execute your code on a
real quantum chip.
In order to do so, you need to configure Qiskit for using the credentials in
your IBM Q account:
Configure your IBMQ credentials


Create an IBM Q > Account if you haven't already done so.


Get an API token from the IBM Q website under My Account > Advanced > API Token.


Take your token from step 2, here called MY_API_TOKEN, and run:
>>> from qiskit import IBMQ
>>> IBMQ.save_account('MY_API_TOKEN')


If you have access to the IBM Q Network features, you also need to pass the
URL listed on your IBM Q account page to save_account.


After calling IBMQ.save_account(), your credentials will be stored on disk.
Once they are stored, at any point in the future you can load and use them
in your program simply via:
>>> from qiskit import IBMQ
>>> IBMQ.load_accounts()
Those who do not want to save their credentials to disk should use instead:
>>> from qiskit import IBMQ
>>> IBMQ.enable_account('MY_API_TOKEN')
and the token will only be active for the session. For examples using Terra with real
devices we have provided a set of examples in examples/python and we suggest starting with using_qiskit_terra_level_0.py and working up in
the levels.
Contribution Guidelines
If you'd like to contribute to Qiskit Terra, please take a look at our
contribution guidelines. This project adheres to Qiskit's code of conduct. By participating, you are expected to uphold this code.
We use GitHub issues for tracking requests and bugs. Please
join the Qiskit Slack community
and use our Qiskit Slack channel for discussion and simple questions.
For questions that are more suited for a forum we use the Qiskit tag in the Stack Exchange.
Next Steps
Now you're set up and ready to check out some of the other examples from our
Qiskit Tutorials repository.
Authors and Citation
Qiskit Terra is the work of many people who contribute
to the project at different levels. If you use Qiskit, please cite as per the included BibTeX file.
License
Apache License 2.0
",batch2,8:10:18,Done
195,tyndy125/7.72-,"forgottenserver   
The Forgotten Server is a free and open-source MMORPG server emulator written in C++. It is a fork of the OpenTibia Server project. To connect to the server, you can use OTClient or OpenTibiaUnity.
Getting Started

Compiling, alternatively download AppVeyor builds for Windows
Scripting Reference

Support
If you need help, please visit the support forum on OTLand. Our issue tracker is not a support forum, and using it as one will result in your issue being closed. If you were unable to get assistance in the support forum, you should consider becoming a premium user on OTLand which grants you access to the premium support forum and supports OTLand financially.
Issues
We use the issue tracker on GitHub. Keep in mind that everyone who is watching the repository gets notified by e-mail when there is activity, so be thoughtful and avoid writing comments that aren't meaningful for an issue (e.g. ""+1""). If you'd like for an issue to be fixed faster, you should either fix it yourself and submit a pull request, or place a bounty on the issue.
",batch2,8:10:17,Done
196,shitolepriya/test-frappe,"frappe 
Full-stack web application framework that uses Python and MariaDB on the server side and a tightly integrated client side library. Built for ERPNext
Installation
Install via Frappe Bench
Website
For details and documentation, see the website
https://frappe.io
License
MIT License
",batch2,8:09:15,Done
197,chenguangming/llvm,,batch1,16:57:59,Done
198,nalaswad/nalaswad-closure,,batch2,8:10:17,Done
199,blackflux/lambda-monitor,"Lambda Monitoring








Lambda log monitoring and streaming to external services.
What it does

Parsing and analysis of AWS Lambda CloudWatch Logs
Pipes AWS Lambda Logs to external logging services (i.e. Loggly, Logz or Datadog)
Detects and sends AWS Lambda anomalies to external monitoring service (i.e. Rollbar)
Fully transparent, no changes to existing Lambda functions required


Setup
1. Create New Github Project

Install dev dependencies with yarn add -DE js-gardener @blackflux/robo-config-plugin eslint object-hash and
Dependencies with yarn add -E lambda-monitor
Add gardener.js containing

// eslint-disable-next-line import/no-extraneous-dependencies
const gardener = require('js-gardener');

if (require.main === module) {
  gardener().catch(() => process.exit(1));
}

Add .roboconfig.json containing (adjust as necessary!)

{
  ""lambda-monitor"": {
    ""tasks"": [
      ""assorted/@default""
    ],
    ""variables"": {
      ""enableCloudTrail"": ""FILL_IN"",
      ""awsRegion"": ""AWS_REGION""
    }
  },
  ""@blackflux/robo-config-plugin"": {
    ""tasks"": [
      ""assorted/@sls-closedsource""
    ],
    ""variables"": {
      ""repoKey"": ""ORG_NAME/REPO_NAME"",
      ""circleCiReadToken"": ""CIRCLE_CI_TOKEN"",
      ""projectName"": ""PROJECT_NAME"",
      ""owner"": ""GH_USER_NAME"",
      ""ownerName"": ""ORG_NAME"",
      ""mergeBot"": ""MERGE_BOT_NAME"",
      ""awsRegion"": ""AWS_REGION"",
      ""namespace"": ""com.sls.ORG_NAME""
    }
  }
}

Run node gardener.js
Then run yarn install && yarn u
Then enter docker container with . manage.sh
Then run yarn install && u && t
Create .depunusedignore

@blackflux/robo-config-plugin
@blackflux/eslint-plugin-rules
object-hash


Follow instructions of generated file HOWTO.md (and generated CONFDOCS.md)

Disable Logging for a Lambda Function
To exclude a lambda function from being monitored simply add the tag ""MONITORED"": ""0"". Note that you need to manually unsubscribe if process-log has already been subscribed to the CloudWatch stream.
How it works
While deploying this project is straight forward, there is a lot of complexity going on behind the scenes to ensure:

All Lambda functions are subscribed on initial deploy
Newly created Lambda function are immediately subscribed
Periodic checks for Lambda functions not subscribed (self healing)

There are four lambda function created per stage. All operations are only performed on lambda functions tagged with the corresponding stage.
process-logs - This lambda function is subscribed to CloudWatch and processes the logs. Anomalies are submitted to rollbar and all detected log events are sent to the configured logging services. Tagged with ""MONITOR"": ""1"" and ""MONITORED"": ""0"".
subscribe - Subscribes the process-logs lambda function (detected using the MONITOR tag) to all relevant CloudWatch Groups, excluding those functions that have the MONITORED tag set to 0.
empty-bucket - Empty CloudTrail bucket when stage is removed from AWS.
",batch2,8:10:18,Done
200,xDyN/PokemonGo-Bot1311,"PokemonGo-Bot
PokemonGo-Bot is a project created by the PokemonGoF team.
Is the bot working? Yes, since this PR
Table of Contents

Installation
Documentation
Support
Help
Bugs
Feature Requests
Pull Requests
Features
Credits

The project is currently setup in two main branches:

dev also known as beta - This is where the latest features are, but you may also experience some issues with stability/crashes.
master also known as stable - The bot 'should' be stable on this branch, and is generally well tested.

Slack Channel
Configuration issues/help
If you need any help please don't create an issue as we have a great community on Slack. You can count on the community in #help channel.

Click here to signup (first time only)
Join here if you're already a member

Development Channel

#dev channel in slack

Discord

Click here to join discord server

###Bugs / Issues
If you discover a bug in the bot, please search our issue tracker first. If it hasn't been reported, please create a new issue and ensure you follow the template guide so that our team can assist you as quickly as possible.
###Feature Requests
If you have a great idea to improve the bot, please search our feature tracker first to ensure someone else hasn't already come up with the same great idea.  If it hasn't been requested, please create a new request and ensure you follow the template guide so that it doesnt get lost with the bug reports.
While you're there vote on other feature requests to let the devs know what is most important to you.
###Pull Requests
If you'd like to make your own changes, make sure you follow the pull request template, and ensure your PR is made against the 'dev' branch.
If this is your first time making a PR or aren't sure of the standard practice of making a PR, here are some articles to get you started.

GitHub Pull Request Tutorial
How to write the perfect pull request
A great example from one of our own contributors

Features

 GPS Location configuration
 Search Pokestops
 Catch Pokemon
 Determine which pokeball to use (uses Razz Berry if the catch percentage is low!)
 Exchange Pokemon as per configuration
 Evolve Pokemon as per configuration
 Auto switch mode (Inventory Checks - switches between catch/farming items)
 Limit the step to farm specific area for pokestops
 Rudimentary IV Functionality filter
 Ignore certain pokemon filter
 Adjust delay between Pokemon capture & Transfer as per configuration
 Hatch eggs
 Incubate eggs
 Crowd Sourced Map Prototype
 [Standalone Desktop Application] (https://github.com/PokemonGoF/PokemonGo-Bot-Desktop)
 Use candy

Analytics
PokemonGo-Bot is very popular and has a vibrant community. Because of that, it has become very difficult for us to know how the bot is used and what errors people hit. By capturing small amounts of data, we can prioritize our work better such as fixing errors that happen to a large percentage of our user base, not just a vocal minority.
Our goal is to help inform our decisions by capturing data that helps us get aggregate usage and error reports, not personal information. To view the code that handles analytics in our master branch, you can use this search link.
If there are any concerns with this policy or you believe we are tracking something we shouldn't, please open a ticket in the tracker. The contributors always intend to do the right thing for our users, and we want to make sure we are held to that path.
If you do not want any data to be gathered, you can turn off this feature by setting health_record to false in your config.json.
Credits

tejado many thanks for the API
pogodevorg Without keyphact's coordination, this would not gonna happan again.
Mila432 for the login secrets
elliottcarlson for the Google Auth PR
AeonLucid for improved protos
AHAAAAAAA for parts of the s2sphere stuff
Breeze ro for some of the MQTT/Map stuff

Contributors

eggins [first pull request]
crack00r
ethervoid
Bashin
tstumm
TheGoldenXY
Reaver01
rarshonsky
earthchie
haykuro
05-032
sinistance
CapCap
YvesHenri
mzupan
gnekic(GeXx)
Shoh
JSchwerberg
luizperes
brantje
VirtualSatai
dmateusp
jtdroste
msoedov
Grace
Calcyfer
asaf400
guyz
DavidK1m
budi-khoirudin
riberod07
th3w4y
Leaklessgfy
steffwiz
pulgalipe
BartKoppelmans
phil9l
VictorChen
AlvaroGzP
fierysolid
surfaace
surceis
SpaceWhale
klingan
reddivision
DayBr3ak
kbinani
mhdasding
MFizz
NamPNQ
z4ppy.bbc
matheussampaio
Abraxas000
lucasfevi
pokepal
Moonlight-Angel
mjmadsen
nikofil
bigkraig
nikhil-pandey
thebigjc
JaapMoolenaar
eevee-github
g0vanish
cmezh
Nivong
kestel
simonsmh
joaodragao
extink
Quantra
pmquan
net8q
SyncX
umbreon222
DeXtroTip
rawgni
Breeze Ro
bruno-kenji
Gobberwart
javajohnHub
kolinkorr839

Disclaimer
©2016 Niantic, Inc. ©2016 Pokémon. ©1995–2016 Nintendo / Creatures Inc. / GAME FREAK inc. © 2016 Pokémon/Nintendo Pokémon and Pokémon character names are trademarks of Nintendo. The Google Maps Pin is a trademark of Google Inc. and the trade dress in the product design is a trademark of Google Inc. under license to The Pokémon Company. Other trademarks are the property of their respective owners.
Privacy Policy
PokemonGo-Bot is intended for academic purposes and should not be used to play the game PokemonGo as it violates the TOS and is unfair to the community. Use the bot at your own risk.
PokemonGoF does not support the use of 3rd party apps or apps that violate the TOS.

",batch2,8:10:17,Done
201,metaborg/spoofax-eclipse,"spoofax
",batch2,8:10:17,Done
202,Bind/my-zsh,"
Oh My Zsh is an open source, community-driven framework for managing your zsh configuration. That sounds boring. Let's try this again.
Oh My Zsh is a way of life! Once installed, your terminal prompt will become the talk of the town or your money back! Each time you interace with your command prompt, you'll be able take advantage of the hundreds of bundled plugins and pretty themes. Strangers will come up to you in cafés and ask you, ""that is amazing. are you some sort of genius?"" Finally, you'll begin to get the sort of attention that you always felt that you deserved. ...or maybe you'll just use the time that you saved to start flossing more often.
To learn more, visit http://ohmyz.sh and/or follow ohmyzsh on twitter.
Getting Started
Prerequisites
Disclaimer: Oh My Zsh works best on Mac OS X and Linux.

Unix-based operating system (Mac OS X or Linux)
Zsh should be installed (v4.3.9 or more recent)

This is commonly pre-installed. (zsh --version to confirm)


curl or wget should be installed

Basic Installation
Oh My Zsh is installed by running one of the following commands in your terminal. You can install this via the command-line with either curl or wget.
via curl
curl -L https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh | sh
via wget
wget https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O - | sh
Using Oh My Zsh
Plugins
Oh My Zsh comes with a shit load of plugins to take advantage of. You can take a look in the plugins directory and/or the wiki to see what's currently available.
Enabling Plugins
If you spot a plugin (or several) that you would like to use with Oh My Zsh, you will need to edit the ~/.zshrc file. Once you open it with your favorite editor, you'll see a spot to list all the plugins that you'd like Oh My Zsh to load in initialization.
For example, this line might begin to look like...
plugins=(git bundler osx rake ruby)
Using Plugins
Most plugins (should! we're working on this) include a README, which documents how to use them.
Themes
We'll admit it. Early in the Oh My Zsh world... we may have gotten a far too theme happy. We have over one hundred themes now bundled. Most of them have screenshots on the wiki. Check them out!
Selecting a Theme
Robby's theme is the default one. It's not the fanciest one. It's not the simplest one. It's just right (for him).
Once you find a theme that you want to use, you will need to edit the ~/.zshrc file. You'll see an environment variable (all caps) in there that looks like:
ZSH_THEME=""robbyrussell""
To use a different theme, simple change the value to match the name of your desired theme. For example:
ZSH_THEME=""agnoster"" (this is one of the fancy ones)
Open up a new terminal window and your prompt should look something like...
Advanced Topics
If you're the type that likes to get their hands dirty... these sections might resonate.
Advanced Installation
For those who want to install this manually and/or set custom paths.
Custom Directory
The default location is ~/.oh-my-zsh (hidden in your home directory)
If you'd like to change the install directory with the ZSH environment variable, either by running export ZSH=/your/path before installing, or by setting it before the end of the install pipeline like this:
curl -L https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh | ZSH=~/.dotfiles/zsh sh
Manual Installation
1. Clone the repository:
git clone git://github.com/robbyrussell/oh-my-zsh.git ~/.oh-my-zsh
2. Optionally, backup your existing @~/.zshrc@ file:
cp ~/.zshrc ~/.zshrc.orig
3. Create a new zsh configuration file
You can create a new zsh config file by copying the template that we included for you.
cp ~/.oh-my-zsh/templates/zshrc.zsh-template ~/.zshrc
4. Change your default shell
chsh -s /bin/zsh
5. Initialize your new zsh configuration
Once you open up a new terminal window, it should load zsh with Oh My Zsh's configuration.
Installation Problems
If you have any hiccups installing, here are a few common fixes.

You might need to modify your PATH in ~/.zshrc if you're not able to find some commands after switching to oh-my-zsh.
If you installed manually or changed the install location, check the ZSH environment variable in ~/.zshrc.

Custom Plugins and Themes
If you want to override any of the default behaviors, just add a new file (ending in .zsh) in the custom/ directory.
If you have many functions that go well together, you can put them as a abcyzeae.plugin.zsh file in the custom/plugins/ directory and then enable this plugin.
If you would like to override the functionality of a plugin distributed with Oh My Zsh, create a plugin of the same name in the custom/plugins/ directory and it will be loaded instead of the one in plugins/.
Getting Updates
By default, you will be prompted to check for upgrades every few weeks. If you would like oh-my-zsh to automatically upgrade itself without prompting you, set the following in your ~/.zshrc:
DISABLE_UPDATE_PROMPT=true
To disable automatic upgrades, set the following in your ~/.zshrc:
DISABLE_AUTO_UPDATE=true
Manual Updates
If you'd like to upgrade at any point in time (maybe someone just released a new plugin and you don't want to wait a week?)... you just need to run:
upgrade_oh_my_zsh
Magic!
Uninstalling Oh My Zsh
Oh My Zsh isn't for everyone. We'll miss you, but we want to make this an easy breakup.
If you want to uninstall oh-my-zsh, just run uninstall_oh_my_zsh from the command-line. It will remove itself and revert your previous bash or zsh configuration.
Contributing
I'm far from being a Zsh expert and suspect there are many ways to improve – if you have ideas on how to make the configuration easier to maintain (and faster), don't hesitate to fork and send pull requests!
We also need people to test out pull-requests. So take a look through the open issues and help where you can.
Do NOT Send Us Themes
We have (more than) enough themes for the time being. Please fork the project and add one in there – you can let people know how to grab it from there.
Contributors
Oh My Zsh has a vibrant community of happy users and delightful contributors. Without all the time and help from our contributors, it wouldn't be so awesome.
Thank you so much!
Follow Us
We have an ohmyzsh account. You should follow it.
Merchandise
We have stickers and shirts for you to show off your love of Oh My Zsh. Again, this will help you become the talk of the town!
LICENSE
Oh My Zsh is released under the MIT license.
",batch2,8:09:16,Done
203,Chuch2dope/test,"PokemonGo-Bot
PokemonGo-Bot is a project created by the PokemonGoF team.
Table of Contents

Installation
Documentation
Support
Help
Bugs
Feature Requests
Pull Requests
Features
Credits

The project is currently setup in two main branches:

dev also known as beta - This is where the latest features are, but you may also experience some issues with stability/crashes.
master also known as stable - The bot 'should' be stable on this branch, and is generally well tested.

Slack Channel
Configuration issues/help
If you need any help please don't create an issue as we have a great community on Slack. You can count on the community in #help channel.

Click here to signup (first time only)
Join here if you're already a member

Development Channel

#dev channel in slack

Discord

Click here to join discord server

###Bugs / Issues
If you discover a bug in the bot, please search our issue tracker first. If it hasn't been reported, please create a new issue and ensure you follow the template guide so that our team can assist you as quickly as possible.
###Feature Requests
If you have a great idea to improve the bot, please search our feature tracker first to ensure someone else hasn't already come up with the same great idea.  If it hasn't been requested, please create a new request and ensure you follow the template guide so that it doesnt get lost with the bug reports.
While you're there vote on other feature requests to let the devs know what is most important to you.
###Pull Requests
If you'd like to make your own changes, make sure you follow the pull request template, and ensure your PR is made against the 'dev' branch.
If this is your first time making a PR or aren't sure of the standard practice of making a PR, here are some articles to get you started.

GitHub Pull Request Tutorial
How to write the perfect pull request
A great example from one of our own contributors

Features

 GPS Location configuration
 Search Pokestops
 Catch Pokemon
 Determine which pokeball to use (uses Razz Berry if the catch percentage is low!)
 Exchange Pokemon as per configuration
 Evolve Pokemon as per configuration
 Auto switch mode (Inventory Checks - switches between catch/farming items)
 Limit the step to farm specific area for pokestops
 Rudimentary IV Functionality filter
 Ignore certain pokemon filter
 Adjust delay between Pokemon capture & Transfer as per configuration
 Hatch eggs
 Incubate eggs
 Crowd Sourced Map Prototype
 [Standalone Desktop Application] (https://github.com/PokemonGoF/PokemonGo-Bot-Desktop)
 Use candy

Analytics
PokemonGo-Bot is very popular and has a vibrant community. Because of that, it has become very difficult for us to know how the bot is used and what errors people hit. By capturing small amounts of data, we can prioritize our work better such as fixing errors that happen to a large percentage of our user base, not just a vocal minority.
Our goal is to help inform our decisions by capturing data that helps us get aggregate usage and error reports, not personal information. To view the code that handles analytics in our master branch, you can use this search link.
If there are any concerns with this policy or you believe we are tracking something we shouldn't, please open a ticket in the tracker. The contributors always intend to do the right thing for our users, and we want to make sure we are held to that path.
If you do not want any data to be gathered, you can turn off this feature by setting health_record to false in your config.json.
Credits

tejado many thanks for the API
pogodevorg Without keyphact's coordination, this would not gonna happan again.
Mila432 for the login secrets
elliottcarlson for the Google Auth PR
AeonLucid for improved protos
AHAAAAAAA for parts of the s2sphere stuff
Breeze ro for some of the MQTT/Map stuff

Contributors

eggins [first pull request]
crack00r
ethervoid
Bashin
tstumm
TheGoldenXY
Reaver01
rarshonsky
earthchie
haykuro
05-032
sinistance
CapCap
YvesHenri
mzupan
gnekic(GeXx)
Shoh
JSchwerberg
luizperes
brantje
VirtualSatai
dmateusp
jtdroste
msoedov
Grace
Calcyfer
asaf400
guyz
DavidK1m
budi-khoirudin
riberod07
th3w4y
Leaklessgfy
steffwiz
pulgalipe
BartKoppelmans
phil9l
VictorChen
AlvaroGzP
fierysolid
surfaace
surceis
SpaceWhale
klingan
reddivision
DayBr3ak
kbinani
mhdasding
MFizz
NamPNQ
z4ppy.bbc
matheussampaio
Abraxas000
lucasfevi
pokepal
Moonlight-Angel
mjmadsen
nikofil
bigkraig
nikhil-pandey
thebigjc
JaapMoolenaar
eevee-github
g0vanish
cmezh
Nivong
kestel
simonsmh
joaodragao
extink
Quantra
pmquan
net8q
SyncX
umbreon222
DeXtroTip
rawgni
Breeze Ro
bruno-kenji
Gobberwart
javajohnHub
kolinkorr839

Disclaimer
©2016 Niantic, Inc. ©2016 Pokémon. ©1995–2016 Nintendo / Creatures Inc. / GAME FREAK inc. © 2016 Pokémon/Nintendo Pokémon and Pokémon character names are trademarks of Nintendo. The Google Maps Pin is a trademark of Google Inc. and the trade dress in the product design is a trademark of Google Inc. under license to The Pokémon Company. Other trademarks are the property of their respective owners.
Privacy Policy
PokemonGo-Bot is intended for academic purposes and should not be used to play the game PokemonGo as it violates the TOS and is unfair to the community. Use the bot at your own risk.
PokemonGoF does not support the use of 3rd party apps or apps that violate the TOS.

",batch2,8:10:17,Done
204,gwsystems/wasmception-llvm,,batch1,16:57:58,Done
205,sathyanlm/k3,"Minikube




What is Minikube?
Minikube is a tool that makes it easy to run Kubernetes locally. Minikube runs a single-node Kubernetes cluster inside a VM on your laptop for users looking to try out Kubernetes or develop with it day-to-day.
Installation
macOS
brew cask install minikube
Linux
curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && chmod +x minikube && sudo cp minikube /usr/local/bin/ && rm minikube
Windows
Install with Chocolatey:
choco install minikube
Install manually: Download the minikube-windows-amd64.exe file, rename it to minikube.exe and add it to your path.
Linux Continuous Integration without VM Support
Example with kubectl installation:
curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && chmod +x minikube && sudo cp minikube /usr/local/bin/ && rm minikube
curl -Lo kubectl https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl && chmod +x kubectl && sudo cp kubectl /usr/local/bin/ && rm kubectl

export MINIKUBE_WANTUPDATENOTIFICATION=false
export MINIKUBE_WANTREPORTERRORPROMPT=false
export MINIKUBE_HOME=$HOME
export CHANGE_MINIKUBE_NONE_USER=true
mkdir -p $HOME/.kube
touch $HOME/.kube/config

export KUBECONFIG=$HOME/.kube/config
sudo -E minikube start --vm-driver=none

# this for loop waits until kubectl can access the api server that Minikube has created
for i in {1..150}; do # timeout for 5 minutes
   kubectl get po &> /dev/null
   if [ $? -ne 1 ]; then
      break
  fi
  sleep 2
done

# kubectl commands are now able to interact with Minikube cluster
Other Ways to Install

[Linux]

Arch Linux AUR
Fedora/CentOS/Red Hat COPR
openSUSE/SUSE Linux Enterprise


[Windows] Download the minikube-windows-amd64.exe file, rename it to minikube.exe and add it to your path.

Minikube Version Management
The asdf tool offers version management for a wide range of languages and tools. On macOS, asdf is available via Homebrew and can be installed with brew install asdf. Then, the Minikube plugin itself can be installed with asdf plugin-add minikube. A specific version of Minikube can be installed with asdf install minikube <version>. The tool allows you to switch versions for projects using a .tool-versions file inside the project. An asdf plugin exists for kubectl as well.
We also released a Debian package and Windows installer on our releases page. If you maintain a Minikube package, please feel free to add it here.
Requirements

kubectl
macOS

Hyperkit driver, xhyve driver, VirtualBox, or VMware Fusion


Linux

VirtualBox or KVM
NOTE: Minikube also supports a --vm-driver=none option that runs the Kubernetes components on the host and not in a VM. Docker is required to use this driver but no hypervisor. If you use --vm-driver=none, be sure to specify a bridge network for docker. Otherwise it might change between network restarts, causing loss of connectivity to your cluster.


Windows

VirtualBox or Hyper-V


VT-x/AMD-v virtualization must be enabled in BIOS
Internet connection on first run

Quickstart
Here's a brief demo of Minikube usage.
If you want to change the VM driver add the appropriate --vm-driver=xxx flag to minikube start. Minikube supports
the following drivers:

virtualbox
vmwarefusion
KVM2
KVM (deprecated in favor of KVM2)
hyperkit
xhyve
hyperv
none (Linux-only) - this driver can be used to run the Kubernetes cluster components on the host instead of in a VM. This can be useful for CI workloads which do not support nested virtualization.

$ minikube start
Starting local Kubernetes v1.7.5 cluster...
Starting VM...
SSH-ing files into VM...
Setting up certs...
Starting cluster components...
Connecting to cluster...
Setting up kubeconfig...
Kubectl is now configured to use the cluster.

$ kubectl run hello-minikube --image=k8s.gcr.io/echoserver:1.4 --port=8080
deployment ""hello-minikube"" created
$ kubectl expose deployment hello-minikube --type=NodePort
service ""hello-minikube"" exposed

# We have now launched an echoserver pod but we have to wait until the pod is up before curling/accessing it
# via the exposed service.
# To check whether the pod is up and running we can use the following:
$ kubectl get pod
NAME                              READY     STATUS              RESTARTS   AGE
hello-minikube-3383150820-vctvh   1/1       ContainerCreating   0          3s
# We can see that the pod is still being created from the ContainerCreating status
$ kubectl get pod
NAME                              READY     STATUS    RESTARTS   AGE
hello-minikube-3383150820-vctvh   1/1       Running   0          13s
# We can see that the pod is now Running and we will now be able to curl it:
$ curl $(minikube service hello-minikube --url)
CLIENT VALUES:
client_address=192.168.99.1
command=GET
real path=/
...
$ kubectl delete service hello-minikube
service ""hello-minikube"" deleted
$ kubectl delete deployment hello-minikube
deployment ""hello-minikube"" deleted
$ minikube stop
Stopping local Kubernetes cluster...
Machine stopped.
Interacting With Your Cluster
kubectl
The minikube start command creates a ""kubectl context"" called ""minikube"".
This context contains the configuration to communicate with your Minikube cluster.
Minikube sets this context to default automatically, but if you need to switch back to it in the future, run:
kubectl config use-context minikube,
or pass the context on each command like this: kubectl get pods --context=minikube.
Dashboard
To access the Kubernetes Dashboard, run this command in a shell after starting Minikube to get the address:
minikube dashboard
Services
To access a service exposed via a node port, run this command in a shell after starting Minikube to get the address:
minikube service [-n NAMESPACE] [--url] NAME
Design
Minikube uses libmachine for provisioning VMs, and kubeadm to provision a kubernetes cluster
For more information about Minikube, see the proposal.
Additional Links

Advanced Topics and Tutorials
Contributing
Development Guide

Community

#minikube on Kubernetes Slack
kubernetes-users mailing list 
(If you are posting to the list, please prefix your subject with ""minikube: "")

",batch2,8:10:18,Done
206,wzrdtales/tdb,,batch1,16:58:59,Done
207,earl/llvm-mirror,,batch1,16:57:58,Done
208,zillerium/sg1," 
NOTICE
This is a work in progress. It is not ready for release and I don't advise using it yet. It's future is also uncertain at this time.
Welcome to bitcoinj-sv
The bitcoinj-sv library is a Java implementation of the Bitcoin SV protocol. This library is a fork of Mike Hearn's original bitcoinj library aimed at supporting Bitcoin SV.
It allows maintaining a wallet and sending/receiving transactions without needing a full blockchain node.
Technologies

Java 8
Maven 3+ - for building the project
Google Protocol Buffers - for use with serialization and hardware communications

Getting started
To get started, it is best to have the latest JDK and Maven installed. The HEAD of the master branch contains the latest release and the dev branch contains development code.
Building from the command line
To perform a full build use
mvn clean package

The outputs are under the target directory.
Building from an IDE
Alternatively, just import the project using your IDE. IntelliJ has Maven integration built-in and has a free Community Edition. Simply use File | Import Project and locate the pom.xml in the root of the cloned project source tree.
Example applications
These are found in the examples module.
Contributing to bitcoinj-sv
Contributions to bitcoinj-sv are welcome and encouraged.

the development branch is dev
Travis-CI is here
Coveralls test coverage report is here

",batch2,8:09:16,Done
209,viktharien/RM,"
ranvier
Node.js-based MUD engine
Ranvier is a MUD game engine whose goal is to be a simple but powerful way to build whatever MUD you want with special care given to extensibility. The core code strives to be completely unopinionated toward any specific style of game while using the bundle system to build the game you want without having to dig through the engine's code.
Special Features

Robust bundle system: Nearly every aspect of the game can be modified without changing the core and allows for easy
packaging and sharing of commands/areas/items/npcs/channels/behaviors
Unopinionated network layer: easily swap out telnet for any network layer you like. No need to gut the whole codebase
just to support a different transport type, just drop in a file.
Customizable data layer: You are not tied to saving in any particular database or file storage sytem
Optional coordinate based room system allowing for the flexibilty of a standard MUD world with the easy mappability of
a strict 3D world.
Scripting for all entities in the game for any event along with behaviors to create shared, composable scripts
Skill system with passive/active skills
Effects e.g., buffs/debuffs
Quest system allowing for starting/progress/completion from any event in the game
Communication channels with custom audiences

Documentation
Ranvier prides itself on having thorough documentation which is available on our website: ranviermud.com
Slack
We have a Slack channel you can use to ask questions, suggest features, or just keep up to date with the project: https://ranviermud.slack.com
Get an invite
Requirements

Node.js >= v10.12.0

Demo
Point your favorite client or telnet to ranviermud.com port 4000. This demo server is wiped and updated from the master branch every hour.
",batch2,8:10:17,Done
210,stephanfeb/bitcoinj-sv," 
NOTICE
This is a work in progress. It is not ready for release and I don't advise using it yet. It's future is also uncertain at this time.
Welcome to bitcoinj-sv
The bitcoinj-sv library is a Java implementation of the Bitcoin SV protocol. This library is a fork of Mike Hearn's original bitcoinj library aimed at supporting Bitcoin SV.
It allows maintaining a wallet and sending/receiving transactions without needing a full blockchain node.
Technologies

Java 8
Maven 3+ - for building the project
Google Protocol Buffers - for use with serialization and hardware communications

Getting started
To get started, it is best to have the latest JDK and Maven installed. The HEAD of the master branch contains the latest release and the dev branch contains development code.
Building from the command line
To perform a full build use
mvn clean package

The outputs are under the target directory.
Building from an IDE
Alternatively, just import the project using your IDE. IntelliJ has Maven integration built-in and has a free Community Edition. Simply use File | Import Project and locate the pom.xml in the root of the cloned project source tree.
Example applications
These are found in the examples module.
Contributing to bitcoinj-sv
Contributions to bitcoinj-sv are welcome and encouraged.

the development branch is dev
Travis-CI is here
Coveralls test coverage report is here

",batch2,8:09:16,Done
211,pengqing1995/less,"        Chat with Less.js users

Less.js

The dynamic stylesheet language. http://lesscss.org.

This is the JavaScript, official, stable version of Less.
Getting Started
Options for adding Less.js to your project:

Install with npm: npm install less
Download the latest release
Clone the repo: git clone https://github.com/less/less.js.git

More information
For general information on the language, configuration options or usage visit lesscss.org.
Here are other resources for using Less.js:

stackoverflow.com is a great place to get answers about Less.
Less.js Issues for reporting bugs

Contributing
Please read CONTRIBUTING.md. Add unit tests for any new or changed functionality. Lint and test your code using Grunt.
Reporting Issues
Before opening any issue, please search for existing issues and read the Issue Guidelines, written by Nicolas Gallagher. After that if you find a bug or would like to make feature request, please open a new issue.
Please report documentation issues in the documentation project.
Development
Read Developing Less.
Release History
See the changelog
License
Copyright (c) 2009-2017 Alexis Sellier & The Core Less Team
Licensed under the Apache License.
",batch2,8:10:17,Done
212,Emmetttt/OT,"forgottenserver   
The Forgotten Server is a free and open-source MMORPG server emulator written in C++. It is a fork of the OpenTibia Server project. To connect to the server, you can use OTClient or OpenTibiaUnity.
Getting Started

Compiling, alternatively download AppVeyor builds for Windows
Scripting Reference

Support
If you need help, please visit the support forum on OTLand. Our issue tracker is not a support forum, and using it as one will result in your issue being closed. If you were unable to get assistance in the support forum, you should consider becoming a premium user on OTLand which grants you access to the premium support forum and supports OTLand financially.
Issues
We use the issue tracker on GitHub. Keep in mind that everyone who is watching the repository gets notified by e-mail when there is activity, so be thoughtful and avoid writing comments that aren't meaningful for an issue (e.g. ""+1""). If you'd like for an issue to be fixed faster, you should either fix it yourself and submit a pull request, or place a bounty on the issue.
",batch2,8:10:17,Done
213,jeffrizzo/openocd,,batch2,8:09:15,Done
214,fsheik/loungeIRC,"The Lounge
Modern web IRC client designed for self-hosting.





Overview

Modern features brought to IRC. Push notifications, link previews, new message markers, and more bring IRC to the 21st century.
Always connected. Remains connected to IRC servers while you are offline.
Cross platform. It doesn't matter what OS you use, it just works wherever Node.js runs.
Responsive interface. The client works smoothly on every desktop, smartphone and tablet.
Synchronized experience. Always resume where you left off no matter what device.

To learn more about configuration, usage and features of The Lounge, take a look at the website.



The Lounge is the official and community-managed fork of Shout, by Mattias Erming.
Installation and usage
The Lounge requires Node.js v4 or more recent.
Running stable releases from npm (recommended)
Run this in a terminal to install (or upgrade) the latest stable release from
npm:
[sudo] npm install -g thelounge
When installation is complete, run:
thelounge start
For more information, read the documentation, wiki, or run:
thelounge --help
Running from source
The following commands install and run the development version of The Lounge:
git clone https://github.com/thelounge/lounge.git
cd lounge
npm install
NODE_ENV=production npm run build
npm start
When installed like this, npm doesn't create a thelounge executable. Use npm start -- <command> to run subcommands.
⚠️ While it is the most recent codebase, this is not production-ready! Run at
your own risk. It is also not recommended to run this as root.
Development setup
Simply follow the instructions to run The Lounge from source above, on your own
fork.
Before submitting any change, make sure to:

Read the Contributing instructions
Run npm test to execute linters and test suite
Run npm run build if you change or add anything in client/js or client/views

",batch2,8:09:16,Done
215,g0v-data/aec,"aec-data
本處存放原能會所公佈各項資料的 JSON 版本。經由
https://github.com/g0v/aec-process/ 中的程式處理過後，自動發佈於此。
Data URL
每分鐘更新一次。注意：json 為 utf8 編碼，但 csv 皆為 big5 編碼。本地另存與來源相
同的 csv 用意在於對照，以遍日後檢驗問題。


最新幅射偵測: http://g0v-data.github.io/aec/latest/gammamonitor.json

來源網址: http://www.aec.gov.tw/open/spds.csv
本地對照: http://g0v-data.github.io/aec/latest/gammamonitor.json



最新核電廠機組狀態: http://g0v-data.github.io/aec/latest/spds.json

來源網址: http://www.aec.gov.tw/open/gammamonitor.csv
本地對照: http://g0v-data.github.io/aec/latest/spds.csv



",batch2,8:10:18,Done
216,marcelsavegnago/l10n-brazil-kmee,"Core da localização Brasileira do Odoo (novo OpenERP)


Escopo desse repo
Este projeto contêm os principais módulos da localização brasileira do Odoo, estes módulos são a base dos recursos:

Fiscais
Contábil
Sped

Sobre
Como a grande maioria dos módulos da OCA, esse projeto é open source sob licença AGPL v3.
A licença AGPL é derivada da licença GPL e acrescenta para os usuários a garantia de poder baixar o codigo desse projeto assim como dos módulos de extenções, mesmo quando o accesso a oferecido na nuvem. Vale a pena lembrar que a Odoo SA mudou da licença do core do OpenERP de GPL para AGPL durante 2009 e mudou de novo a licença do core para LGPL em 2015 enquanto maioria do ecosistema de modulos eram feitos sob licença AGPL e não poderiam mudar de licença mais devido à diversidade das contribuições.
Esse projeto segue se aperfeiçoando desde o início de 2009. O código era inicialmente desenvolvido no Launchpad. Esse projeto é gerenciado pela fundação OCA com a liderança do projeto:

steering committee Renato Lima AKRETION
core OCA reviewer Raphaël Valyi AKRETION

Esse projeto foi iniciado e principalmente desenvolvido pela AKRETION mas conta hoje com os outros contribuidores importantes:

Luis Mileo KMEE
Danimar Ribeiro TRUSTCODE
Fabio Negrini e varios outros contribuidores.

Contexto e extensões significativas desse projeto


odoo-brazil-eletronic-documents: os módulos do l10n-brazil definem a estrutura de dados das NFe's, porém a transmissão (exportação e importação) é plugável com tecnologias específicas. Hoje a forma mais madura de transmitir as NFe's é com a biblioteca pysped atraves do projeto odoo-brazil-eletronic-documents.
odoo-brazil-banking: a transmissão de boletos, CNAB e a importacão de extratos bancários usando os projetos OCA l10n-brazil e bank-statement-import.
odoo-brazil-sped: o projeto visando a implementar o SPED no Odoo.

Esses projetos ja estão sendo desenvolvidos de forma colaborativa com processos semelhantes aos da OCA (menos burocráticos e mais ágeis porem). A medida que eles amadurescem é provavel que eles ou parte deles integram a OCA tambem.
Esse projeto l10-brazil tambem depende do projeto account-fiscal-rules que foi extraido do l10n-brazil desde 2012 e integrado na OCA e esta hoje utilizado por outras localizões.
Fora do escopo desse repo

Extender ou modificar as funcionalidades do Odoo não vinculadas à localização brasileira. Outros módulos em outros projetos são perfeitos para isso.
De uma forma geral, reimplementar aqui o que ferramentas terceiras já fazem bem.
Quando há uma quantidade razoável de soluções técnicas para resolver um problema, o projeto do core da localização não quer impor uma dependência importante. Outros módulos e projetos são bem-vindos nesse caso. Esse projeto é por exemplo agnostico de tecnologia de transmissão das notas fiscais.
Implementar o PAF-ECF no PDV web do Odoo. Se trata de um trabalho muito burocrático que iria requerer modificar muito o PDV do Odoo, enquanto se conectar com PDV’s do mercado é uma alternativa razoável.
Manter dados para a folha de pagamento legal no Brasil.
Ter a responsabilidade de manter dados fiscais em geral. Em geral esse tipo de serviço requer uma responsabilidade jurídica que se negocia caso a caso.

Aviso importante
Apesar do código ser livremente disponível para baixar, implementar o Odoo de forma sustentável nao é algo fácil (a menos que seja apenas gestao de projeto ou CRM). E muito comum ver empresas achando que sabe mas que acaba desistindo a medida que descobre a dificuldade quando já é tarde demais. Um fator importante de dificuldade é a velocidade de evolução do core feito pela Odoo SA que nem sempre acontece de forma conectada com a comunidade. Isso obriga quem pretende implementar a trabalhar com branches em evolução em vez de pacotes estáveis por varios anos como em alguns outros projetos open source mais maduros. Por isso, é melhor você trabalhar com profisionais altamente especializados nisso (o aprendizado leva anos). O Odoo não foi projetado para o Brasil inicialmente, apenas tornamos isso possível com todo esses modulos. O Odoo pode também não ser tão maduro quanto se pretende. Isso não quer dizer que não serve. Serve sim, mas não para qualquer empresa e deve se observar muitos cuidados. Por fim, muitas vezes o valor aggregado vem mais da possibilidade de ter customizações do que das funcionalidades padrões; apesar de elas estarem sempre melhorando.
Contribuindo com o código
Você pode resolver umas das issues cadastradas no Github ou implementar melhorias em geral, nos enviando um pull request com as suas alterações.
Deve hoje se seguir os fluxos de trabalho padrão da OCA: https://odoo-community.org/page/Contribute
Se você for votado como core OCA reviewer pelos outros reviewers, você tera os direitos de commit no projeto. Você também pode ganhar esse direito só nesse projeto fazendo um esforço de contribuições no tempo comparável aos outros committers.
lista de discussão: https://groups.google.com/forum/#!forum/openerp-brasil
Available addons



addon
version
summary




l10n_br_base
10.0.1.0.0
Brazilian Localization Base


l10n_br_crm
10.0.1.0.0
Brazilian Localization CRM


l10n_br_crm_zip
10.0.1.0.0
Brazilian Localization CRM Zip


l10n_br_data_base
10.0.1.0.0
Brazilian Localisation Data Extension for Base


l10n_br_zip
10.0.1.0.0
Brazilian Localisation ZIP Codes


l10n_br_zip_correios
10.0.1.0.0
Address from Brazilian Localization ZIP by Correios



Unported addons



addon
version
summary




l10n_br_account
8.0.2.0.0 (unported)
Brazilian Localization Account


l10n_br_account_payment
7.0 (unported)
Brazilian Localization Account Payment


l10n_br_account_product
8.0.3.0.0 (unported)
Brazilian Localization Account Product


l10n_br_account_product_service
8.0.2.0.0 (unported)
Brazilian Localization Account Product and Service


l10n_br_account_service
8.0.2.0.0 (unported)
Brazilian Localization Account Service


l10n_br_data_account
8.0.1.0.1 (unported)
Brazilian Localisation Data Extension for Account


l10n_br_data_account_product
8.0.1.0.0 (unported)
Brazilian Localisation Data Extension for Product


l10n_br_data_account_service
8.0.1.0.0 (unported)
Brazilian Localization Data Account for Service


l10n_br_delivery
8.0.1.0.0 (unported)
Brazilian Localization Delivery


l10n_br_hr_timesheet_invoice
1.0 (unported)
Brazilian Invoice on Timesheets


l10n_br_purchase
8.0.1.0.0 (unported)
Brazilian Localization Purchase


l10n_br_sale
8.0.1.0.0 (unported)
Brazilian Localization Sale


l10n_br_sale_product
8.0.1.0.0 (unported)
Brazilian Localization Sale Product


l10n_br_sale_service
8.0.1.0.0 (unported)
Brazilian Localization Sale Service


l10n_br_sale_stock
8.0.1.0.0 (unported)
Brazilian Localization Sales and Warehouse


l10n_br_stock
8.0.1.0.0 (unported)
Brazilian Localization Warehouse


l10n_br_stock_account
8.0.1.0.1 (unported)
Brazilian Localization WMS Accounting


l10n_br_stock_account_report
8.0.1.0.0 (unported)
Brazilian Localization WMS Accounting Report



",batch2,8:09:16,Done
217,ARMmbed/mbed-os-tools,"Mbed OS tools
This repository contains the Python modules needed to work with Mbed OS. Historically, the Mbed OS tools have been delivered in separate packages and repositories. Their APIs have evolved separately over time, each with their own style and syntax. This project unifies these packages into a single intuitive API.
Packages
Below is a table showing what packages are delivered from this repository:



PyPI package
Source
Depends on
Stable API




mbed-os-tools (Documentation)
src/

No


mbed-ls (Documentation)
packages/mbed-ls/
mbed-os-tools
Yes


mbed-host-tests (Documentation)
packages/mbed-host-tests/
mbed-os-tools
Yes


mbed-greentea (Documentation)
packages/mbed-greentea/
mbed-os-tools
Yes



As indicated above, the API mbed-os-tools provides is not stable. For this reason, please continue to use the other packages.
License and contributions
The software is provided under Apache-2.0 license. Contributions to this project are accepted under the same license. Please see contributing.md for more info.
This project contains code from other projects. The original license text is included in those source files. They must comply with our license guide.
",batch2,8:10:17,Done
218,shinefield/mangoszeroscripts,"mangos-zero scripts
mangos-zero scripts are an Open Source GPL version 2 licensed
library for the mangos-zero server, providing unique scripts for
creatures, game objects, events and other game data which requires special
attention and can not be recreated by database.
Supporting the project
If you like mangos-zero scripts and want to support the project, there are
various options:

testing and reporting issues: if you use mangos-zero, please report issues
you find for the scripts.
submit fixes: any help is welcome! It does not matter if its' proof reading
and fixing messages printed by the server, or if you produce documentation
or code. As long as you have fun, be an active part of the project!

Documentation
Documentation is available at docs.getmangos.com, and is
updated on regular basis with the latest changes
Installation
To install mangos-zero on various target platforms, please checkout our
updated installation documentation.
Contributing
All information about contributing to mangos-zero can be found in our
documentation for contributors.
Bug Tracking
mangos-zero scripts uses the BitBucket issue tracker.
If you have found bugs, please create an issue.
License
mangos-zero scripts are licensed under the terms of the GNU GPL version 2.
",batch2,8:09:16,Done
219,Mirantis/libnetwork,"libnetwork - networking for containers
   
Libnetwork provides a native Go implementation for connecting containers
The goal of libnetwork is to deliver a robust Container Network Model that provides a consistent programming interface and the required network abstractions for applications.
Design
Please refer to the design for more information.
Using libnetwork
There are many networking solutions available to suit a broad range of use-cases. libnetwork uses a driver / plugin model to support all of these solutions while abstracting the complexity of the driver implementations by exposing a simple and consistent Network Model to users.
import (
	""fmt""
	""log""

	""github.com/docker/docker/pkg/reexec""
	""github.com/docker/libnetwork""
	""github.com/docker/libnetwork/config""
	""github.com/docker/libnetwork/netlabel""
	""github.com/docker/libnetwork/options""
)

func main() {
	if reexec.Init() {
		return
	}

	// Select and configure the network driver
	networkType := ""bridge""

	// Create a new controller instance
	driverOptions := options.Generic{}
	genericOption := make(map[string]interface{})
	genericOption[netlabel.GenericData] = driverOptions
	controller, err := libnetwork.New(config.OptionDriverConfig(networkType, genericOption))
	if err != nil {
		log.Fatalf(""libnetwork.New: %s"", err)
	}

	// Create a network for containers to join.
	// NewNetwork accepts Variadic optional arguments that libnetwork and Drivers can use.
	network, err := controller.NewNetwork(networkType, ""network1"", """")
	if err != nil {
		log.Fatalf(""controller.NewNetwork: %s"", err)
	}

	// For each new container: allocate IP and interfaces. The returned network
	// settings will be used for container infos (inspect and such), as well as
	// iptables rules for port publishing. This info is contained or accessible
	// from the returned endpoint.
	ep, err := network.CreateEndpoint(""Endpoint1"")
	if err != nil {
		log.Fatalf(""network.CreateEndpoint: %s"", err)
	}

	// Create the sandbox for the container.
	// NewSandbox accepts Variadic optional arguments which libnetwork can use.
	sbx, err := controller.NewSandbox(""container1"",
		libnetwork.OptionHostname(""test""),
		libnetwork.OptionDomainname(""docker.io""))
	if err != nil {
		log.Fatalf(""controller.NewSandbox: %s"", err)
	}

	// A sandbox can join the endpoint via the join api.
	err = ep.Join(sbx)
	if err != nil {
		log.Fatalf(""ep.Join: %s"", err)
	}

	// libnetwork client can check the endpoint's operational data via the Info() API
	epInfo, err := ep.DriverInfo()
	if err != nil {
		log.Fatalf(""ep.DriverInfo: %s"", err)
	}

	macAddress, ok := epInfo[netlabel.MacAddress]
	if !ok {
		log.Fatalf(""failed to get mac address from endpoint info"")
	}

	fmt.Printf(""Joined endpoint %s (%s) to sandbox %s (%s)\n"", ep.Name(), macAddress, sbx.ContainerID(), sbx.Key())
}
Future
Please refer to roadmap for more information.
Contributing
Want to hack on libnetwork? Docker's contributions guidelines apply.
Copyright and license
Code and documentation copyright 2015 Docker, inc. Code released under the Apache 2.0 license. Docs released under Creative commons.
",batch2,8:10:17,Done
220,wouf/hj191,,batch1,16:57:58,Done
221,WebmecanikDev/mautic-docs,"Mautic Documentation
Introduction
This book serves as the documentation for Mautic, the open source marketing automation system. Just as the code is open source and available for everyone, so is the documentation. Everyone is welcome to help make this information better and improve as needed.
Download as PDF!
Click here to download these docs as a PDF in English.
Click here to download these docs as a PDF in French.
Click here to download these docs as a PDF in Japanese.
How to contribute to the docs
This repository is the source code for Gitbook published at www.mautic.org/docs. The source code is shared here on GitHub so anyone can contribute to the documentation in the same way the programmers do with the actual Mautic code.
Why is git used for the documentation

versions. Anyone can go back and look at what the text looked like.
authorship. Not only every file, but every line has its author.
community contributions. No need to worry about deleting someone else's work while working on the same document.

Although some git knowledge is required to clone, modify, commit and push changes, there is a way to avoid that and edit the files directly in the GitHub web interface. If you know git, use the workflow you like. If not, the following guide will show you how to contribute easily.
Edit documents in a browser

Fork this repository under your account so you'll have permission to edit.
Select a file to edit. The file structure is explained below. Now, let's edit the README.md file to show the principles. Click on it.
The content of README.md should be visible and the Edit button (the pencil icon) above as well. Hit it.
The content is written in Markdown markup. Very simple text based formatting.
Make a change to the file. For example add to the end This is my first contribution.
When you have made a change, scroll down and notice the form called Commit changes. This is important. To save a change, you have to describe what you've changed and why. Write for example A new line added for testing purposes. Do not save yet!
Because the GitHub web interface does not provide all features of git, we won't have an easy way to revert our change back to the original state. We'd have to create another commit where we'd delete the added line. That would make a mess in the commit history. So instead, we create a new branch. There is a checkbox for it ""Create a new branch..."". The branch has to have a name. {yourusername}-patch-1 will be prefilled. Let's change it to {yourusername}-testing. Click the Propose file change button now.
Ok, so the change exists in your repository now. To propose the change to the official repository, you have to send a pull request (PR). You've been redirected to do just that. Here you describe your proposed change and click (please don't send the testing PRs) the Create pull request button.

If you want to clean up after the testing, go to the Branches section and delete the testing branch.
The file structure
We've worked with the README.md file in the previous section. This file is shown in the home page of a GitHub repository and you are reading its content right now. It doesn't have anything to do with the Mautic documentation.
The SUMMARY.md file defines the menu of the documentation. If you add a new page to the documentation, you'll have to also add a new line there defining the title and the link to the file. It's pretty straightforward when you see the current menu items.
The folders are here to group the topics together. Open for example the asset folder. You'll see it has its own README.md file. It is the main content when you click on the Asset menu item. The manage_assets.md file is a subitem. The media subfolder contains all the images used in the md files.
Links
Often you'll want to make a link into another place in the documentation. In Markdown, the link looks like this:
[link title](http://example.com)

This will create an external link with absolute URL. If you want to create an internal link, use a relative URL like this:
[these steps](./../plugins/integration_test.html)

This will link to plugins/integration_test.html on the documentation website created from the md source file.
Images
As noted above, the images can be placed in the media subfolders. The images probably aren't possible to upload via the GitHub web interface, but you can upload them to any public URL and link them from there.
![alternative text here](http://example.com/images/apple.png ""Tooltip text here"")

Or, if you want to display an image already uploaded to the documentation repository, you can use a relative path:
![alternative text here](/assets/media/assets-newcategory.png ""Tooltip text here"")

",batch2,8:10:16,Done
222,unicef/etools-partnership-management,"
Etools Frontend Template App
This an app shell for Unicef eTools apps, a starting point based on Polymer 3, Redux and Typescript.
Install

requirements: node, npm, polymer-cli, typescript, gulp
npm install
npm run start

Check package.json scripts for more...
TODO:

Update page header element to use etools-app-selector, countries dropdown, profile menu and refresh data button
Improve documentation
Update tests
Test build

",batch2,8:09:15,Done
223,zubieta/homebrew-core,"Homebrew Core
Core formulae for the Homebrew package manager.
Core formulae for the Linuxbrew package manager (Homebrew on Linux or Windows 10 Subsystem for Linux) are in Homebrew/linuxbrew-core.

How do I install these formulae?
Just brew install <formula>. This is the default tap for Homebrew and is installed by default.
More Documentation, Troubleshooting, Contributing, Security, Community, Donations, License and Sponsors
See these sections in Homebrew/brew's README.
",batch1,16:57:59,Done
224,smaslennikov/whattimeisitrightmeow,"What time is it right meow?
You know you want to find out.
Who needs ntp when you have this?
Status
Dead. GitHub started noticeably lagging on the repo page, and even my slowpoke of a VPS started to struggle pushing the changes in less than thirty seconds. A solution is to bump my sleep limit, but more importantly, GitHub itself is mad. It was a good run!
Usage
Fork me, eat me, put me in a tree
Run ./bin/time.sh forever
Watch your own personalized time!
Do I use this?
Hell yes I do, in prod. Someone tell him to merge it already!
Contributing
Only if you really want to.
Historical nonsense
I woke up one morning after seeing the traeish of Bojack Horseman and thought:
""what's the worst possible way I can show time on the internet?""

Behold.
License
MIT license it is.
",batch1,16:57:57,Done
225,fpiper/emacs,,batch1,16:58:59,Done
226,CloudNativeMySQL/mariadb-server,,batch1,16:57:59,Done
227,dbuskariol-org/chromium," Chromium
Chromium is an open-source browser project that aims to build a safer, faster,
and more stable way for all users to experience the web.
The project's web site is https://www.chromium.org.
Documentation in the source is rooted in docs/README.md.
Learn how to Get Around the Chromium Source Code Directory Structure
.
For historical reasons, there are some small top level directories. Now the
guidance is that new top level directories are for product (e.g. Chrome,
Android WebView, Ash). Even if these products have multiple executables, the
code should be in subdirectories of the product.
",batch1,16:57:58,Done
228,altera2015/llvm-c74,,batch1,16:57:58,Done
229,clChenLiang/temp-vue-sc,"










Supporting Vue.js
Vue.js is an MIT-licensed open source project with its ongoing development made possible entirely by the support of these awesome backers. If you'd like to join them, please consider:

Become a backer or sponsor on Patreon.
Become a backer or sponsor on Open Collective.
One-time donation via PayPal or crypto-currencies.

What's the difference between Patreon and OpenCollective?
Funds donated via Patreon go directly to support Evan You's full-time work on Vue.js. Funds donated via OpenCollective are managed with transparent expenses and will be used for compensating work and expenses for core team members or sponsoring community events. Your name/logo will receive proper recognition and exposure by donating on either platform.
Special Sponsors





Platinum Sponsors

































Platinum Sponsors (China)
















Gold Sponsors
















































































































Sponsors via Open Collective
Platinum


Gold






Introduction
Vue (pronounced /vjuː/, like view) is a progressive framework for building user interfaces. It is designed from the ground up to be incrementally adoptable, and can easily scale between a library and a framework depending on different use cases. It consists of an approachable core library that focuses on the view layer only, and an ecosystem of supporting libraries that helps you tackle complexity in large Single-Page Applications.
Browser Compatibility
Vue.js supports all browsers that are ES5-compliant (IE8 and below are not supported).
Ecosystem



Project
Status
Description




vue-router

Single-page application routing


vuex

Large-scale state management


vue-cli

Project scaffolding


vue-loader

Single File Component (*.vue file) loader for webpack


vue-server-renderer

Server-side rendering support


vue-class-component

TypeScript decorator for a class-based API


vue-rx

RxJS integration


vue-devtools

Browser DevTools extension



Documentation
To check out live examples and docs, visit vuejs.org.
Questions
For questions and support please use the official forum or community chat. The issue list of this repo is exclusively for bug reports and feature requests.
Issues
Please make sure to read the Issue Reporting Checklist before opening an issue. Issues not conforming to the guidelines may be closed immediately.
Changelog
Detailed changes for each release are documented in the release notes.
Stay In Touch

Twitter
Blog
Job Board

Contribution
Please make sure to read the Contributing Guide before making a pull request. If you have a Vue-related project/component/tool, add it with a pull request to this curated list!
Thank you to all the people who already contributed to Vue!

License
MIT
Copyright (c) 2013-present, Yuxi (Evan) You
",batch2,8:10:17,Done
230,favorinfo/ghost,"



Ghost 目由非盈利性组织 Ghost Foundation 和一群优秀的独立贡献者共同维护。我们正在尽最大努力让在线内容创作变得更好。

Ghost 官网（英文） & Ghost 中国
最新版本（英文） & 最新版本（中文）
技术支持（英文） & 问答社区（中文）
主题文档
贡献指南
心愿列表
技术博客

注意：如果你在使用 Ghost 过程中遇到难题需要帮助，请尽量加入 Slack 社区 寻求帮助而不是在 Github 上新开一个 issue。
快速安装
安装前请确保已经安装了 Node.js - 我们建议使用 Node v0.10.x 的最新版本。
Ghost 同时也支持 Node v0.12 和 io.js v1.2 ，但是请注意，这些版本很有可能导致安装失败。如果遇到问题，请到论坛寻求帮助。

下载 最新版本 的 Ghost
解压文件至你所希望的安装位置
启动一个命令行窗口
执行 npm install --production 命令
启动 Ghost

本地环境：npm start
生产环境：npm start --production


启动浏览器，打开 http://localhost:2368/ghost 链接

还可以参考详细的安装指南 。
安装中文版
Node.js 是必须的，同样也是建议使用  Node v0.10.x 的最新版本。如果你使用的是 Node v0.12 或 io.js v1.2 版本，请小心！遇到问题可以到 问答社区 讨论。

下载最新的 Ghost 中文版 。建议下载集成安装包，大概20M左右。
解压所有文件到你所希望的安装目录
启动一个命令行窗口
如果你下载的不是集成安装包（也就是没有 node_modules 目录），清闲执行 npm install --production 命令
启动 Ghost

本地环境：npm start
生产环境：npm start --production


启动浏览器，打开 http://localhost:2368/ghost 链接


开发者(从 git 下载 Ghost)
安装 Node.js. （请参考 受支持的 Node.js 版本）
# Node v0.10.x - 推荐
# Node v0.12.x 和 v4.2+ LTS - 已支持
#
# 自行斟酌吧
克隆 👻
git clone git://github.com/tryghost/ghost.git
cd ghost
安装 grunt。
npm install -g grunt-cli
安装 Ghost。 如果你是在本地环境运行 ghost，可以使用 master 分支。如果是在生产环境运行，请使用 stable 分支。 🚫🚀🔬
npm install
编译！
grunt init
为生产环境压缩各种文件。
grunt prod
启动博客。
npm start

##  让 Ghost 在生产环境中运行请添加 --production 参数
祝贺你，一切搞定了！顺便说一下，你还可以直接执行 npm install ghost 指令将 Ghost 作为 npm 包来使用。将 Ghost 作为 NPM 模块来使用 是一份很详尽的文档。
还可以参考更详细的安装指南 。
部署 Ghost

Ghost 官方支持的 Ghost(Pro) 服务能够帮你节约大量时间，只需点几下鼠标就能部署一个 Ghost 实例到 DigitalOcean 的服务器上，并且还可以免费享受到全球化的 CDN 服务。
从 Ghost(Pro) 所获得的所有收益都将用于 Ghost 基金 -- 一个非营利性的组织，为 Ghost 的开发和维护提供支持。
如果你希望自己部署 Ghost，可以参考这里 。
如果你使用的是阿里云主机，还可以参考我们撰写的系列文章，按照文中指引一步步安装 Ghost 以及依赖的各个组件。
保持更新
当 Ghost 有新版本发布时，请参考 升级指南 以了解如何升级 Ghost。
你可以加入 问答社区（中文） 和其他 Ghost 用户交流，或者在 public Slack team 与 Ghost 开发者沟通。我们每周二下午 5:30 都会在 Slack 上开碰头会。请注意，我们说的是伦敦时间。
每次有新版本都会在 技术博客 上公布。你可以通过邮件订阅或者在 Twitter 上关注 @TryGhost_Dev。
🎷🐢
版权 & 协议
Copyright (c) 2013-2016 Ghost Foundation - Released under the MIT license.
中文版本及插件
Copyright (c) 2013-2016 Ghost 中国/中文网 - 采用 MIT 许可协议 发布。
",batch2,8:09:16,Done
231,rburchell/ck,,batch2,8:09:16,Done
232,BusFactor1Inc/ELPE,"GIT CLONE THIS RESPOSITORY WITH --RECURSIVE:
git clone --recursive https://github.com/BusFactor1Inc/ELPE

==
Emacs Lisp Programming Environment
To build run ./doconf-25.sh and then ./build.sh.
Changelog
Initial configuration.
--
BusFactor1 Inc. 2017
",batch1,16:59:00,Done
233,Rentu/redis-read,"redis 阅读笔记

基于5.0

CMakeLists.txt 已全部添加，一些报错也已解决，理论上下载下来即可运行，如不可以，请去官方 github 下载，重新编译
编译步骤见 https://rentu.github.io/2020/08/07/redis-%E6%BA%90%E7%A0%81-%E7%BC%96%E8%AF%91/

添加一些中文注释和自己的理解

",batch2,8:09:15,Done
234,arifakhriwilaga/masskredit,"MassCredit
About MassCredit
MassCredit is built with angular2 and webpack.
url web here ..
Configuration
Configuration here ..
Requirment

Node Version : 5.0
NPM Version : 3

Installation
Clone Project:
$ git clone git@git.wgs.co.id:kredit-plus/mass-credit-frontend.git
$ cd mass-credit-frontend
$ npm install
Running Project:
$ npm start
Built production:
$ npm run build:prod
Deployment
Staging:
$ command example
Production:
$ command example
Plugins
MassCredit is .....



Plugin
README




example
[plugins/dropbox/README.md] [PlDb]



Verify the deployment by navigating to your server address in your preferred browser.
127.0.0.1:3009
Todos

text here
text here

License
Example
",batch2,8:10:18,Done
235,zxware/forgottenserver,"forgottenserver
The Forgotten Server is a free and open-source MMORPG server emulator written in C++. It is a fork of the OpenTibia Server project. To connect to the server, you can use OTClient or OpenTibiaUnity.
Compiling
$ apt-get install git cmake build-essential liblua5.2-dev libgmp3-dev libmariadb-dev-compat libboost-filesystem-dev libboost-system-dev libboost-iostreams-dev libpugixml-dev libcrypto++-dev
$ mkdir build && cd build
$ cmake ..
$ make -j4
",batch2,8:10:17,Done
236,IntTeam-24hcode/BoubouGUI,"Blockly 
Google's Blockly is a web-based, visual programming editor.  Users can drag
blocks together to build programs.  All code is free and open source.
The project page is https://developers.google.com/blockly/

Blockly has an active developer forum. Please drop by and say hello. Show us your prototypes early; collectively we have a lot of experience and can offer hints which will save you time.
Help us focus our development efforts by telling us what you are doing with
Blockly. The questionnaire only takes
a few minutes and will help us better support the Blockly community.
Want to contribute? Great! First, read our guidelines for contributors.
",batch2,8:09:16,Done
237,T-J-Teru/emacs,,batch1,16:59:00,Done
238,ryangopher/emacs-mac,,batch1,16:59:01,Done
239,FoOtY/ClassyWoW,,batch2,8:10:18,Done
240,wangyanjunmsn/tinyusb,"TinyUSB

  
TinyUSB is an open-source cross-platform USB Host/Device stack for embedded system, designed to be memory-safe with no dynamic allocation and thread-safe with all interrupt events are deferred then handled in the non-ISR task function.

.
├── docs            # Documentation
├── examples        # Sample with Makefile and Segger Embedded build support
├── hw
│   ├── bsp         # Supported boards source files
│   └── mcu         # Low level mcu core & peripheral drivers
├── lib             # Sources from 3rd party such as freeRTOS, fatfs ...
├── src             # All sources files for TinyUSB stack itself.
├── test            # Unit tests for the stack
└── tools           # Files used internally

Contributors
Special thanks to all the people who spent their precious time and effort to help this project so far. Check out the
CONTRIBUTORS.md file for the list of all contributors and their awesome work for the stack.
Supported MCUs
The stack supports the following MCUs:

Espressif: ESP32-S2
MicroChip: SAMD21, SAMD51 (device only)
NordicSemi: nRF52833, nRF52840
Nuvoton: NUC120, NUC121/NUC125, NUC126, NUC505
NXP:

LPC Series: 11Uxx, 13xx, 175x_6x, 177x_8x, 18xx, 40xx, 43xx, 51Uxx, 54xxx, 55xx
iMX RT Series: RT1011, RT1015, RT1021, RT1052, RT1062, RT1064


Sony: CXD56
ST: STM32 series: L0, F0, F1, F2, F3, F4, F7, H7 (device only)
TI: MSP430
ValentyUSB eptri

Here is the list of supported Boards that can be used with provided examples.
Device Stack
Supports multiple device configurations by dynamically changing usb descriptors. Low power functions such like suspend, resume, and remote wakeup. Following device classes are supported:

Communication Class (CDC)
Human Interface Device (HID): Generic (In & Out), Keyboard, Mouse, Gamepad etc ...
Mass Storage Class (MSC): with multiple LUNs
Musical Instrument Digital Interface (MIDI)
Network with RNDIS, CDC-ECM (work in progress)
USB Test and Measurement Class (USBTMC)
Vendor-specific class support with generic In & Out endpoints. Can be used with MS OS 2.0 compatible descriptor to load winUSB driver without INF file.
WebUSB with vendor-specific class

Host Stack
Most active development is on the Device stack. The Host stack is under rework and largely untested.

Human Interface Device (HID): Keyboard, Mouse, Generic
Mass Storage Class (MSC)
Hub currently only supports 1 level of hub (due to my laziness)

OS Abstraction layer
TinyUSB is completely thread-safe by pushing all ISR events into a central queue, then process it later in the non-ISR context task function. It also uses semaphore/mutex to access shared resources such as CDC FIFO. Therefore the stack needs to use some of OS's basic APIs. Following OSes are already supported out of the box.

No OS : Disabling USB IRQ is used as way to provide mutex
FreeRTOS
Mynewt Due to the newt package build system, Mynewt examples are better to be on its own repo

Getting Started
Here are the details for getting started with the stack.
Porting
Want to help add TinyUSB support for a new MCU? Read here for an explanation on the low-level API needed by TinyUSB.
License
MIT license for all TinyUSB sources src folder, Full license is here. However, each file is individually licensed especially those in lib and hw/mcu folder. Please make sure you understand all the license term for files you use in your project.
Uses
TinyUSB is currently used by these other projects:

Adafruit nRF52 Arduino
Adafruit nRF52 Bootloader
Adafruit SAMD Arduino
CircuitPython
MicroPython
TinyUSB Arduino Library

Let me know if your project also uses TinyUSB and want to share.
",batch2,8:10:17,Done
241,ABaldwinHunter/sinatra-classic,"Sinatra
Sinatra is a DSL for
quickly creating web applications in Ruby with minimal effort:
# myapp.rb
require 'sinatra'

get '/' do
  'Hello world!'
end
Install the gem:
gem install sinatra
And run with:
ruby myapp.rb
View at: http://localhost:4567
It is recommended to also run gem install thin, which Sinatra will
pick up if available.
Table of Contents

Sinatra

Table of Contents
Routes
Conditions
Return Values
Custom Route Matchers
Static Files
Views / Templates

Literal Templates
Available Template Languages

Haml Templates
Erb Templates
Builder Templates
Nokogiri Templates
Sass Templates
SCSS Templates
Less Templates
Liquid Templates
Markdown Templates
Textile Templates
RDoc Templates
AsciiDoc Templates
Radius Templates
Markaby Templates
RABL Templates
Slim Templates
Creole Templates
MediaWiki Templates
CoffeeScript Templates
Stylus Templates
Yajl Templates
WLang Templates


Accessing Variables in Templates
Templates with yield and nested layouts
Inline Templates
Named Templates
Associating File Extensions
Adding Your Own Template Engine
Using Custom Logic for Template Lookup


Filters
Helpers

Using Sessions
Halting
Passing
Triggering Another Route
Setting Body, Status Code and Headers
Streaming Responses
Logging
Mime Types
Generating URLs
Browser Redirect
Cache Control
Sending Files
Accessing the Request Object
Attachments
Dealing with Date and Time
Looking Up Template Files


Configuration

Configuring attack protection
Available Settings


Environments
Error Handling

Not Found
Error


Rack Middleware
Testing
Sinatra::Base - Middleware, Libraries, and Modular Apps

Modular vs. Classic Style
Serving a Modular Application
Using a Classic Style Application with a config.ru
When to use a config.ru?
Using Sinatra as Middleware
Dynamic Application Creation


Scopes and Binding

Application/Class Scope
Request/Instance Scope
Delegation Scope


Command Line

Multi-threading


Requirement
The Bleeding Edge

With Bundler
Roll Your Own
Install Globally


Versioning
Further Reading



Routes
In Sinatra, a route is an HTTP method paired with a URL-matching pattern.
Each route is associated with a block:
get '/' do
  .. show something ..
end

post '/' do
  .. create something ..
end

put '/' do
  .. replace something ..
end

patch '/' do
  .. modify something ..
end

delete '/' do
  .. annihilate something ..
end

options '/' do
  .. appease something ..
end

link '/' do
  .. affiliate something ..
end

unlink '/' do
  .. separate something ..
end
Routes are matched in the order they are defined. The first route that
matches the request is invoked.
Route patterns may include named parameters, accessible via the
params hash:
get '/hello/:name' do
  # matches ""GET /hello/foo"" and ""GET /hello/bar""
  # params['name'] is 'foo' or 'bar'
  ""Hello #{params['name']}!""
end
You can also access named parameters via block parameters:
get '/hello/:name' do |n|
  # matches ""GET /hello/foo"" and ""GET /hello/bar""
  # params['name'] is 'foo' or 'bar'
  # n stores params['name']
  ""Hello #{n}!""
end
Route patterns may also include splat (or wildcard) parameters, accessible
via the params['splat'] array:
get '/say/*/to/*' do
  # matches /say/hello/to/world
  params['splat'] # => [""hello"", ""world""]
end

get '/download/*.*' do
  # matches /download/path/to/file.xml
  params['splat'] # => [""path/to/file"", ""xml""]
end
Or with block parameters:
get '/download/*.*' do |path, ext|
  [path, ext] # => [""path/to/file"", ""xml""]
end
Route matching with Regular Expressions:
get /\A\/hello\/([\w]+)\z/ do
  ""Hello, #{params['captures'].first}!""
end
Or with a block parameter:
get %r{/hello/([\w]+)} do |c|
  # Matches ""GET /meta/hello/world"", ""GET /hello/world/1234"" etc.
  ""Hello, #{c}!""
end
Route patterns may have optional parameters:
get '/posts.?:format?' do
  # matches ""GET /posts"" and any extension ""GET /posts.json"", ""GET /posts.xml"" etc.
end
Routes may also utilize query parameters:
get '/posts' do
  # matches ""GET /posts?title=foo&author=bar""
  title = params['title']
  author = params['author']
  # uses title and author variables; query is optional to the /posts route
end
By the way, unless you disable the path traversal attack protection (see below),
the request path might be modified before matching against your routes.
Conditions
Routes may include a variety of matching conditions, such as the user agent:
get '/foo', :agent => /Songbird (\d\.\d)[\d\/]*?/ do
  ""You're using Songbird version #{params['agent'][0]}""
end

get '/foo' do
  # Matches non-songbird browsers
end
Other available conditions are host_name and provides:
get '/', :host_name => /^admin\./ do
  ""Admin Area, Access denied!""
end

get '/', :provides => 'html' do
  haml :index
end

get '/', :provides => ['rss', 'atom', 'xml'] do
  builder :feed
end
provides searches the request's Accept header.
You can easily define your own conditions:
set(:probability) { |value| condition { rand <= value } }

get '/win_a_car', :probability => 0.1 do
  ""You won!""
end

get '/win_a_car' do
  ""Sorry, you lost.""
end
For a condition that takes multiple values use a splat:
set(:auth) do |*roles|   # <- notice the splat here
  condition do
    unless logged_in? && roles.any? {|role| current_user.in_role? role }
      redirect ""/login/"", 303
    end
  end
end

get ""/my/account/"", :auth => [:user, :admin] do
  ""Your Account Details""
end

get ""/only/admin/"", :auth => :admin do
  ""Only admins are allowed here!""
end
Return Values
The return value of a route block determines at least the response body passed
on to the HTTP client, or at least the next middleware in the Rack stack.
Most commonly, this is a string, as in the above examples. But other values are
also accepted.
You can return any object that would either be a valid Rack response, Rack
body object or HTTP status code:

An Array with three elements: [status (Fixnum), headers (Hash), response body (responds to #each)]
An Array with two elements: [status (Fixnum), response body (responds to #each)]
An object that responds to #each and passes nothing but strings to
the given block
A Fixnum representing the status code

That way we can, for instance, easily implement a streaming example:
class Stream
  def each
    100.times { |i| yield ""#{i}\n"" }
  end
end

get('/') { Stream.new }
You can also use the stream helper method (described below) to reduce boiler
plate and embed the streaming logic in the route.
Custom Route Matchers
As shown above, Sinatra ships with built-in support for using String patterns
and regular expressions as route matches. However, it does not stop there. You
can easily define your own matchers:
class AllButPattern
  Match = Struct.new(:captures)

  def initialize(except)
    @except   = except
    @captures = Match.new([])
  end

  def match(str)
    @captures unless @except === str
  end
end

def all_but(pattern)
  AllButPattern.new(pattern)
end

get all_but(""/index"") do
  # ...
end
Note that the above example might be over-engineered, as it can also be
expressed as:
get // do
  pass if request.path_info == ""/index""
  # ...
end
Or, using negative look ahead:
get %r{^(?!/index$)} do
  # ...
end
Static Files
Static files are served from the ./public directory. You can specify
a different location by setting the :public_folder option:
set :public_folder, File.dirname(__FILE__) + '/static'
Note that the public directory name is not included in the URL. A file
./public/css/style.css is made available as
http://example.com/css/style.css.
Use the :static_cache_control setting (see below) to add
Cache-Control header info.
Views / Templates
Each template language is exposed via its own rendering method. These
methods simply return a string:
get '/' do
  erb :index
end
This renders views/index.erb.
Instead of a template name, you can also just pass in the template content
directly:
get '/' do
  code = ""<%= Time.now %>""
  erb code
end
Templates take a second argument, the options hash:
get '/' do
  erb :index, :layout => :post
end
This will render views/index.erb embedded in the
views/post.erb (default is views/layout.erb, if it exists).
Any options not understood by Sinatra will be passed on to the template
engine:
get '/' do
  haml :index, :format => :html5
end
You can also set options per template language in general:
set :haml, :format => :html5

get '/' do
  haml :index
end
Options passed to the render method override options set via set.
Available Options:

locals

    List of locals passed to the document. Handy with partials.
    Example: erb ""<%= foo %>"", :locals => {:foo => ""bar""}

default_encoding

    String encoding to use if uncertain. Defaults to
    settings.default_encoding.
  
views

    Views folder to load templates from. Defaults to settings.views.
  
layout

    Whether to use a layout (true or false). If it's a
    Symbol, specifies what template to use. Example:
    erb :index, :layout => !request.xhr?

content_type

    Content-Type the template produces. Default depends on template language.
  
scope

    Scope to render template under. Defaults to the application instance. If you
    change this, instance variables and helper methods will not be available.
  
layout_engine

    Template engine to use for rendering the layout. Useful for languages that
    do not support layouts otherwise. Defaults to the engine used for the
    template. Example: set :rdoc, :layout_engine => :erb

layout_options

    Special options only used for rendering the layout. Example:
    set :rdoc, :layout_options => { :views => 'views/layouts' }


Templates are assumed to be located directly under the ./views directory. To
use a different views directory:
set :views, settings.root + '/templates'
One important thing to remember is that you always have to reference templates
with symbols, even if they're in a subdirectory (in this case, use:
:'subdir/template' or 'subdir/template'.to_sym). You must use a symbol
because otherwise rendering methods will render any strings passed to them
directly.
Literal Templates
get '/' do
  haml '%div.title Hello World'
end
Renders the template string.
Available Template Languages
Some languages have multiple implementations. To specify what implementation
to use (and to be thread-safe), you should simply require it first:
require 'rdiscount' # or require 'bluecloth'
get('/') { markdown :index }
Haml Templates


Dependency
haml


File Extension
.haml


Example
haml :index, :format => :html5


Erb Templates


Dependency

erubis
      or erb (included in Ruby)
    


File Extensions
.erb, .rhtml or .erubis (Erubis only)


Example
erb :index


Builder Templates


Dependency

builder



File Extension
.builder


Example
builder { |xml| xml.em ""hi"" }


It also takes a block for inline templates (see example).
Nokogiri Templates


Dependency
nokogiri


File Extension
.nokogiri


Example
nokogiri { |xml| xml.em ""hi"" }


It also takes a block for inline templates (see example).
Sass Templates


Dependency
sass


File Extension
.sass


Example
sass :stylesheet, :style => :expanded


SCSS Templates


Dependency
sass


File Extension
.scss


Example
scss :stylesheet, :style => :expanded


Less Templates


Dependency
less


File Extension
.less


Example
less :stylesheet


Liquid Templates


Dependency
liquid


File Extension
.liquid


Example
liquid :index, :locals => { :key => 'value' }


Since you cannot call Ruby methods (except for yield) from a Liquid
template, you almost always want to pass locals to it.
Markdown Templates


Dependency

      Anyone of:
        RDiscount,
        RedCarpet,
        BlueCloth,
        kramdown,
        maruku



File Extensions
.markdown, .mkd and .md


Example
markdown :index, :layout_engine => :erb


It is not possible to call methods from markdown, nor to pass locals to it.
You therefore will usually use it in combination with another rendering
engine:
erb :overview, :locals => { :text => markdown(:introduction) }
Note that you may also call the markdown method from within other templates:
%h1 Hello From Haml!
%p= markdown(:greetings)
Since you cannot call Ruby from Markdown, you cannot use layouts written in
Markdown. However, it is possible to use another rendering engine for the
template than for the layout by passing the :layout_engine option.
Textile Templates


Dependency
RedCloth


File Extension
.textile


Example
textile :index, :layout_engine => :erb


It is not possible to call methods from textile, nor to pass locals to it. You
therefore will usually use it in combination with another rendering engine:
erb :overview, :locals => { :text => textile(:introduction) }
Note that you may also call the textile method from within other templates:
%h1 Hello From Haml!
%p= textile(:greetings)
Since you cannot call Ruby from Textile, you cannot use layouts written in
Textile. However, it is possible to use another rendering engine for the
template than for the layout by passing the :layout_engine option.
RDoc Templates


Dependency
RDoc


File Extension
.rdoc


Example
rdoc :README, :layout_engine => :erb


It is not possible to call methods from rdoc, nor to pass locals to it. You
therefore will usually use it in combination with another rendering engine:
erb :overview, :locals => { :text => rdoc(:introduction) }
Note that you may also call the rdoc method from within other templates:
%h1 Hello From Haml!
%p= rdoc(:greetings)
Since you cannot call Ruby from RDoc, you cannot use layouts written in
RDoc. However, it is possible to use another rendering engine for the
template than for the layout by passing the :layout_engine option.
AsciiDoc Templates


Dependency
Asciidoctor


File Extension
.asciidoc, .adoc and .ad


Example
asciidoc :README, :layout_engine => :erb


Since you cannot call Ruby methods directly from an AsciiDoc template, you
almost always want to pass locals to it.
Radius Templates


Dependency
Radius


File Extension
.radius


Example
radius :index, :locals => { :key => 'value' }


Since you cannot call Ruby methods directly from a Radius template, you almost
always want to pass locals to it.
Markaby Templates


Dependency
Markaby


File Extension
.mab


Example
markaby { h1 ""Welcome!"" }


It also takes a block for inline templates (see example).
RABL Templates


Dependency
Rabl


File Extension
.rabl


Example
rabl :index


Slim Templates


Dependency
Slim Lang


File Extension
.slim


Example
slim :index


Creole Templates


Dependency
Creole


File Extension
.creole


Example
creole :wiki, :layout_engine => :erb


It is not possible to call methods from creole, nor to pass locals to it. You
therefore will usually use it in combination with another rendering engine:
erb :overview, :locals => { :text => creole(:introduction) }
Note that you may also call the creole method from within other templates:
%h1 Hello From Haml!
%p= creole(:greetings)
Since you cannot call Ruby from Creole, you cannot use layouts written in
Creole. However, it is possible to use another rendering engine for the
template than for the layout by passing the :layout_engine option.
MediaWiki Templates


Dependency
WikiCloth


File Extension
.mediawiki and .mw


Example
mediawiki :wiki, :layout_engine => :erb


It is not possible to call methods from MediaWiki markup, nor to pass locals to
it. You therefore will usually use it in combination with another rendering
engine:
erb :overview, :locals => { :text => mediawiki(:introduction) }
Note that you may also call the mediawiki method from within other templates:
%h1 Hello From Haml!
%p= mediawiki(:greetings)
Since you cannot call Ruby from MediaWiki, you cannot use layouts written in
MediaWiki. However, it is possible to use another rendering engine for the
template than for the layout by passing the :layout_engine option.
CoffeeScript Templates


Dependency


        CoffeeScript
       and a
      
        way to execute javascript
      



File Extension
.coffee


Example
coffee :index


Stylus Templates


Dependency


        Stylus
       and a
      
        way to execute javascript
      



File Extension
.styl


Example
stylus :index


Before being able to use Stylus templates, you need to load stylus and
stylus/tilt first:
require 'sinatra'
require 'stylus'
require 'stylus/tilt'

get '/' do
  stylus :example
end
Yajl Templates


Dependency
yajl-ruby


File Extension
.yajl


Example


        yajl :index,
             :locals => { :key => 'qux' },
             :callback => 'present',
             :variable => 'resource'
      



The template source is evaluated as a Ruby string, and the
resulting json variable is converted using #to_json:
json = { :foo => 'bar' }
json[:baz] = key
The :callback and :variable options can be used to decorate the rendered
object:
var resource = {""foo"":""bar"",""baz"":""qux""};
present(resource);
WLang Templates


Dependency
WLang


File Extension
.wlang


Example
wlang :index, :locals => { :key => 'value' }


Since calling ruby methods is not idiomatic in WLang, you almost always want to
pass locals to it. Layouts written in WLang and yield are supported, though.
Accessing Variables in Templates
Templates are evaluated within the same context as route handlers. Instance
variables set in route handlers are directly accessible by templates:
get '/:id' do
  @foo = Foo.find(params['id'])
  haml '%h1= @foo.name'
end
Or, specify an explicit Hash of local variables:
get '/:id' do
  foo = Foo.find(params['id'])
  haml '%h1= bar.name', :locals => { :bar => foo }
end
This is typically used when rendering templates as partials from within
other templates.
Templates with yield and nested layouts
A layout is usually just a template that calls yield.
Such a template can be used either through the :template option as
described above, or it can be rendered with a block as follows:
erb :post, :layout => false do
  erb :index
end
This code is mostly equivalent to erb :index, :layout => :post.
Passing blocks to rendering methods is most useful for creating nested layouts:
erb :main_layout, :layout => false do
  erb :admin_layout do
    erb :user
  end
end
This can also be done in fewer lines of code with:
erb :admin_layout, :layout => :main_layout do
  erb :user
end
Currently, the following rendering methods accept a block: erb, haml,
liquid, slim , wlang. Also the general render method accepts a block.
Inline Templates
Templates may be defined at the end of the source file:
require 'sinatra'

get '/' do
  haml :index
end

__END__

@@ layout
%html
  = yield

@@ index
%div.title Hello world.
NOTE: Inline templates defined in the source file that requires sinatra are
automatically loaded. Call enable :inline_templates explicitly if you
have inline templates in other source files.
Named Templates
Templates may also be defined using the top-level template method:
template :layout do
  ""%html\n  =yield\n""
end

template :index do
  '%div.title Hello World!'
end

get '/' do
  haml :index
end
If a template named ""layout"" exists, it will be used each time a template
is rendered. You can individually disable layouts by passing
:layout => false or disable them by default via
set :haml, :layout => false:
get '/' do
  haml :index, :layout => !request.xhr?
end
Associating File Extensions
To associate a file extension with a template engine, use
Tilt.register. For instance, if you like to use the file extension
tt for Textile templates, you can do the following:
Tilt.register :tt, Tilt[:textile]
Adding Your Own Template Engine
First, register your engine with Tilt, then create a rendering method:
Tilt.register :myat, MyAwesomeTemplateEngine

helpers do
  def myat(*args) render(:myat, *args) end
end

get '/' do
  myat :index
end
Renders ./views/index.myat. See https://github.com/rtomayko/tilt to
learn more about Tilt.
Using Custom Logic for Template Lookup
To implement your own template lookup mechanism you can write your
own #find_template method:
configure do
  set :views [ './views/a', './views/b' ]
end

def find_template(views, name, engine, &block)
  Array(views).each do |v|
    super(v, name, engine, &block)
  end
end
Filters
Before filters are evaluated before each request within the same
context as the routes will be and can modify the request and response. Instance
variables set in filters are accessible by routes and templates:
before do
  @note = 'Hi!'
  request.path_info = '/foo/bar/baz'
end

get '/foo/*' do
  @note #=> 'Hi!'
  params['splat'] #=> 'bar/baz'
end
After filters are evaluated after each request within the same context as the
routes will be and can also modify the request and response. Instance variables
set in before filters and routes are accessible by after filters:
after do
  puts response.status
end
Note: Unless you use the body method rather than just returning a String from
the routes, the body will not yet be available in the after filter, since it is
generated later on.
Filters optionally take a pattern, causing them to be evaluated only if the
request path matches that pattern:
before '/protected/*' do
  authenticate!
end

after '/create/:slug' do |slug|
  session[:last_slug] = slug
end
Like routes, filters also take conditions:
before :agent => /Songbird/ do
  # ...
end

after '/blog/*', :host_name => 'example.com' do
  # ...
end
Helpers
Use the top-level helpers method to define helper methods for use in
route handlers and templates:
helpers do
  def bar(name)
    ""#{name}bar""
  end
end

get '/:name' do
  bar(params['name'])
end
Alternatively, helper methods can be separately defined in a module:
module FooUtils
  def foo(name) ""#{name}foo"" end
end

module BarUtils
  def bar(name) ""#{name}bar"" end
end

helpers FooUtils, BarUtils
The effect is the same as including the modules in the application class.
Using Sessions
A session is used to keep state during requests. If activated, you have one
session hash per user session:
enable :sessions

get '/' do
  ""value = "" << session[:value].inspect
end

get '/:value' do
  session['value'] = params['value']
end
Note that enable :sessions actually stores all data in a cookie. This
might not always be what you want (storing lots of data will increase your
traffic, for instance). You can use any Rack session middleware: in order to
do so, do not call enable :sessions, but instead pull in your
middleware of choice as you would any other middleware:
use Rack::Session::Pool, :expire_after => 2592000

get '/' do
  ""value = "" << session[:value].inspect
end

get '/:value' do
  session['value'] = params['value']
end
To improve security, the session data in the cookie is signed with a session
secret. A random secret is generated for you by Sinatra. However, since this
secret will change with every start of your application, you might want to
set the secret yourself, so all your application instances share it:
set :session_secret, 'super secret'
If you want to configure it further, you may also store a hash with options in
the sessions setting:
set :sessions, :domain => 'foo.com'
To share your session across other apps on subdomains of foo.com, prefix the
domain with a . like this instead:
set :sessions, :domain => '.foo.com'
Halting
To immediately stop a request within a filter or route use:
halt
You can also specify the status when halting:
halt 410
Or the body:
halt 'this will be the body'
Or both:
halt 401, 'go away!'
With headers:
halt 402, {'Content-Type' => 'text/plain'}, 'revenge'
It is of course possible to combine a template with halt:
halt erb(:error)
Passing
A route can punt processing to the next matching route using pass:
get '/guess/:who' do
  pass unless params['who'] == 'Frank'
  'You got me!'
end

get '/guess/*' do
  'You missed!'
end
The route block is immediately exited and control continues with the next
matching route. If no matching route is found, a 404 is returned.
Triggering Another Route
Sometimes pass is not what you want, instead you would like to get the result
of calling another route. Simply use call to achieve this:
get '/foo' do
  status, headers, body = call env.merge(""PATH_INFO"" => '/bar')
  [status, headers, body.map(&:upcase)]
end

get '/bar' do
  ""bar""
end
Note that in the example above, you would ease testing and increase performance
by simply moving ""bar"" into a helper used by both /foo and /bar.
If you want the request to be sent to the same application instance rather than
a duplicate, use call! instead of call.
Check out the Rack specification if you want to learn more about call.
Setting Body, Status Code and Headers
It is possible and recommended to set the status code and response body with the
return value of the route block. However, in some scenarios you might want to
set the body at an arbitrary point in the execution flow. You can do so with the
body helper method. If you do so, you can use that method from there on to
access the body:
get '/foo' do
  body ""bar""
end

after do
  puts body
end
It is also possible to pass a block to body, which will be executed by the
Rack handler (this can be used to implement streaming, see ""Return Values"").
Similar to the body, you can also set the status code and headers:
get '/foo' do
  status 418
  headers \
    ""Allow""   => ""BREW, POST, GET, PROPFIND, WHEN"",
    ""Refresh"" => ""Refresh: 20; http://www.ietf.org/rfc/rfc2324.txt""
  body ""I'm a tea pot!""
end
Like body, headers and status with no arguments can be used to access
their current values.
Streaming Responses
Sometimes you want to start sending out data while still generating parts of
the response body. In extreme examples, you want to keep sending data until
the client closes the connection. You can use the stream helper to avoid
creating your own wrapper:
get '/' do
  stream do |out|
    out << ""It's gonna be legen -\n""
    sleep 0.5
    out << "" (wait for it) \n""
    sleep 1
    out << ""- dary!\n""
  end
end
This allows you to implement streaming APIs,
Server Sent Events, and can be used as
the basis for WebSockets. It can also be
used to increase throughput if some but not all content depends on a slow
resource.
Note that the streaming behavior, especially the number of concurrent requests,
highly depends on the web server used to serve the application. Some servers
might not even support streaming at all. If the server does not support
streaming, the body will be sent all at once after the block passed to stream
finishes executing. Streaming does not work at all with Shotgun.
If the optional parameter is set to keep_open, it will not call close on
the stream object, allowing you to close it at any later point in the
execution flow. This only works on evented servers, like Thin and Rainbows.
Other servers will still close the stream:
# long polling

set :server, :thin
connections = []

get '/subscribe' do
  # register a client's interest in server events
  stream(:keep_open) do |out|
    connections << out
    # purge dead connections
    connections.reject!(&:closed?)
  end
end

post '/:message' do
  connections.each do |out|
    # notify client that a new message has arrived
    out << params['message'] << ""\n""

    # indicate client to connect again
    out.close
  end

  # acknowledge
  ""message received""
end
Logging
In the request scope, the logger helper exposes a Logger instance:
get '/' do
  logger.info ""loading data""
  # ...
end
This logger will automatically take your Rack handler's logging settings into
account. If logging is disabled, this method will return a dummy object, so
you do not have to worry about it in your routes and filters.
Note that logging is only enabled for Sinatra::Application by default, so if
you inherit from Sinatra::Base, you probably want to enable it yourself:
class MyApp < Sinatra::Base
  configure :production, :development do
    enable :logging
  end
end
To avoid any logging middleware to be set up, set the logging setting to
nil. However, keep in mind that logger will in that case return nil. A
common use case is when you want to set your own logger. Sinatra will use
whatever it will find in env['rack.logger'].
Mime Types
When using send_file or static files you may have mime types Sinatra
doesn't understand. Use mime_type to register them by file extension:
configure do
  mime_type :foo, 'text/foo'
end
You can also use it with the content_type helper:
get '/' do
  content_type :foo
  ""foo foo foo""
end
Generating URLs
For generating URLs you should use the url helper method, for instance, in
Haml:
%a{:href => url('/foo')} foo
It takes reverse proxies and Rack routers into account, if present.
This method is also aliased to to (see below for an example).
Browser Redirect
You can trigger a browser redirect with the redirect helper method:
get '/foo' do
  redirect to('/bar')
end
Any additional parameters are handled like arguments passed to halt:
redirect to('/bar'), 303
redirect 'http://google.com', 'wrong place, buddy'
You can also easily redirect back to the page the user came from with
redirect back:
get '/foo' do
  ""<a href='/bar'>do something</a>""
end

get '/bar' do
  do_something
  redirect back
end
To pass arguments with a redirect, either add them to the query:
redirect to('/bar?sum=42')
Or use a session:
enable :sessions

get '/foo' do
  session[:secret] = 'foo'
  redirect to('/bar')
end

get '/bar' do
  session[:secret]
end
Cache Control
Setting your headers correctly is the foundation for proper HTTP caching.
You can easily set the Cache-Control header like this:
get '/' do
  cache_control :public
  ""cache it!""
end
Pro tip: Set up caching in a before filter:
before do
  cache_control :public, :must_revalidate, :max_age => 60
end
If you are using the expires helper to set the corresponding header,
Cache-Control will be set automatically for you:
before do
  expires 500, :public, :must_revalidate
end
To properly use caches, you should consider using etag or last_modified.
It is recommended to call those helpers before doing any heavy lifting, as
they will immediately flush a response if the client already has the current
version in its cache:
get ""/article/:id"" do
  @article = Article.find params['id']
  last_modified @article.updated_at
  etag @article.sha1
  erb :article
end
It is also possible to use a
weak ETag:
etag @article.sha1, :weak
These helpers will not do any caching for you, but rather feed the necessary
information to your cache. If you are looking for a quick reverse-proxy caching
solution, try rack-cache:
require ""rack/cache""
require ""sinatra""

use Rack::Cache

get '/' do
  cache_control :public, :max_age => 36000
  sleep 5
  ""hello""
end
Use the :static_cache_control setting (see below) to add
Cache-Control header info to static files.
According to RFC 2616, your application should behave differently if the If-Match
or If-None-Match header is set to *, depending on whether the resource
requested is already in existence. Sinatra assumes resources for safe (like get)
and idempotent (like put) requests are already in existence, whereas other
resources (for instance post requests) are treated as new resources. You
can change this behavior by passing in a :new_resource option:
get '/create' do
  etag '', :new_resource => true
  Article.create
  erb :new_article
end
If you still want to use a weak ETag, pass in a :kind option:
etag '', :new_resource => true, :kind => :weak
Sending Files
To return the contents of a file as the response, you can use the send_file
helper method:
get '/' do
  send_file 'foo.png'
end
It also takes options:
send_file 'foo.png', :type => :jpg
The options are:

filename
File name to be used in the response, defaults to the real file name.
last_modified
Value for Last-Modified header, defaults to the file's mtime.
type
Value for Content-Type header, guessed from the file extension if
    missing.
disposition

      Value for Content-Disposition header, possible values: nil
      (default), :attachment and :inline

length
Value for Content-Length header, defaults to file size.
status

      Status code to be sent. Useful when sending a static file as an error page.
  If supported by the Rack handler, other means than streaming from the Ruby
  process will be used. If you use this helper method, Sinatra will
  automatically handle range requests.
</dd>


Accessing the Request Object
The incoming request object can be accessed from request level (filter, routes,
error handlers) through the request method:
# app running on http://example.com/example
get '/foo' do
  t = %w[text/css text/html application/javascript]
  request.accept              # ['text/html', '*/*']
  request.accept? 'text/xml'  # true
  request.preferred_type(t)   # 'text/html'
  request.body                # request body sent by the client (see below)
  request.scheme              # ""http""
  request.script_name         # ""/example""
  request.path_info           # ""/foo""
  request.port                # 80
  request.request_method      # ""GET""
  request.query_string        # """"
  request.content_length      # length of request.body
  request.media_type          # media type of request.body
  request.host                # ""example.com""
  request.get?                # true (similar methods for other verbs)
  request.form_data?          # false
  request[""some_param""]       # value of some_param parameter. [] is a shortcut to the params hash.
  request.referrer            # the referrer of the client or '/'
  request.user_agent          # user agent (used by :agent condition)
  request.cookies             # hash of browser cookies
  request.xhr?                # is this an ajax request?
  request.url                 # ""http://example.com/example/foo""
  request.path                # ""/example/foo""
  request.ip                  # client IP address
  request.secure?             # false (would be true over ssl)
  request.forwarded?          # true (if running behind a reverse proxy)
  request.env                 # raw env hash handed in by Rack
end
Some options, like script_name or path_info, can also be written:
before { request.path_info = ""/"" }

get ""/"" do
  ""all requests end up here""
end
The request.body is an IO or StringIO object:
post ""/api"" do
  request.body.rewind  # in case someone already read it
  data = JSON.parse request.body.read
  ""Hello #{data['name']}!""
end
Attachments
You can use the attachment helper to tell the browser the response should be
stored on disk rather than displayed in the browser:
get '/' do
  attachment
  ""store it!""
end
You can also pass it a file name:
get '/' do
  attachment ""info.txt""
  ""store it!""
end
Dealing with Date and Time
Sinatra offers a time_for helper method that generates a Time object from the
given value. It is also able to convert DateTime, Date and similar classes:
get '/' do
  pass if Time.now > time_for('Dec 23, 2012')
  ""still time""
end
This method is used internally by expires, last_modified and akin. You can
therefore easily extend the behavior of those methods by overriding time_for
in your application:
helpers do
  def time_for(value)
    case value
    when :yesterday then Time.now - 24*60*60
    when :tomorrow  then Time.now + 24*60*60
    else super
    end
  end
end

get '/' do
  last_modified :yesterday
  expires :tomorrow
  ""hello""
end
Looking Up Template Files
The find_template helper is used to find template files for rendering:
find_template settings.views, 'foo', Tilt[:haml] do |file|
  puts ""could be #{file}""
end
This is not really useful. But it is useful that you can actually override this
method to hook in your own lookup mechanism. For instance, if you want to be
able to use more than one view directory:
set :views, ['views', 'templates']

helpers do
  def find_template(views, name, engine, &block)
    Array(views).each { |v| super(v, name, engine, &block) }
  end
end
Another example would be using different directories for different engines:
set :views, :sass => 'views/sass', :haml => 'templates', :default => 'views'

helpers do
  def find_template(views, name, engine, &block)
    _, folder = views.detect { |k,v| engine == Tilt[k] }
    folder ||= views[:default]
    super(folder, name, engine, &block)
  end
end
You can also easily wrap this up in an extension and share with others!
Note that find_template does not check if the file really exists but
rather calls the given block for all possible paths. This is not a performance
issue, since render will use break as soon as a file is found. Also,
template locations (and content) will be cached if you are not running in
development mode. You should keep that in mind if you write a really crazy
method.
Configuration
Run once, at startup, in any environment:
configure do
  # setting one option
  set :option, 'value'

  # setting multiple options
  set :a => 1, :b => 2

  # same as `set :option, true`
  enable :option

  # same as `set :option, false`
  disable :option

  # you can also have dynamic settings with blocks
  set(:css_dir) { File.join(views, 'css') }
end
Run only when the environment (RACK_ENV environment variable) is set to
:production:
configure :production do
  ...
end
Run when the environment is set to either :production or :test:
configure :production, :test do
  ...
end
You can access those options via settings:
configure do
  set :foo, 'bar'
end

get '/' do
  settings.foo? # => true
  settings.foo  # => 'bar'
  ...
end
Configuring attack protection
Sinatra is using
Rack::Protection to defend
your application against common, opportunistic attacks. You can easily disable
this behavior (which will open up your application to tons of common
vulnerabilities):
disable :protection
To skip a single defense layer, set protection to an options hash:
set :protection, :except => :path_traversal
You can also hand in an array in order to disable a list of protections:
set :protection, :except => [:path_traversal, :session_hijacking]
By default, Sinatra will only set up session based protection if :sessions
has been enabled. Sometimes you want to set up sessions on your own, though. In
that case you can get it to set up session based protections by passing the
:session option:
use Rack::Session::Pool
set :protection, :session => true
Available Settings

absolute_redirects

    If disabled, Sinatra will allow relative redirects, however, Sinatra will no
    longer conform with RFC 2616 (HTTP 1.1), which only allows absolute redirects.
  

    Enable if your app is running behind a reverse proxy that has not been set up
    properly. Note that the url helper will still produce absolute URLs, unless you
    pass in false as the second parameter.
  
Disabled by default.
add_charset

    Mime types the content_type helper will automatically add the charset info to.
    You should add to it rather than overriding this option:
    settings.add_charset << ""application/foobar""

app_file

    Path to the main application file, used to detect project root, views and public
    folder and inline templates.
  
bind
IP address to bind to (default: 0.0.0.0 or
localhost if your `environment` is set to development). Only used
  for built-in server.
default_encoding
Encoding to assume if unknown (defaults to ""utf-8"").
dump_errors
Display errors in the log.
environment

    Current environment. Defaults to ENV['RACK_ENV'], or
    ""development"" if not available.
  
logging
Use the logger.
lock

    Places a lock around every request, only running processing on request
    per Ruby process concurrently.
  
Enabled if your app is not thread-safe. Disabled per default.
method_override

    Use _method magic to allow put/delete forms in browsers that
    don't support it.
  
port
Port to listen on. Only used for built-in server.
prefixed_redirects

    Whether or not to insert request.script_name into redirects if no
    absolute path is given. That way redirect '/foo' would behave like
    redirect to('/foo'). Disabled per default.
  
protection
Whether or not to enable web attack protections. See protection section
  above.
public_dir
Alias for public_folder. See below.
public_folder

    Path to the folder public files are served from. Only used if static
    file serving is enabled (see static setting below). Inferred from
    app_file setting if not set.
  
reload_templates

    Whether or not to reload templates between requests. Enabled in development
    mode.
  
root

    Path to project root folder. Inferred from app_file setting if not
    set.
  
raise_errors

    Raise exceptions (will stop application). Enabled by default when
    environment is set to ""test"", disabled otherwise.
  
run

    If enabled, Sinatra will handle starting the web server. Do not
    enable if using rackup or other means.
  
running
Is the built-in server running now? Do not change this setting!
server

    Server or list of servers to use for built-in server. Order indicates
    priority, default depends on Ruby implementation.
  
sessions

    Enable cookie-based sessions support using Rack::Session::Cookie.
    See 'Using Sessions' section for more information.
  
show_exceptions

    Show a stack trace in the browser when an exception happens. Enabled by
    default when environment is set to ""development"",
    disabled otherwise.
  

    Can also be set to :after_handler to trigger app-specified error
    handling before showing a stack trace in the browser.
  
static
Whether Sinatra should handle serving static files.
Disable when using a server able to do this on its own.
Disabling will boost performance.

    Enabled per default in classic style, disabled for modular apps.
  
static_cache_control

    When Sinatra is serving static files, set this to add Cache-Control
    headers to the responses. Uses the cache_control helper. Disabled
    by default.
  

    Use an explicit array when setting multiple values:
    set :static_cache_control, [:public, :max_age => 300]

threaded

    If set to true, will tell Thin to use EventMachine.defer
    for processing the request.
  
traps
Whether Sinatra should handle system signals.
views

    Path to the views folder. Inferred from app_file setting if
    not set.
  
x_cascade

    Whether or not to set the X-Cascade header if no route matches.
    Defaults to true.
  

Environments
There are three predefined environments: ""development"", ""production"" and
""test"". Environments can be set through the RACK_ENV environment variable.
The default value is ""development"". In the ""development"" environment all
templates are reloaded between requests, and special not_found and error
handlers display stack traces in your browser. In the ""production"" and
""test"" environments, templates are cached by default.
To run different environments, set the RACK_ENV environment variable:
RACK_ENV=production ruby my_app.rb
You can use predefined methods: development?, test? and production? to
check the current environment setting:
get '/' do
  if settings.development?
    ""development!""
  else
    ""not development!""
  end
end
Error Handling
Error handlers run within the same context as routes and before filters, which
means you get all the goodies it has to offer, like haml,
erb, halt, etc.
Not Found
When a Sinatra::NotFound exception is raised, or the response's status
code is 404, the not_found handler is invoked:
not_found do
  'This is nowhere to be found.'
end
Error
The error handler is invoked any time an exception is raised from a route
block or a filter. But note in development it will only run if you set the
show exceptions option to :after_handler:
set :show_exceptions, :after_handler
The exception object can be obtained from the sinatra.error Rack variable:
error do
  'Sorry there was a nasty error - ' + env['sinatra.error'].message
end
Custom errors:
error MyCustomError do
  'So what happened was...' + env['sinatra.error'].message
end
Then, if this happens:
get '/' do
  raise MyCustomError, 'something bad'
end
You get this:
So what happened was... something bad

Alternatively, you can install an error handler for a status code:
error 403 do
  'Access forbidden'
end

get '/secret' do
  403
end
Or a range:
error 400..510 do
  'Boom'
end
Sinatra installs special not_found and error handlers when
running under the development environment to display nice stack traces
and additional debugging information in your browser.
Rack Middleware
Sinatra rides on Rack, a minimal standard
interface for Ruby web frameworks. One of Rack's most interesting capabilities
for application developers is support for ""middleware"" -- components that sit
between the server and your application monitoring and/or manipulating the
HTTP request/response to provide various types of common functionality.
Sinatra makes building Rack middleware pipelines a cinch via a top-level
use method:
require 'sinatra'
require 'my_custom_middleware'

use Rack::Lint
use MyCustomMiddleware

get '/hello' do
  'Hello World'
end
The semantics of use are identical to those defined for the
Rack::Builder DSL
(most frequently used from rackup files). For example, the use method
accepts multiple/variable args as well as blocks:
use Rack::Auth::Basic do |username, password|
  username == 'admin' && password == 'secret'
end
Rack is distributed with a variety of standard middleware for logging,
debugging, URL routing, authentication, and session handling. Sinatra uses
many of these components automatically based on configuration so you
typically don't have to use them explicitly.
You can find useful middleware in
rack,
rack-contrib,
or in the Rack wiki.
Testing
Sinatra tests can be written using any Rack-based testing library or framework.
Rack::Test
is recommended:
require 'my_sinatra_app'
require 'minitest/autorun'
require 'rack/test'

class MyAppTest < Minitest::Test
  include Rack::Test::Methods

  def app
    Sinatra::Application
  end

  def test_my_default
    get '/'
    assert_equal 'Hello World!', last_response.body
  end

  def test_with_params
    get '/meet', :name => 'Frank'
    assert_equal 'Hello Frank!', last_response.body
  end

  def test_with_rack_env
    get '/', {}, 'HTTP_USER_AGENT' => 'Songbird'
    assert_equal ""You're using Songbird!"", last_response.body
  end
end
Note: If you are using Sinatra in the modular style, replace
Sinatra::Application above with the class name of your app.
Sinatra::Base - Middleware, Libraries, and Modular Apps
Defining your app at the top-level works well for micro-apps but has
considerable drawbacks when building reusable components such as Rack
middleware, Rails metal, simple libraries with a server component, or even
Sinatra extensions. The top-level assumes a micro-app style configuration
(e.g., a single application file, ./public and ./views
directories, logging, exception detail page, etc.). That's where
Sinatra::Base comes into play:
require 'sinatra/base'

class MyApp < Sinatra::Base
  set :sessions, true
  set :foo, 'bar'

  get '/' do
    'Hello world!'
  end
end
The methods available to Sinatra::Base subclasses are exactly the same as
those available via the top-level DSL. Most top-level apps can be converted to
Sinatra::Base components with two modifications:

Your file should require sinatra/base instead of sinatra;
otherwise, all of Sinatra's DSL methods are imported into the main
namespace.
Put your app's routes, error handlers, filters, and options in a subclass
of Sinatra::Base.

Sinatra::Base is a blank slate. Most options are disabled by default,
including the built-in server. See
Configuring Settings
for details on available options and their behavior. If you want
behavior more similar to when you define your app at the top level (also
known as Classic style), you
can subclass Sinatra::Application.
require 'sinatra/base'

class MyApp < Sinatra::Application
  get '/' do
    'Hello world!'
  end
end
Modular vs. Classic Style
Contrary to common belief, there is nothing wrong with the classic style. If it
suits your application, you do not have to switch to a modular application.
The main disadvantage of using the classic style rather than the modular style
is that you will only have one Sinatra application per Ruby process. If you
plan to use more than one, switch to the modular style. There is no reason you
cannot mix the modular and the classic styles.
If switching from one style to the other, you should be aware of slightly
different default settings:


Setting
Classic
Modular
Modular


app_file
file loading sinatra
file subclassing Sinatra::Base
file subclassing Sinatra::Application


run
$0 == app_file
false
false


logging
true
false
true


method_override
true
false
true


inline_templates
true
false
true


static
true
false
true


Serving a Modular Application
There are two common options for starting a modular app, actively starting with
run!:
# my_app.rb
require 'sinatra/base'

class MyApp < Sinatra::Base
  # ... app code here ...

  # start the server if ruby file executed directly
  run! if app_file == $0
end
Start with:
ruby my_app.rb
Or with a config.ru file, which allows using any Rack handler:
# config.ru (run with rackup)
require './my_app'
run MyApp
Run:
rackup -p 4567
Using a Classic Style Application with a config.ru
Write your app file:
# app.rb
require 'sinatra'

get '/' do
  'Hello world!'
end
And a corresponding config.ru:
require './app'
run Sinatra::Application
When to use a config.ru?
A config.ru file is recommended if:

You want to deploy with a different Rack handler (Passenger, Unicorn,
Heroku, ...).
You want to use more than one subclass of Sinatra::Base.
You want to use Sinatra only for middleware, and not as an endpoint.

There is no need to switch to a config.ru simply because you switched to
the modular style, and you don't have to use the modular style for running with
a config.ru.
Using Sinatra as Middleware
Not only is Sinatra able to use other Rack middleware, any Sinatra application
can in turn be added in front of any Rack endpoint as middleware itself. This
endpoint could be another Sinatra application, or any other Rack-based
application (Rails/Ramaze/Camping/...):
require 'sinatra/base'

class LoginScreen < Sinatra::Base
  enable :sessions

  get('/login') { haml :login }

  post('/login') do
    if params['name'] == 'admin' && params['password'] == 'admin'
      session['user_name'] = params['name']
    else
      redirect '/login'
    end
  end
end

class MyApp < Sinatra::Base
  # middleware will run before filters
  use LoginScreen

  before do
    unless session['user_name']
      halt ""Access denied, please <a href='/login'>login</a>.""
    end
  end

  get('/') { ""Hello #{session['user_name']}."" }
end
Dynamic Application Creation
Sometimes you want to create new applications at runtime without having to
assign them to a constant. You can do this with Sinatra.new:
require 'sinatra/base'
my_app = Sinatra.new { get('/') { ""hi"" } }
my_app.run!
It takes the application to inherit from as an optional argument:
# config.ru (run with rackup)
require 'sinatra/base'

controller = Sinatra.new do
  enable :logging
  helpers MyHelpers
end

map('/a') do
  run Sinatra.new(controller) { get('/') { 'a' } }
end

map('/b') do
  run Sinatra.new(controller) { get('/') { 'b' } }
end
This is especially useful for testing Sinatra extensions or using Sinatra in
your own library.
This also makes using Sinatra as middleware extremely easy:
require 'sinatra/base'

use Sinatra do
  get('/') { ... }
end

run RailsProject::Application
Scopes and Binding
The scope you are currently in determines what methods and variables are
available.
Application/Class Scope
Every Sinatra application corresponds to a subclass of Sinatra::Base.
If you are using the top-level DSL (require 'sinatra'), then this
class is Sinatra::Application, otherwise it is the subclass you
created explicitly. At class level you have methods like get or before, but
you cannot access the request or session objects, as there is only a
single application class for all requests.
Options created via set are methods at class level:
class MyApp < Sinatra::Base
  # Hey, I'm in the application scope!
  set :foo, 42
  foo # => 42

  get '/foo' do
    # Hey, I'm no longer in the application scope!
  end
end
You have the application scope binding inside:

Your application class body
Methods defined by extensions
The block passed to helpers
Procs/blocks used as value for set
The block passed to Sinatra.new

You can reach the scope object (the class) like this:

Via the object passed to configure blocks (configure { |c| ... })
settings from within the request scope

Request/Instance Scope
For every incoming request, a new instance of your application class is
created, and all handler blocks run in that scope. From within this scope you
can access the request and session objects or call rendering methods like
erb or haml. You can access the application scope from within the request
scope via the settings helper:
class MyApp < Sinatra::Base
  # Hey, I'm in the application scope!
  get '/define_route/:name' do
    # Request scope for '/define_route/:name'
    @value = 42

    settings.get(""/#{params['name']}"") do
      # Request scope for ""/#{params['name']}""
      @value # => nil (not the same request)
    end

    ""Route defined!""
  end
end
You have the request scope binding inside:

get, head, post, put, delete, options, patch, link, and unlink blocks
before and after filters
helper methods
templates/views

Delegation Scope
The delegation scope just forwards methods to the class scope. However, it
does not behave exactly like the class scope, as you do not have the class
binding. Only methods explicitly marked for delegation are available, and you
do not share variables/state with the class scope (read: you have a different
self). You can explicitly add method delegations by calling
Sinatra::Delegator.delegate :method_name.
You have the delegate scope binding inside:

The top level binding, if you did require ""sinatra""
An object extended with the Sinatra::Delegator mixin

Have a look at the code for yourself: here's the
Sinatra::Delegator mixin
being extending the main object.
Command Line
Sinatra applications can be run directly:
ruby myapp.rb [-h] [-x] [-e ENVIRONMENT] [-p PORT] [-o HOST] [-s HANDLER]
Options are:
-h # help
-p # set the port (default is 4567)
-o # set the host (default is 0.0.0.0)
-e # set the environment (default is development)
-s # specify rack server/handler (default is thin)
-x # turn on the mutex lock (default is off)

Multi-threading
Paraphrasing from [this StackOverflow answer][so-answer] by Konstantin
Sinatra doesn't impose any concurrency model, but leaves that to the
underlying Rack handler (server) like Thin, Puma or WEBrick. Sinatra
itself is thread-safe, so there won't be any problem if the Rack handler
uses a threaded model of concurrency. This would mean that when starting
the server, you'd have to specify the correct invocation method for the
specific Rack handler. The following example is a demonstration of how
to start a multi-threaded Thin server:
# app.rb

require 'sinatra/base'

class App < Sinatra::Base
  get '/' do
    ""Hello, World""
  end
end

App.run!
To start the server, the command would be:
thin --threaded start
[so-answer]: http://stackoverflow.com/questions/6278817/is-sinatra-multi-threaded/6282999#6282999)
Requirement
The following Ruby versions are officially supported:

Ruby 1.8.7

    1.8.7 is fully supported, however, if nothing is keeping you from it, we
    recommend upgrading or switching to JRuby or Rubinius. Support for 1.8.7
    will not be dropped before Sinatra 2.0. Ruby 1.8.6 is no longer supported.
  
Ruby 1.9.2

    1.9.2 is fully supported. Do not use 1.9.2p0, as it is known to cause
    segmentation faults when running Sinatra. Official support will continue
    at least until the release of Sinatra 1.5.
  
Ruby 1.9.3

    1.9.3 is fully supported and recommended. Please note that switching to 1.9.3
    from an earlier version will invalidate all sessions. 1.9.3 will be supported
    until the release of Sinatra 2.0.
  
Ruby 2.x

    2.x is fully supported and recommended. There are currently no plans to drop
    official support for it.
  
Rubinius

    Rubinius is officially supported (Rubinius >= 2.x). It is recommended to
    gem install puma.
  
JRuby

    The latest stable release of JRuby is officially supported. It is not
    recommended to use C extensions with JRuby. It is recommended to
    gem install trinidad.
  

We also keep an eye on upcoming Ruby versions.
The following Ruby implementations are not officially supported but still are
known to run Sinatra:

Older versions of JRuby and Rubinius
Ruby Enterprise Edition
MacRuby, Maglev, IronRuby
Ruby 1.9.0 and 1.9.1 (but we do recommend against using those)

Not being officially supported means if things only break there and not on a
supported platform, we assume it's not our issue but theirs.
We also run our CI against ruby-head (future releases of MRI), but we can't
guarantee anything, since it is constantly moving. Expect upcoming 2.x releases
to be fully supported.
Sinatra should work on any operating system supported by the chosen Ruby
implementation.
If you run MacRuby, you should gem install control_tower.
Sinatra currently doesn't run on Cardinal, SmallRuby, BlueRuby or any
Ruby version prior to 1.8.7.
The Bleeding Edge
If you would like to use Sinatra's latest bleeding-edge code, feel free to run your
application against the master branch, it should be rather stable.
We also push out prerelease gems from time to time, so you can do a
gem install sinatra --pre
to get some of the latest features.
With Bundler
If you want to run your application with the latest Sinatra, using
Bundler is the recommended way.
First, install bundler, if you haven't:
gem install bundler
Then, in your project directory, create a Gemfile:
source 'https://rubygems.org'
gem 'sinatra', :github => ""sinatra/sinatra""

# other dependencies
gem 'haml'                    # for instance, if you use haml
gem 'activerecord', '~> 3.0'  # maybe you also need ActiveRecord 3.x
Note that you will have to list all your application's dependencies in the Gemfile.
Sinatra's direct dependencies (Rack and Tilt) will, however, be automatically
fetched and added by Bundler.
Now you can run your app like this:
bundle exec ruby myapp.rb
Roll Your Own
Create a local clone and run your app with the sinatra/lib directory
on the $LOAD_PATH:
cd myapp
git clone git://github.com/sinatra/sinatra.git
ruby -I sinatra/lib myapp.rb
To update the Sinatra sources in the future:
cd myapp/sinatra
git pull
Install Globally
You can build the gem on your own:
git clone git://github.com/sinatra/sinatra.git
cd sinatra
rake sinatra.gemspec
rake install
If you install gems as root, the last step should be:
sudo rake install
Versioning
Sinatra follows Semantic Versioning, both SemVer and
SemVerTag.
Further Reading

Project Website - Additional documentation,
news, and links to other resources.
Contributing - Find a bug? Need
help? Have a patch?
Issue tracker
Twitter
Mailing List
IRC: #sinatra on http://freenode.net
Sinatra & Friends on Slack and see
here for an invite.
Sinatra Book Cookbook Tutorial
Sinatra Recipes Community
contributed recipes
API documentation for the latest release
or the current HEAD on
http://rubydoc.info
CI server

",batch2,8:10:17,Done
242,KDr2/emacs,,batch1,16:58:59,Done
243,Lordsm926/l,"LenoxBot

LenoxBot is a Discord bot that offers many cool new features to your Discord server!






Table of Content

Installation
Support
License

Installation
You can find the whole instructions to download and how to configure the bot in our documentation here.
Support
Twitter
Discord Server
Documentation
License
MIT
",batch2,8:09:15,Done
244,GatorEducator/gatorgrader,"GatorGrader



The only tool you'll need to ensure your student's code and writing is up to
speed!


     
Table of Contents

Quickstart Guide
Key Features
What Do People Think about GatorGrader?
Installing GatorGrader
Testing GatorGrader

Automated Testing
Test Coverage
Testing with Multiple Python Versions
Code Linting


Running GatorGrader
Using Docker
Comparison to Other Tools
Presentations
Contributing
Contributors

Quickstart Guide

Starter Repositories
An easy way to get started with GatorGrader is to check out our sample starter repositories.
The following starter repositories provide examples of how GatorGrader files should be created
to check programs and documentation for different languages:
Java,
Python, LaTex and HTML with CSS. These
examples also show how to integrate GatorGrader with GitHub
Classroom and Travis
CI. When you follow these examples, the Gradle plugin
for GatorGrader will install it automatically when you run gradle grade in a
terminal window. Please be aware that these repositories are meant to have a majority
of red checks. This is only meant to be a “starter” and give an insight into what must be accomplished within the lab.
Solution Repositories
The next step to get involved with GatorGrader is to checkout our sample solution
repositories. The following solution repositories provide examples of how GatorGrader files should be
created to check programs and documentation for different languages:
Java,
Python, LaTex , and HTML with CSS. These examples
also show how to integrate GatorGrader with  GitHub
Classroom and Travis
CI.  When you follow these examples, the Gradle plugin
for GatorGrader will install it automatically when you run gradle grade in a
terminal window. Please be aware that these repositories are meant to have a majority
of green checks. This is meant to show what must be accomplished within a lab/practical
and what it looks like when those tasks are completed.

Key Features
GatorGrader automatically checks the work of technical writers and programmers.
It can:


Use its Gradle plugin to check
projects implemented and documented in a wide variety of languages (e.g.,
Java, Python, LaTeX, Markdown, HTML, and CSS).


Integrate with GitHub Classroom to check
solution and starter repositories created for professors and students,
respectively.


Run in a cloud-based environment like Travis CI or
on the command-line of a developer's workstation.


Operate as a ""batteries included"" grading tool, supporting automated checks
like the following:


Does a file exist in the correct directory with the requested name?


Does technical writing contain the desired number of words and paragraphs?


Does source code contain the designated number of language-specific comments?


Does source code or technical writing contain a required fragment or match
a specified regular expression?


Does a command execute correctly and produce the expected number of output
lines?


Does a command execute and produce output containing a fragment or matching
a regular expression?


Does a GitHub repository contain the anticipated number of commits?




Aligning with key recommendations in a recent National Academies
report,
GatorGrader helps instructors automatically check student submissions in both
introductory and application-oriented classes using languages like Markdown,
Java, Python, JavaScript, CSS, and HTML. GatorGrader does not aim to solve
problems related to building and linting a project or managing an assignment's
submission, instead integrating with existing tools and systems like
Gradle, GitHub, and GitHub
Classroom to effectively handle those tasks.
Installing GatorGrader
Installing GatorGrader is not necessary if you intend to use it through its
Gradle plugin. If you want to
participate in the development of GatorGrader, the project maintainers suggest
the use of Pyenv to install Python 3.6 or
above. In addition to installing Git to access the
project's GitHub repository, you should also install
Pipenv for its support of package and virtual
environment management. After completing the installation of these tools, you
can type the following command in your terminal window to clone GatorGrader's
GitHub repository:
git clone https://github.com/GatorEducator/gatorgrader.git
If you plan to develop new features for GatorGrader or if you want to run the
tool's test suite in Pytest, then you
will need to install the developer dependencies by typing pipenv install --python=""$(pyenv which python)"" --dev in the directory that contains
GatorGrader. If you want to use GatorGrader, then you can type pipenv install --python=""$(pyenv which python)"" instead. Once these commands complete
successfully, you have officially installed GatorGrader! Note that running these
commands will ensure that Pipenv creates a virtual environment for GatorGrader
that is bound to the version of Python that Pyenv set for use.
Testing GatorGrader
Automated Testing
The developers use Pytest for the testing
of GatorGrader. Depending on your goals, there are several different
configurations in which you can run the provided test suite. If you want to run
the test suite to see if the test cases are passing, then running this command
in a terminal window will perform testing with the version of Python to which
Pipenv's virtual environment is currently bound.
pipenv run test

Test Coverage
Along with running the test suite, the developers of GatorGrader use statement
and branch coverage to enhance their testing activities. To see the coverage of
the tests while also highlighting the lines that are not currently covered by
the tests, you can run this command in a terminal window. As with the previous
command, this will run the tests in the version of Python to which Pipenv's
virtual environment is currently bound.
pipenv run cover

Testing with Multiple Python Versions
The previous two commands are restricted to running the test suite in the
version of Python to which Pipenv was bound. If you have installed multiple
versions of Python with Pyenv and you want to iteratively bind Pipenv to each
version and then run the test suite, then you should first run the following
commands to install Pipx and use Pipx to
install Invoke. The first of these three
commands will install pipx, a program that supports the execution of Python
packages in isolated environments. The second command makes the directory
~/.local/bin/ a part of the search path for executable Python programs and the
third one installs the invoke command so that it is available on your
workstation outside of a virtual environment managed by Pipenv, thereby ensuring
that it is always available to run tasks.
pip install pipx --user
python -m userpath append ~/.local/bin/
pipx install invoke

Now you can run the test suite in the specified versions of Python with the
following command. This example command will run the test suite in Python 3.6.8
and Python 3.7.3.
invoke -c scripts/tasks test --pyenv 3.6.8 --pyenv 3.7.3

If you want to track test coverage while running the tests in both Python 3.6.8
and 3.7.3, then you can run the following command.
invoke -c scripts/tasks cover --pyenv 3.6.8 --pyenv 3.7.3

You can switch the version to which Pipenv is bound by running the following
command that adopts, for instance, Python 3.7.3.
invoke -c scripts/tasks switch --pyenv 3.7.3

Code Linting
The developers of GatorGrader use linting and code formatting tools, such as
Pylint,
Pydocstyle, and
Black. After installing GatorGrader's
development dependencies with Pipenv, you can run all of the linters by typing
this command in a terminal window.
pipenv run lint --check
Automated Checks
Want to learn about our linting checks? Check us out on our website,
GatorGrader! We have detailed
descriptions of our linting checks and more! To get an idea of the linting checks we
offer, here is a quick list:


ConfirmFileExists


CountCommandOutput


CountCommits


CountFileLines


CountFileParagraphs


Want to learn about our automated checks? Check them out on our website,
gatorgrader.org! We have detailed
descriptions of our automated checks and more!
Something you should know when working with our checks is that all of
them come with some optional arguments. Optional arguments that you are likely
to encounter:

-h
--help
--exact
--advanced

If --help is tagged along with a check then a help message will be displayed and
then exited. If further assistance is needed, please contact us on GitHub.
Another feature with our automated checks is the plug-in based approach. This allows
users to implement their own check if our initial 15 do not fulfill a check that
you find necessary.
Running GatorGrader
Students and instructors normally use GatorGrader through its Gradle
plugin, specifying the requested
checks in a config/gatorgrader.yml file. When run through Gradle, GatorGrader
reports each check that it performed, additionally sharing a diagnostic message
for each check that did not pass. Individuals who want to run GatorGrader as a
stand-alone Python application should first install it's application
dependencies with Pipenv and then learn about the supported checks and their
defaults by typing pipenv run python3 gatorgrader.py --help in a terminal
window.
Instructors often run GatorGrader in conjunction with other tools that check
source code and technical writing. For instance, in a Java-based introductory
course, instructors could verify student submissions with
Checkstyle, thereby ensuring that
the Java source code adheres to the requirements in the Google Java Style
Guide. In this course, an
instructor could require that Markdown files with technical writing meet the
standards described in the Markdown Syntax
Guide, meaning that all
Markdown files must pass the checks performed by the Markdown linting
tool. These assignments could
also require that all submitted technical writing must adhere to the standards
set by the Proselint tool. Since GatorGrader can run an
arbitrary command and check its error code, it is also possible to integrate it
with a wide variety of other linters, code formatters, and testing tools.
Instructors may at times need to see a full list of checks to have a better understanding
and therefore, we feel that it is important to know that there is an easy way for that to happen.
This action will be completed through command line and therefore, you can type
pipenv run python gatorgrader.py ListChecks into your terminal. This allows for
all of the checks to be printed out as output. This output will have the necessary
name labeled with the required and optional arguments. If this output does not give enough content,
we warmly invite you to navigate to our website, where we go into more detail about our Automated Checks.
Using Docker
A vital part of our process for GatorGrader is to implement and use new techniques
to further our tool to grow. This is why we chose to use Docker! Docker is a container
platform and therefore, allows students using GatorGrader to just open a container
and have easy access to run all commands that would allow them to build, run, and
grade their labs and practicals. Docker is an industry standard and therefore,
gives us an advantage. To open a container that will allow for the use of GatorGrader,
run the following command in your terminal window:
docker run -it --rm --name dockagator \
  -v ""$(pwd)"":/project \
  -v ""$HOME/.dockagator"":/root/.local/share \
  gatoreducator/dockagator /bin/bash

From here, you are set! Test it out by building, running, or grading your lab/practical!
If you would like to learn more about Docker, please follow this link.
Comparison to Other Tools
Other automated grading tools include:

autograde-github-classroom: ""scripts to download and grade submissions to Github Classroom""
check50: ""a tool for checking student code""
Classroom Assistant: ""desktop application to help you get student repositories for grading""
nbgrader: ""a system for assigning and grading notebooks""
nerfherder: ""scripts for automating grading with GitHub Classroom and Moodle""
Submitty: ""homework submission, automated grading, and TA grading system""
WebCat: ""all-in-one plugin for full processing and feedback generation""

Designed for instructors who want an alternative to simple scripts or
stand-alone platforms that do not integrate with industry-standard
infrastructure like GitHub and Travis CI, GatorGrader is a tool that
automatically checks the work of technical writers and programmers. Unlike other
systems, GatorGrader provides a ""batteries included"" approach to automated
grading that makes it easy for instructors to combine well-tested checks for
projects that students implement in a wide variety of programming languages.
GatorGrader's developers take its engineering seriously, maintaining
standards-compliant source code, a test suite with 100% statement and branch
coverage, and top-notch source code and user documentation.
Presentations
GatorGrader's creators give presentations about the development, use, and
assessment of the tool. Please contact one of the developers if you would like
to feature a presentation about GatorGrader at your technical conference. The
following list includes some of our team's recent presentations:

A Hands-on Guide to Teaching Programming with GitHub, Travis CI, and Python  at PyOhio 2018
Using GitHub, Travis CI, and Python to Introduce Collaborative Software Development  at PyCon Education Summit 2018
Using Python, Travis CI, and GitHub to Effectively Teach Programming  at PyGotham 2018

Contributing
Are you interested in contributing to
GatorGrader,
GatorGradle, or any of the
sample assignments (e.g.,
Java,
LaTeX, or HTML
with CSS)? Great,
because we appreciate the involvement of new contributors! Before you raise an
issue or start to make a contribution to GatorGrader's repository, we ask that
you review the project's code of conduct and the
contribution guidelines.
Contributors
Thanks goes to these wonderful people (emoji key):


Gregory M. Kapfhammer📢 💻 📖 🎨 🚇 ⚠️
Saejin Mahlau-Heinert💻 📖 🚇 👀
Christian Walker🖋
Andrew Everitt💻 📖
Christian Lussier💻 📖
Simon Burrows💻
Austin Bristol💻
Matt💻 📖
Christopher Miller💻 🎨 📖
Spencer Huang💻 📖


Mohammad Khan💻
Zachary Shaffer💻 📖
Alexander Yarkosky🐛 📖
Dillon📖
Zachary Leonardo💻
Jonathan W. Mendez💻 📖
Tyler Lyle💻
finneyj2💻 📖
schultzh🐛 💻 📖 👀
alexheinle🐛 💻 📖 👀


Zachary Andrews💻 🎨 📖
Nicholas Tocci📖
Devin Ho📖
Matthew Baldeosingh📖
Jordan Durci💻
Karol Vargas📖
Jerfenson Cerda Mejia💻 📖
Tara Douglass💻 📖
Alexander Butler💻
corlettim🐛 💻 📖 👀


Carson Quigley💻
Joshua Yee💻 📖
Madelyn Kapfhammer📖 🚇 ⚠️


This project follows the
all-contributors
specification. Contributions of any kind are welcome!
",batch2,8:09:16,Done
245,dusenberrymw/IBM-SystemML,"SystemML
SystemML is a flexible, scalable machine learning (ML) language written in Java.
SystemML's distinguishing characteristics are: (1) algorithm customizability,
(2) multiple execution modes, including Standalone, Hadoop Batch, and Spark Batch,
and (3) automatic optimization.
Algorithm Customizability
ML algorithms in SystemML are specified in a high-level, declarative machine learning (DML) language.
Algorithms can be expressed in either an R-like syntax or a Python-like syntax. DML includes
linear algebra primitives, statistical functions, and additional constructs.
This high-level language significantly increases the productivity of
data scientists as it provides (1) full flexibility in expressing custom
analytics and (2) data independence from the underlying input formats and
physical data representations.
Multiple Execution Modes
SystemML computations can be executed in a variety of different modes. To begin with, SystemML
can be operated in Standalone mode on a single machine, allowing data scientists to develop
algorithms locally without need of a distributed cluster. Algorithms can be distributed across Hadoop or Spark.
This flexibility allows the utilization of an organization's existing resources and expertise. In addition, SystemML
can be operated via Java, Scala, and Python. SystemML also features an embedded API for scoring models.
Automatic Optimization
Algorithms specified in DML are dynamically compiled and optimized based on data and cluster characteristics
using rule-based and cost-based optimization techniques. The optimizer automatically generates hybrid runtime
execution plans ranging from in-memory single-node execution to distributed computations on Spark or Hadoop.
This ensures both efficiency and scalability. Automatic optimization reduces or eliminates the need to hand-tune
distributed runtime execution plans and system configurations.

Building SystemML
SystemML is built using Apache Maven.
SystemML will build on Windows, Linux, or MacOS and requires Maven 3 and Java 7 (or higher).
To build SystemML, run:
mvn clean package


Testing SystemML
SystemML features a comprehensive set of integration tests. To perform these tests, run:
cd system-ml
mvn verify 

Note: these that these tests requires R to be installed and available as part of the PATH variable on the machine you are running these tests.
To install required packages for running integration tests, execute following command in R:
install.packages(c(""batch"", ""bitops"", ""boot"", ""caTools"", ""data.table"", ""doMC"", ""doSNOW"", ""ggplot2"", ""glmnet"", ""lda"", ""Matrix"", ""matrixStats"", ""moments"", ""plotrix"", ""psych"", ""reshape"", ""topicmodels"", ""wordcloud"", ""methods""), dependencies=TRUE) 

Known issue: package 'methods' is not available for R version 3.2.1. In which case, please downgrade R to version 3.1.1.

Algorithms
SystemML features a suite of algorithms that can be grouped into five broad categories:
Descriptive Statistics, Classification, Clustering, Regression, and Matrix Factorization. Detailed descriptions of
these algorithms can be found in the Algorithm Reference packaged with SystemML.

Linear Regression Example
As an example of the capabilities and power of SystemML and DML, let's consider the Linear Regression algorithm.
We require sets of data to train and test our model. To obtain this data, we can either use real data or
generate data for our algorithm. The UCI Machine Learning Repository Datasets
is one location for real data. Use of real data typically involves some degree of data wrangling. In the following
example, we will use SystemML to generate random data to train and test our model.
This example consists of the following parts:

Create DML Script to Generate Random Data
Run DML Script to Generate Random Data
Divide Generated Data into Two Sample Groups
Split Label Column from First Sample
Split Label Column from Second Sample
Train Model on First Sample
Test Model on Second Sample

SystemML is distributed in several packages, including a standalone package.
We'll operate in Standalone mode in this example.
If you unpack the .tar.gz file, Standalone mode
can be executed either on Mac/Unix using the runStandaloneSystemML.sh script or on Windows using the
runStandaloneSystemML.bat file.

Create DML Script to Generate Random Data
Below, we have a genLinearRegressionData.dml script. In takes several named input parameters.
It generates a matrix of random data with a label column appended to this data matrix.
#
# This script generates random data for linear regression. A matrix is generated
# consisting of a data matrix with a label column appended to it.
#
# INPUT PARAMETERS:
# --------------------------------------------------------------------------------------------
# NAME            TYPE    DEFAULT  MEANING
# --------------------------------------------------------------------------------------------
# numSamples      Int     ---      Number of samples
# numFeatures     Int     ---      Number of features (independent variables)
# maxFeatureValue Int     ---      Maximum feature value (absolute value)
# maxWeight       Int     ---      Maximum weight (absolute value)
# addNoise        Boolean ---      Determines whether noise should be added to Y
# b               Double  ---      Intercept
# sparsity        Double  ---      Controls the sparsity in the generated data (a value between 0 and 1)
# output          String  ---      Location to write the generated data/label matrix
# format          String  ---      Matrix output format
# --------------------------------------------------------------------------------------------
# OUTPUT: Matrix of random data with appended label column
# ---------------------------------------------------------------------------------------------
#
# Example
# ./runStandaloneSystemML.sh algorithms/datagen/genLinearRegressionData.dml -nvargs numSamples=1000 numFeatures=50 maxFeatureValue=5 maxWeight=5 addNoise=FALSE b=0 sparsity=0.7 output=linRegData.csv format=csv
#

X = Rand(cols=$numFeatures, max=1, min=-1, pdf=""uniform"", rows=$numSamples, seed=0, sparsity=$sparsity)
X = X * $maxFeatureValue

w = Rand(cols=1, max=1, min=-1, pdf=""uniform"", rows=$numFeatures, seed=0)
w = w * $maxWeight

Y = X %*% w
Y = Y + $b

if ($addNoise == TRUE) {
    noise = Rand(cols=1, pdf=""normal"", rows=$numSamples, seed=0)
    Y = Y + noise
}

Z = append(X,Y)
write(Z, $output, format=$format)


Run DML Script to Generate Random Data
We can execute the genLinearRegressionData.dml script in Standalone mode using either the runStandaloneSystemML.sh or runStandaloneSystemML.bat file.
In this eample, we'll generate a matrix of 1000 rows of 50 columns of test data, with sparsity 0.7. In addition to this, a 51st column consisting of labels will
be appended to the matrix.
./runStandaloneSystemML.sh algorithms/datagen/genLinearRegressionData.dml -nvargs numSamples=1000 numFeatures=50 maxFeatureValue=5 maxWeight=5 addNoise=FALSE b=0 sparsity=0.7 output=linRegData.csv format=csv

This generates the following files:
linRegData.csv      # 1000 rows of 51 columns of doubles (50 data columns and 1 label column), csv format
linRegData.csv.mtd  # metadata file


Divide Generated Data into Two Sample Groups
Next, we'll create two subsets of the generated data, each of size ~50%. We can accomplish this using the sample.dml script.
This script will randomly sample rows from the linRegData.csv file and place them into 2 files.
To do this, we need to create a csv file for the sv named argument (see sample.dml for more details),
which I called perc.csv. Based on the information in
sample.dml, I created a perc.csv file containing the following:
0.5
0.5

This will create two sample groups of roughly 50 percent each. In addition, we need a perc.csv.mtd file, so I
copied the contents of another metadata file and modified the contents so that we have 2 rows and 1 column.
{ 
    ""data_type"": ""matrix""
    ,""value_type"": ""double""
    ,""rows"": 2
    ,""cols"": 1
    ,""format"": ""csv""
    ,""header"": false
    ,""sep"": "",""
    ,""description"": { ""author"": ""SystemML"" } 
}

Now, the sample.dml script can be run.
./runStandaloneSystemML.sh algorithms/utils/sample.dml -nvargs X=linRegData.csv sv=perc.csv O=linRegDataParts ofmt=csv

This script creates two partitions of the original data and places them in a linRegDataParts folder. The files created are
as follows:
linRegDataParts/1       # first partition of data, ~50% of rows of linRegData.csv, csv format
linRegDataParts/1.mtd   # metadata
linRegDataParts/2       # second partition of data, ~50% of rows of linRegData.csv, csv format
linRegDataParts/2.mtd   # metadata

The 1 file contains the first partition of data, and the 2 file contains the second partition of data.
An associated metadata file describes
the nature of each partition of data. If we open 1 and 2 and look at the number of rows, we can see that typically
the partitions are not exactly 50% but instead are close to 50%. However, we find that the total number of rows in the
original data file equals the sum of the number of rows in 1 and 2.

Split Label Column from First Sample
The next task is to split the label column from the first sample. We can do this using the splitXY.dml script.
./runStandaloneSystemML.sh algorithms/utils/splitXY.dml -nvargs X=linRegDataParts/1 y=51 OX=linRegData.train.data.csv OY=linRegData.train.labels.csv ofmt=csv

This splits column 51, the label column, off from the data. When done, the following files have been created.
linRegData.train.data.csv        # training data of 50 columns, csv format
linRegData.train.data.csv.mtd    # metadata
linRegData.train.labels.csv      # training labels of 1 column, csv format
linRegData.train.labels.csv.mtd  # metadata


Split Label Column from Second Sample
We also need to split the label column from the second sample.
./runStandaloneSystemML.sh algorithms/utils/splitXY.dml -nvargs X=linRegDataParts/2 y=51 OX=linRegData.test.data.csv OY=linRegData.test.labels.csv ofmt=csv

This splits column 51 off the data, resulting in the following files:
linRegData.test.data.csv        # test data of 50 columns, csv format
linRegData.test.data.csv.mtd    # metadata
linRegData.test.labels.csv      # test labels of 1 column, csv format
linRegData.test.labels.csv.mtd  # metadata


Train Model on First Sample
Now, we can train our model based on the first sample. To do this, we utilize the LinearRegDS.dml (Linear Regression
Direct Solve) script. Note that SystemML also includes a LinearRegCG.dml (Linear Regression Conjugate Gradient) algorithm
for situations where the number of features is large.
./runStandaloneSystemML.sh algorithms/LinearRegDS.dml -nvargs X=linRegData.train.data.csv Y=linRegData.train.labels.csv B=betas.csv fmt=csv

This will generate the following files:
betas.csv      # betas, 50 rows of 1 column, csv format
betas.csv.mtd  # metadata

The LinearRegDS.dml script generates statistics to standard output similar to the following.
BEGIN LINEAR REGRESSION SCRIPT
Reading X and Y...
Calling the Direct Solver...
Computing the statistics...
AVG_TOT_Y,-2.160284487670675
STDEV_TOT_Y,66.86434576808432
AVG_RES_Y,-3.3127468704080085E-10
STDEV_RES_Y,1.7231785003947183E-8
DISPERSION,2.963950542926297E-16
PLAIN_R2,1.0
ADJUSTED_R2,1.0
PLAIN_R2_NOBIAS,1.0
ADJUSTED_R2_NOBIAS,1.0
PLAIN_R2_VS_0,1.0
ADJUSTED_R2_VS_0,1.0
Writing the output matrix...
END LINEAR REGRESSION SCRIPT

Now that we have our betas.csv, we can test our model with our second set of data.

Test Model on Second Sample
To test our model on the second sample, we can use the GLM-predict.dml script. This script can be used for both
prediction and scoring. Here, we're using it for scoring since we include the Y named argument. Our betas.csv
file is specified as the B named argument.
./runStandaloneSystemML.sh algorithms/GLM-predict.dml -nvargs X=linRegData.test.data.csv Y=linRegData.test.labels.csv B=betas.csv fmt=csv

This generates the following statistics to standard output.
LOGLHOOD_Z,,FALSE,NaN
LOGLHOOD_Z_PVAL,,FALSE,NaN
PEARSON_X2,,FALSE,1.895530994504798E-13
PEARSON_X2_BY_DF,,FALSE,4.202951207327712E-16
PEARSON_X2_PVAL,,FALSE,1.0
DEVIANCE_G2,,FALSE,0.0
DEVIANCE_G2_BY_DF,,FALSE,0.0
DEVIANCE_G2_PVAL,,FALSE,1.0
LOGLHOOD_Z,,TRUE,NaN
LOGLHOOD_Z_PVAL,,TRUE,NaN
PEARSON_X2,,TRUE,1.895530994504798E-13
PEARSON_X2_BY_DF,,TRUE,4.202951207327712E-16
PEARSON_X2_PVAL,,TRUE,1.0
DEVIANCE_G2,,TRUE,0.0
DEVIANCE_G2_BY_DF,,TRUE,0.0
DEVIANCE_G2_PVAL,,TRUE,1.0
AVG_TOT_Y,1,,1.0069397725436522
STDEV_TOT_Y,1,,68.29092137526905
AVG_RES_Y,1,,-4.1450397073455047E-10
STDEV_RES_Y,1,,2.0519206226041048E-8
PRED_STDEV_RES,1,TRUE,1.0
PLAIN_R2,1,,1.0
ADJUSTED_R2,1,,1.0
PLAIN_R2_NOBIAS,1,,1.0
ADJUSTED_R2_NOBIAS,1,,1.0

We see that the STDEV_RES_Y value of the testing phase is of similar magnitude
to the value obtained from the model training phase.
For convenience, we can encapsulate our DML invocations in a single script:
#!/bin/bash

./runStandaloneSystemML.sh algorithms/datagen/genLinearRegressionData.dml -nvargs numSamples=1000 numFeatures=50 maxFeatureValue=5 maxWeight=5 addNoise=FALSE b=0 sparsity=0.7 output=linRegData.csv format=csv

./runStandaloneSystemML.sh algorithms/utils/sample.dml -nvargs X=linRegData.csv sv=perc.csv O=linRegDataParts ofmt=csv

./runStandaloneSystemML.sh algorithms/utils/splitXY.dml -nvargs X=linRegDataParts/1 y=51 OX=linRegData.train.data.csv OY=linRegData.train.labels.csv ofmt=csv

./runStandaloneSystemML.sh algorithms/utils/splitXY.dml -nvargs X=linRegDataParts/2 y=51 OX=linRegData.test.data.csv OY=linRegData.test.labels.csv ofmt=csv

./runStandaloneSystemML.sh algorithms/LinearRegDS.dml -nvargs X=linRegData.train.data.csv Y=linRegData.train.labels.csv B=betas.csv fmt=csv

./runStandaloneSystemML.sh algorithms/GLM-predict.dml -nvargs X=linRegData.test.data.csv Y=linRegData.test.labels.csv B=betas.csv fmt=csv

In this example, we've seen a small part of the capabilities of SystemML. For more detailed information,
please consult the SystemML Algorithm Reference and SystemML Language Reference.
",batch2,8:09:16,Done
246,ravigupta112/laravel-timegrid,"


timegrid








Timegrid helps contractors and customers to find the perfect meeting time through online appointments.




Features

Built with Laravel 5.3 framework for PHP
Classic and oAuth2 Sign-in/Sign-up with Socialite
Business management

Clients Addressbook
Services
Staff
Availability
Appointments


Calendar sharing through iCalendar link
Scheduling view with fullcalendar
Self-service reservation with datepicker
Basic email notifications
i18n Support
Multiple Timezones Support
Live chat with TidioChat
Admin GUI with AdminLTE Twitter Bootstrap 3 based theme.

Future features here
Documentation

User Manual (work-in-progress and help is welcome!)

Live Demo
Test drive the beta live demo

HINT: You may use a maildrop.cc fake account for testing

Installing
Read the INSTALLING section.
Get started in 10 min with a Docker image for development environment.
Localization
Current supported user interface languages are:

American English (en_US)
Spanish (es_ES and es_AR)
Italian (it_IT)

Future targeted translations might be French and Arabic, however, feel free to contribute with your preferred
translation!
Appointment Library
Timegrid uses Concierge package for dealing with appointments.

Author
Timegrid is developed and maintained by Ariel Vallese.
Contributing
Contributions are welcome. Please read the following notes.
Author Notes
IMPORTANT NOTE
Dear tiny but human-qualified community,
Timegrid was born as a pet project and aimed to solve my own needs as a freelancer.
I'm also happy to hear that timegrid helped to solve a problem for others, at least partially.
That said, I'm sorry to mention that currently overloaded with other projects, I won't be
able to keep up on timegrid development on a fulltime basis like before.
PR's and other contributions, as well as regular maintenance will still take place.
Thanks for your patience and thanks for being around.
Special Thanks & Credits

PeGa! for infra support
Mohamed G.Hafez for contributions
John Ezekiel for friendly hints and creating a really nice booking-app
Victor for testing and documentation contributions
Mohammad Hossein Mojtahedi for doc review
Jose V Herrera for contributions
Bruno Gangemi for useful feedback
Calvin Roger S. Canas for contributions
Khouadja Achraf for contributions
Nick N. Huynh for contributions
Kashyap Sharma for contributions
Niharika Khanna for contributions
Webearit.com for contribution on Italian translation
Draganrakovic for contributions
Nerxo for contributions
Sahil Sharma for contributions and smart suggestions
Using modified icon originally made by SimpleIcon from www.flaticon.com

License
Timegrid is open-sourced software licensed under the AGPL
May all beings be happy.
",batch2,8:10:17,Done
247,kernsuite-debian/git-buildpackage,"Git-buildpackage
Tools to ease the development of Debian and (partially) RPM packages in git
repositories.  For documentation see the manual online at:
https://gbp.sigxcpu.org/manual/
On Debian systems, the documentation can be found in
/usr/share/doc/git-buildpackage/manual-html.
The API documentation of the gbp module can be found at:
https://gbp.sigxcpu.org/apidocs/
The mailing list is at:

https://lists.sigxcpu.org/mailman/listinfo/git-buildpackage
git-buildpackage at lists.sigxcpu.org

See the HACKING document for details on contributing to gbp development. The
package is also available on Pypi at:
https://pypi.python.org/pypi/gbp/



",batch2,8:10:17,Done
248,StygianBlues/mbsebbs,,batch2,8:10:17,Done
249,steinarb/emacs-gnus-cloud-sb,,batch1,16:59:01,Done
250,starling021/uh,"Universal Radio Hacker  






The Universal Radio Hacker (URH) is a software for investigating unknown wireless protocols. Features include

hardware interfaces for common Software Defined Radios
easy demodulation of signals
assigning participants to keep overview of your data
customizable decodings to crack even sophisticated encodings like CC1101 data whitening
assign labels to reveal the logic of the protocol
automatic reverse engineering of protocol fields
fuzzing component to find security leaks
modulation support to inject the data back into the system
simulation environment to perform stateful attacks

To get started, download the official userguide (PDF), watch the demonstration videos (YouTube)
or check out the wiki for more information and supported devices. Scroll down this page to learn how to install URH on your system.
Want to stay in touch? 
If you find URH useful, please consider giving this repository a ⭐ or even donate via PayPal. We appreciate your support!
If you use URH in your research paper, please cite this WOOT'18 paper, or directly use the following BibTeX entry.


BibTeX entry for citing URH

@inproceedings {220562,
author = {Johannes Pohl and Andreas Noack},
title = {Universal Radio Hacker: A Suite for Analyzing and Attacking Stateful Wireless Protocols},
booktitle = {12th {USENIX} Workshop on Offensive Technologies ({WOOT} 18)},
year = {2018},
address = {Baltimore, MD},
url = {https://www.usenix.org/conference/woot18/presentation/pohl},
publisher = {{USENIX} Association},
}

Installation
Universal Radio Hacker can be installed via pip or using the package manager of your distribution (if included).
Below you find more specific installation instructions for:

Windows
Linux

Install via Package Manager
Generic Installation with pip (Ubuntu/Debian)
Docker Image


MacOS
Updating your installation

Updating with pip
Updating with MSI


Running from source

Windows
On Windows, URH can be installed with it's MSI Installer. No further dependencies are required.
If you get an error about missing api-ms-win-crt-runtime-l1-1-0.dll, run Windows Update or directly install KB2999226.
Linux
Install via Package Manager
URH is included in the repositories of many linux distributions such as Arch Linux, Gentoo, Fedora, openSUSE or NixOS. There is also a package for FreeBSD. If available, simply use your package manager to install URH.
Generic Installation with pip (Ubuntu/Debian)
URH you can also be installed with using python3 -m pip install urh.
In case you are running Ubuntu or Debian read on for more specific instructions.
In order to use native device backends, make sure you install the -dev package for your desired SDRs, that is libairspy-dev, libhackrf-dev, librtlsdr-dev , libuhd-dev .
If your device does not have a -dev package, e.g. LimeSDR, you need to manually create a symlink to the .so, like this:
sudo ln -s /usr/lib/x86_64-linux-gnu/libLimeSuite.so.17.02.2 /usr/lib/x86_64-linux-gnu/libLimeSuite.so
before installing URH, using:
sudo apt-get update
sudo apt-get install python3-numpy python3-psutil python3-zmq python3-pyqt5 g++ libpython3-dev python3-pip cython3
sudo pip3 install urh
Docker Image
The official URH docker image is available here.
MacOS
Using DMG
It is recommended to use at least macOS 10.14 when using the DMG available here.
With pip

Install Python 3 for Mac OS X.
If you experience issues with preinstalled Python, make sure you update to a recent version using the given link.
(Optional) Install desired native libs e.g. brew install librtlsdr for
corresponding native device support.
In a terminal, type: pip3 install urh.
Type urh in a terminal to get it started.

Update your installation
Updating with pip
If you installed URH via pip you can keep it up to date with pip3 install --upgrade urh, or, if this should not work python3 -m pip install --upgrade urh.
Updating with MSI
If you experience issues after updating URH using the .msi installer on Windows, please perform a full uninstallation. That is, uninstall URH via Windows and after that remove the installation folder (something like C:\Program Files\Universal Radio Hacker). Now, install the new version using the recent .msi .
Running from source
If you like to live on bleeding edge, you can run URH from source.
Without installation
To execute the Universal Radio Hacker without installation, just run:
git clone https://github.com/jopohl/urh/
cd urh/src/urh
./main.py
Note, before first usage the C++ extensions will be built.
Installing from source
To install from source you need to have python-setuptools installed. You can get it e.g. with pip install setuptools.
Once the setuptools are installed use:
git clone https://github.com/jopohl/urh/
cd urh
python setup.py install
And start the application by typing urh in a terminal.
Articles
Hacking stuff with URH

Hacking Burger Pagers
Reverse-engineer and Clone a Remote Control
Reverse-engineering Weather Station RF Signals
Reverse-engineering Wireless Blinds
Attacking Logitech Wireless Presenters (German Article)
Attacking Wireless Keyboards
Reverse-engineering a 433MHz Remote-controlled Power Socket for use with Arduino

General presentations and tutorials on URH

Hackaday Article
RTL-SDR.com Article
Short Tutorial on URH with LimeSDR Mini
Brute-forcing a RF Device: a Step-by-step Guide

External decodings
See wiki for a list of external decodings provided by our community! Thanks for that!
Screenshots
Get the data out of raw signals

Keep an overview even on complex protocols

Record and send signals

",batch2,8:10:18,Done
251,justots/forgottenserver,"forgottenserver   
The Forgotten Server is a free and open-source MMORPG server emulator written in C++. It is a fork of the OpenTibia Server project. To connect to the server, you can use OTClient or OpenTibiaUnity.
Getting Started

Compiling, alternatively download AppVeyor builds for Windows
Scripting Reference

Support
If you need help, please visit the support forum on OTLand. Our issue tracker is not a support forum, and using it as one will result in your issue being closed. If you were unable to get assistance in the support forum, you should consider becoming a premium user on OTLand which grants you access to the premium support forum and supports OTLand financially.
Issues
We use the issue tracker on GitHub. Keep in mind that everyone who is watching the repository gets notified by e-mail when there is activity, so be thoughtful and avoid writing comments that aren't meaningful for an issue (e.g. ""+1""). If you'd like for an issue to be fixed faster, you should either fix it yourself and submit a pull request, or place a bounty on the issue.
",batch2,8:10:17,Done
252,kfowler/emacs,,batch1,16:58:59,Done
253,raxod502/emacs,,batch1,16:59:00,Done
254,DingZefang/buginn,"test
",batch2,8:10:17,Done
255,wahjava/emacs,,batch1,16:59:00,Done
256,Yixiao-Han/my-cmssw,,batch1,16:57:58,Done
257,Novi890s/doraeminbot1,"
Xiao





This bot is not available for invite.

Xiao is a Discord bot coded in JavaScript with
discord.js using the
Commando command framework. With over
300 commands, she is one of the most feature-rich bots out there.
Installing
Before You Begin

Make sure you have installed Node.js >=10 and Git.
Clone this repository with git clone https://github.com/dragonfire535/xiao.git.
Run cd xiao to move into the folder that you just created.

Windows

Open an ADMIN POWERSHELL window and run npm i -g --production windows-build-tools.
Follow these instructions to install the dependencies for node-canvas.
Run npm i --production in the folder you cloned the bot.
Run npm i -g pm2 to install PM2.
Run pm2 start Xiao.js --name xiao to run the bot.

Mac

Use a real (cheaper!) OS to host your bot.
???
Profit.

Ubuntu and other Debian-based systems

Run apt update.
Run apt upgrade to install the latest dependencies of your distro.
Run apt install python to install python.
Follow these instructions to install the dependencies for node-canvas.
Run npm i --production in the folder you cloned the bot.
Run npm i -g pm2 to install PM2.
Run pm2 start Xiao.js --name xiao to run the bot.

Commands (341)
Utility:

eval: Executes JavaScript code.
changelog: Responds with the bot's latest 10 commits.
credit: Responds with a command's credits list.
donate: Responds with the bot's donation links.
help: Displays a list of available commands, or detailed information for a specific command.
info: Responds with detailed bot information.
invite: Responds with the bot's invite links.
ping: Checks the bot's ping to the Discord server.

Discord Information:

avatar: Responds with a user's avatar.
channel: Responds with detailed information on a channel.
discriminator: Searches for other users with a certain discriminator.
emoji-image: Responds with an emoji's full-scale image.
emoji-list: Responds with a list of the server's custom emoji.
emoji: Responds with detailed information on an emoji.
first-message: Responds with the first message ever sent to a channel.
id: Responds with a user's ID.
message-source: Responds with a codeblock containing a message's contents.
message: Responds with detailed information on a message.
role: Responds with detailed information on a role.
server: Responds with detailed information on the server.
user: Responds with detailed information on a user.

Random Response:

8-ball: Asks your question to the Magic 8 Ball.
advice: Responds with a random bit of advice.
bird: Responds with a random image of a bird.
cat-fact: Responds with a random cat fact.
cat: Responds with a random cat image.
charlie-charlie: Asks your question to Charlie.
choose: Chooses between options you provide.
chuck-norris: Responds with a random Chuck Norris joke.
coin: Flips a coin.
compliment: Compliments a user.
dog-fact: Responds with a random dog fact.
dog: Responds with a random dog image.
draw-cards: Draws a random hand of playing cards.
duck: Responds with a random duck image.
fact-core: Responds with a random Fact Core quote.
fact: Responds with a random fact.
fidget: Responds with a random image of Fidget.
fortune: Responds with a random fortune.
fox: Responds with a random fox image.
github-zen: Responds with a random GitHub design philosophy.
joke: Responds with a random joke.
karen: Responds with a random image of Karen.
kiss-marry-kill: Determines who to kiss, who to marry, and who to kill.
magic-conch: Asks your question to the Magic Conch.
meme: Responds with a random meme.
name: Responds with a random name, with the gender of your choice.
number-fact: Responds with a random fact about a specific number.
offspring: Determines if your new child will be a boy or a girl.
opinion: Determines the opinion on something.
oracle-turret: Responds with a random Oracle Turret quote.
pun: Responds with a random pun.
quantum-coin: Flips a coin that lands on some form of nothing.
quote: Responds with a random quote.
random-user: Randomly chooses a member of the server.
rate: Rates something.
roast: Roasts a user.
roll: Rolls a dice with a maximum value of your choice.
security-key: Responds with a random security key.
shiba: Responds with a random image of a Shiba Inu.
shower-thought: Responds with a random shower thought, directly from r/Showerthoughts.
smw-level: Responds with a random Super Mario World level name.
subreddit: Responds with a random post from a subreddit.
suggest-command: Suggests a random command for you to try.
superpower: Responds with a random superpower.
this-for-that: So, basically, it's like a bot command for this dumb meme.
waifu: Responds with a randomly generated waifu and backstory.
would-you-rather: Responds with a random ""Would you rather ...?"" question.
xiao: Responds with a random image of Xiao Pai.

Single Response:

can-you-not: Can YOU not?
dark-light: Determines whether you use dark or light theme.
eat-pant: Eat pant.
eggs-get-laid: Sends the ultimate roast.
fly: Sends a fake fly that looks surprisngly real.
give-flower: Gives Xiao Pai a flower.
hi: Hello.
isnt-joke: Isn't joke...
its-joke: It's joke!
just-do-it: Sends a link to the ""Just Do It!"" motivational speech.
lenny: Responds with the lenny face.
no-u: no u
spam: Responds with a picture of Spam.
tableflip: Flips a table... With animation!
wynaut: Why not? Wynaut?
yoff: Posts a picture that truly defines modern art.

Events:

apod: Responds with today's Astronomy Picture of the Day.
calendar: Responds with today's holidays.
days-until: Responds with how many days there are until a certain date.
doomsday-clock: Responds with the current time of the Doomsday Clock.
google-doodle: Responds with a Google Doodle, either the latest one or a random one from the past.
horoscope: Responds with today's horoscope for a specific Zodiac sign.
humble-bundle: Responds with the current Humble Bundle.
is-tuesday: Determines if today is Tuesday.
neko-atsume-password: Responds with today's Neko Atsume password.
time: Responds with the current time in a particular location.
today-in-history: Responds with an event that occurred today in history.

Search:

anime: Searches AniList for your query, getting anime results.
azur-lane: Responds with information on an Azur Lane ship.
book: Searches Google Books for a book.
bulbapedia: Searches Bulbapedia for your query.
character: Searches AniList for your query, getting character results.
danbooru: Responds with an image from Danbooru, with optional query.
define: Defines a word.
derpibooru: Responds with an image from Derpibooru.
deviantart: Responds with an image from a DeviantArt section, with optional query.
docs: Searches the Discord.js docs for your query.
esrb: Searches ESRB for your query.
flickr: Searches Flickr for your query.
giphy: Searches Giphy for your query.
github: Responds with information on a GitHub repository.
google-autofill: Responds with a list of the Google Autofill results for a particular query.
google: Searches Google for your query.
gravatar: Responds with the Gravatar for an email.
http-cat: Responds with a cat for an HTTP status code.
http-dog: Responds with a dog for an HTTP status code.
imgur: Searches Imgur for your query.
itunes: Searches iTunes for your query.
jisho: Defines a word, but with Japanese.
kickstarter: Searches Kickstarter for your query.
know-your-meme: Searches Know Your Meme for your query.
konachan: Responds with an image from Konachan, with optional query.
league-of-legends: Responds with information on a League of Legends champion.
manga: Searches AniList for your query, getting manga results.
map: Responds with a map of a specific location.
mayo-clinic: Searches Mayo Clinic for your query.
mdn: Searches MDN for your query.
movie: Searches TMDB for your query, getting movie results.
nasa: Searches NASA's image archive for your query.
neopet: Responds with the image of a specific Neopet.
neopets-item: Responds with information on a specific Neopets item.
npm: Responds with information on an NPM package.
osu: Responds with information on an osu! user.
periodic-table: Finds an element on the periodic table.
pokedex: Searches the Pokédex for a Pokémon.
recipe: Searches for recipes based on your query.
reddit: Responds with information on a Reddit user.
rotten-tomatoes: Searches Rotten Tomatoes for your query.
rule: Responds with a rule of the internet.
safebooru: Responds with an image from Safebooru, with optional query.
soundcloud: Searches SoundCloud for your query.
stack-overflow: Searches Stack Overflow for your query.
steam: Searches Steam for your query.
stocks: Responds with the current stocks for a specific symbol.
tenor: Searches Tenor for your query.
tumblr: Responds with information on a Tumblr blog.
tv-show: Searches TMDB for your query, getting TV show results.
twitter: Responds with information on a Twitter user.
urban: Defines a word, but with Urban Dictionary.
visual-novel: Responds with information on a Visual Novel.
vocadb: Searches VocaDB for your query.
wattpad: Searches Wattpad for your query.
weather: Responds with weather information for a specific location.
wikia: Searches a specific Wikia wiki for your query.
wikihow: Searches Wikihow for your query.
wikipedia: Searches Wikipedia for your query.
xkcd: Responds with an XKCD comic, either today's, a random one, or a specific one.
youtube: Searches YouTube for your query.
yu-gi-oh: Responds with info on a Yu-Gi-Oh! card.

Analyzers:

age: Responds with how old someone born in a certain year is.
butt: Determines a user's butt quality.
character-count: Responds with the character count of text.
chinese-zodiac: Responds with the Chinese Zodiac Sign for the given year.
coolness: Determines a user's coolness.
dick: Determines your dick size.
face: Determines the race, gender, and age of a face.
gender: Determines the gender of a name.
guess-looks: Guesses what a user looks like.
iq: Determines a user's IQ.
psycho-pass: Determines your Crime Coefficient.
read-qr-code: Reads a QR Code.
scrabble-score: Responds with the scrabble score of a word.
severe-toxicity: Determines the toxicity of text, but less sensitive to milder language.
ship: Ships two users together.
toxicity: Determines the toxicity of text.
what-anime: Determines what anime a screenshot is from.
zodiac-sign: Responds with the Zodiac Sign for the given month/day.

Games:

balloon-pop: Don't let yourself be the last one to pump the balloon before it pops!
battle: Engage in a turn-based battle against another user or the AI.
blackjack: Play a game of blackjack.
box-choosing: Do you believe that there are choices in life? Taken from Higurashi Chapter 4.
captcha: Try to guess what the captcha says.
chance: Attempt to win with a 1 in 1000 (or your choice) chance of winning.
doors: Open the right door, and you win the money! Make the wrong choice, and you get the fire!
emoji-emoji-revolution: Can you type arrow emoji faster than anyone else has ever typed them before?
fishy: Go fishing.
google-feud: Attempt to determine the top suggestions for a Google search.
gunfight: Engage in a western gunfight against another user. High noon.
hangman: Prevent a man from being hanged by guessing a word as fast as you can.
hunger-games: Simulate a Hunger Games match with up to 24 tributes.
lottery: Attempt to win the lottery with 6 numbers.
mafia: Who is the Mafia? Who is the doctor? Who is the detective? Will the Mafia kill them all?
math-quiz: See how fast you can answer a math problem in a given time limit.
quiz-duel: Answer a series of quiz questions against an opponent.
quiz: Answer a quiz question.
rock-paper-scissors: Play Rock-Paper-Scissors.
roulette: Play a game of roulette.
slots: Play a game of slots.
sorting-hat: Take a quiz to determine your Hogwarts house.
tic-tac-toe: Play a game of tic-tac-toe with another user.
typing-test: See how fast you can type a sentence in a given time limit.
whos-that-pokemon: Guess who that Pokémon is.
wizard-convention: Who is the Dragon? Who is the healer? Who is the mind reader? Will the Dragon eat them all?
word-chain: Try to come up with words that start with the last letter of your opponent's word.

Image Manipulation:

achievement: Sends a Minecraft achievement with the text of your choice.
approved: Draws an ""approved"" stamp over an image or a user's avatar.
be-like-bill: Sends a ""Be Like Bill"" meme with the name of your choice.
brazzers: Draws an image with the Brazzers logo in the corner.
circle: Draws an image or a user's avatar as a circle.
color: Sends an image of the color you choose.
contrast: Draws an image or a user's avatar but with contrast.
create-qr-code: Converts text to a QR Code.
demotivational: Draws an image or a user's avatar and the text you specify as a demotivational poster.
distort: Draws an image or a user's avatar but distorted.
fire: Draws a fiery border over an image or a user's avatar.
frame: Draws a frame around an image or a user's avatar.
glitch: Draws an image or a user's avatar but glitched.
greyscale: Draws an image or a user's avatar in greyscale.
gru-plan: Sends a Gru's Plan meme with steps of your choice.
ifunny: Draws an image with the iFunny logo.
illegal: Makes President Trump make your text illegal.
invert: Draws an image or a user's avatar but inverted.
kyon-gun: Draws an image or a user's avatar behind Kyon shooting a gun.
lisa-presentation: Sends a ""Lisa Presentation"" meme with the presentation of your choice.
meme-gen: Sends a meme with the text and background of your choice.
minecraft-skin: Sends the Minecraft skin for a user.
needs-more-jpeg: Draws an image or a user's avatar as a low quality JPEG.
new-password: Sends a ""Weak Password/Strong Password"" meme with the passwords of your choice.
nike-ad: Sends a ""Believe in Something"" Nike Ad meme with the text of your choice.
osu-signature: Creates a card based on an osu! user's stats.
pixelize: Draws an image or a user's avatar pixelized.
pokemon-fusion: Fuses two Generation I Pokémon together.
rainbow: Draws a rainbow over an image or a user's avatar.
rejected: Draws a ""rejected"" stamp over an image or a user's avatar.
robohash: Creates a robot based on the text you provide.
sepia: Draws an image or a user's avatar in sepia.
shields-io-badge: Creates a badge from shields.io.
silhouette: Draws a silhouette of an image or a user's avatar.
sora-selfie: Draws an image or a user's avatar behind Sora taking a selfie.
square: Draws an image or a user's avatar as a square.
thug-life: Draws ""Thug Life"" over an image or a user's avatar.
tint: Draws an image or a user's avatar but tinted a specific color.
to-be-continued: Draws an image with the ""To Be Continued..."" arrow.
vietnam-flashbacks: Edits Vietnam flashbacks behind an image or a user's avatar.

Avatar Manipulation:

3000-years: Draws a user's avatar over Pokémon's ""It's been 3000 years"" meme.
avatar-fusion: Draws a a user's avatar over a user's avatar.
beautiful: Draws a user's avatar over Gravity Falls' ""Oh, this? This is beautiful."" meme.
bob-ross: Draws a user's avatar over Bob Ross' canvas.
challenger: Draws a user's avatar over Super Smash Bros.'s ""Challenger Approaching"" screen.
dexter: Draws a user's avatar over the screen of Dexter from Pokémon.
distracted-boyfriend: Draws three user's avatars over the ""Distracted Boyfriend"" meme.
drakeposting: Draws two user's avatars over the ""Drakeposting"" meme.
food-broke: Draws a user's avatar over the ""Food Broke"" meme.
girl-worth-fighting-for: Draws a user's avatar as the object of Ling's affection.
hat: Draws a hat over a user's avatar.
he-lives-in-you: Draws a user's avatar over Simba from The Lion King's reflection.
hearts: Draws hearts around a user's avatar.
i-have-the-power: Draws a user's avatar over He-Man's face.
look-at-this-photograph: Draws a user's avatar over Nickelback's photograph.
look-what-karen-have: Draws a user's avatar over Karen's piece of paper.
rip: Draws a user's avatar over a gravestone.
sip: Draws a user's avatar sipping tea.
steam-card: Draws a user's avatar on a Steam Trading Card.
steam-now-playing: Draws a user's avatar and the game of your choice over a Steam ""now playing"" notification.
triggered: Draws a user's avatar over the ""Triggered"" meme.
ultimate-tattoo: Draws a user's avatar as ""The Ultimate Tattoo"".
wanted: Draws a user's avatar over a wanted poster.
worthless: Draws a user's avatar over Gravity Falls' ""Oh, this? This is worthless."" meme.
yu-gi-oh-token: Draws a user's avatar over a blank Yu-Gi-Oh! Token card.

Text Manipulation:

base64: Converts text to/from Base64.
binary: Converts text to binary.
braille: Converts text to braille.
brony-speak: Converts text to brony speak.
clap: Sends 👏 text 👏 like 👏 this.
cow-say: Makes a cow say your text.
cursive: Converts text to cursive.
dvorak: Converts text to Dvorak encoding.
embed: Sends text in an embed.
emojify: Converts text to emoji form.
fancy: Converts text to fancy letters.
hex: Converts text to hex.
latlmes: Creates a Latlmes fake link that redirects to a rickroll.
lmgtfy: Creates a LMGTFY link with the query you provide.
lowercase: Converts text to lowercase.
md5: Creates a hash of text with the MD5 algorithm.
mocking: SenDs TexT lIkE ThiS.
morse: Converts text to morse code.
nobody-name: Converts a name into the Organization XIII style.
owo: OwO.
pig-latin: Converts text to pig latin.
pirate: Converts text to pirate.
portal-send: Send a message to a portal channel.
repeat: Repeat text over and over and over and over (etc).
reverse: Reverses text.
say: Make me say what you want, master.
sha-1: Creates a hash of text with the SHA-1 algorithm.
sha-256: Creates a hash of text with the SHA-256 algorithm.
ship-name: Creates a ship name from two names.
shuffle: Shuffles text.
snake-speak: Convertsssss text to sssssnake ssssspeak.
spoiler-letter: Sends text with each and every character as an individual spoiler.
superscript: Converts text to tiny text.
tebahpla: Reverses the alphabet of text.
temmie: Converts text to Temmie speak.
translate: Translates text to a specific language.
unspoiler: Removes all spoilers from text.
uppercase: Converts text to uppercase.
upside-down: Flips text upside-down.
url-encode: Encodes text to URL-friendly characters.
webhook: Posts a message to the webhook defined in the bot owner's process.env.
yoda: Converts text to Yoda speak.
zalgo: Converts text to zalgo.

Number Manipulation:

currency: Converts currency from one currency to another.
final-grade: Determines the grade you need to make on your final to get your desired course grade.
grade: Determines your grade on an assignment on an 100-point scale.
gravity: Determines weight on another planet.
math: Evaluates a math expression.
prime: Determines if a number is a prime number.
roman: Converts a number to roman numerals.
scientific-notation: Converts a number to scientific notation.
units: Converts units to/from other units.

Other:

cleverbot: Talk to Cleverbot.
prune: Deletes up to 99 messages from the current channel.
rename-all: Renames every member of the server.
strawpoll: Generates a Strawpoll with the options you provide.

Roleplay:

blush: Blushes at a user.
bro-hoof: Gives a user a bro hoof.
eat: Feeds a user.
fist-bump: Fist-bumps a user.
high-five: High Fives a user.
hold-hands: Holds hands with a user.
hug: Hugs a user.
kill: Kills a user.
kiss: Kisses a user.
pat: Pats a user.
poke: Pokes a user.
punch: Punches a user.
slap: Slaps a user.
sleep: Puts a user to sleep.
wake-up: Wakes up a user.
wave: Waves at a user.
wink: Winks at a user.

Licensing
The bot is licensed under the GPL 3.0 license. See the file LICENSE for more
information. If you plan to use any part of this source code in your own bot, I
would be grateful if you would include some form of credit somewhere.
",batch2,8:09:16,Done
258,sollidsnake/emacs,,batch1,16:58:59,Done
259,krell/tg-elysium-xiii,"/tg/Elysium XIII

Website: https://www.elysium-xiii.fr
Code: https://github.com/krell/tg-elysium-xiii
Discord: https://discord.gg/uj9YNvA

CONTRIBUTING
Please see CONTRIBUTING.md
LICENSE
All code after commit 333c566b88108de218d882840e61928a9b759d8f on 2014/31/12 at 4:38 PM PST is licensed under GNU AGPL v3.
All code before commit 333c566b88108de218d882840e61928a9b759d8f on 2014/31/12 at 4:38 PM PST is licensed under GNU GPL v3.
(Including tools unless their readme specifies otherwise.)
See LICENSE and GPLv3.txt for more details.
tgui clientside is licensed as a subproject under the MIT license.
Font Awesome font files, used by tgui, are licensed under the SIL Open Font License v1.1
tgui assets are licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.
The TGS3 API is licensed as a subproject under the MIT license.
See tgui/LICENSE.md for the MIT license.
See tgui/assets/fonts/SIL-OFL-1.1-LICENSE.md for the SIL Open Font License.
See the footers of code/__DEFINES/server_tools.dm, code/modules/server_tools/st_commands.dm, and code/modules/server_tools/st_inteface.dm for the MIT license.
All assets including icons and sound are under a Creative Commons 3.0 BY-SA license unless otherwise indicated.
",batch1,16:58:59,Done
260,mschuwalow/emacs-mac,,batch1,16:59:00,Done
261,levants/lightmare,"lightmare
A group of Java EE based projects:
lightmare-ejb - Embeddable ejb container (works for stateless session beans) with JPA / Hibernate support
lightmare-criteria - JPA-QL query generator using lambda expressions
criteria-tester - test application for ""lightmare-criteria"" JPA-QL query generator
lightmare-utils - Utility classes for java Strings, Collections, Reflection API, ClassLoaders etc.
lightmare-tester- test application for ""lightmare-ejb"" embeddable EJB container
",batch2,8:09:15,Done
262,amerlyq/airy,"airy
Modular system configuration and package installation.

Encapsulates user settings to manage by version control systems.
Aimed for development purposes with bias to vim, tiling WM and terminals.
Automatic setup on clean system, with following update/maintenance.

Currently supported host systems:

Arch Linux (preferable)
Ubuntu-based distro (clean if possible)

Initial
TEMP: Manually symlink or mount --bind
ln -s  /data/chroots /chroot
ln -s  /home/work    /work
ln -s  ~/Downloads   /_dld
ln -s  /data/music   ~/.config/mpd/music
ALSO

encrypt your SSH keys
add SSH keys to github/gitlab

Install
If you wish to feel what it's like, default setup contains only necessary
symlinks and terminal tools, preserving your system as much as possible.
mkdir -p ~/aura && cd ~/aura
git clone https://github.com/amerlyq/airy
cd airy
make           # install basic 'airy' symlinks
make all       # clean all and then install pkgs
make continue  # to continue installation after error was fixed
Go make some tea, in one minute basic shell tools will be installed.
Private parts
You can keep your system profiles and private modules/resources in your own
private repository (gitlab.com, bitbucket.com, dropbox, etc).
Overall directories structure (in repo's root or some subdirectory) is next:
repo
├── airy
│   ├── hostname1  # home
│   ├── hostname2  # work
│   └── hostname3  # server (headless)
├── fonts
│   └── setup
├── mutt
│   └── acc
│       ├── home
│       └── work
└── Makefile

Main repo/Makefile must be launched on clean system before everything else.
It will symlink your chosen profile/modules and launches system setup.
#!/usr/bin/make -f
.DEFAULT_GOAL = install
MAKEFLAGS += -rR --silent
d_airy := $(HOME)/aura/airy
d_cfg := $(or $(AIRY_CONFIG),$(or $(XDG_CONFIG_HOME),$(HOME)/.config)/airy)
prf := $(shell pwd)/airy/$(shell hostname)
install:
	$(MAKE) -C '$(d_airy)' -- configure
	ln -svfT '$(prf)' '$(d_cfg)/profile'
In such way your own clean system setup will be as simple as:
mkdir -p ~/aura && cd ~/aura
git clone https://github.com/amerlyq/airy
git clone https://gitlab.com/$yourname/$airyprivate
cd $airyprivate
make
cd ../airy
make -B
Note, that in case of using Xorg mods, to fully setup system you must run
setup command inside Xorg session -- because some vars (like dpi)
impossible to extract from plain console. Therefore on clean system install
the setup script must be run twice -- in console and then in Xorg.
Profile
Each profile prf/hostname is written in plain bash.
These text files usually non-executable and sourced by scripts explicitly.
# vim: ft=sh
CURR_PROF=home
AIRY_MODS=( airy pacman git zsh vim ranger tmux xorg )

### Git ###
MAIN_NAME=""<Full Name>""
MAIN_MAIL=""username@gmail.com""
MAIN_SKYPE=""<username>""
Profiles can be nested/inherited.
This allows to distribute settings and nicely reuse parts of configs for similar hosts.
dprf=$(dirname ""$(readlink -e ""$BASH_SOURCE"")"")
AIRY_OVERLAY_PATH=${dprf%/*}
source ""$dprf/home"" || return
CURR_PROF=home_vbox
AIRY_SKIP+=( browser )
MAIN_MAIL=""vboxuser@email.com""
Example how to use profile vars in your own modules/scripts:
#!/bin/bash -e
source ~/.shell/profile
...
r.mutt-acc ${MAIN_MAIL:?}
",batch2,8:10:17,Done
263,RockISI/bitcoin," 

Welcome to bitcoinj
The bitcoinj library is a Java implementation of the Bitcoin protocol, which allows it to maintain a wallet and send/receive transactions without needing a local copy of Bitcoin Core. It comes with full documentation and some example apps showing how to use it.
Technologies

Java 7 for the core modules, Java 8 for everything else
Gradle 3.4+ - for building the project
Google Protocol Buffers - for use with serialization and hardware communications

Getting started
To get started, it is best to have the latest JDK and Gradle installed. The HEAD of the master branch contains the latest development code and various production releases are provided on feature branches.
Building from the command line
To perform a full build use
gradle clean build

You can also run
gradle javadoc

to generate the JavaDocs.
The outputs are under the build directory.
Building from an IDE
Alternatively, just import the project using your IDE. IntelliJ has Gradle integration built-in and has a free Community Edition. Simply use File | New | Project from Existing Sources and locate the build.gradle in the root of the cloned project source tree.
Example applications
These are found in the examples module.
Where next?
Now you are ready to follow the tutorial.
",batch2,8:09:16,Done
264,Nik-Menendez/L1Trigger,,batch1,16:57:58,Done
265,UniTime/unitime,"
UniTime
Comprehensive University Timetabling System
https://www.unitime.org
UniTime is a comprehensive educational scheduling system that supports developing
course and exam timetables, managing changes to these timetables, sharing rooms
with other events, and scheduling students to individual classes.
It is a distributed system that allows multiple university and departmental schedule managers
to coordinate efforts to build and modify a schedule that meets their diverse organizational
needs while allowing for minimization of student course conflicts. It can be used alone to
create and maintain a school's schedule of classes and/or exams, or interfaced with
an existing student information system.
The system was originally developed as a collaborative effort by faculty,
students, and staff at universities in North America and Europe. The software
is distributed free under an open source license in hopes that other colleges
and universities can benefit their students through better scheduling or wish to
contribute to ongoing research in this area. The UniTime project has become
a sponsored project of the Apereo Foundation in March 2015.
Components

Course Timetabling & Management
Examination Timetabling
Event Management
Student Scheduling

Tutorials

Installation Instructions
Building UniTime
Setting up UniTime in Eclipse
Customization
Localization

Links

UniTime 4.3 documentation
Online Documentation
Online Demo
Downloads
Nightly Builds
XML Interfaces
Publications

",batch2,8:09:15,Done
266,wrye-bash/wrye-bash,"Wrye Bash


About
Wrye Bash is a mod management utility for games based on Bethesda's Creation
Engine, with a rich set of features.
This is a fork of the Wrye Bash related code from the
SVN 3177 trunk revision.
We are in the process of refactoring the code to eventually support more games,
offering the same feature set for all of them.
Please read the Contributing section below if interested in
contributing.
Supported Games
Here is a list of supported games with the minimal patch version that Bash was
tested on (previous versions or latest versions may or may not work):

Morrowind (very early support, patch 1.6.1820)
Oblivion (patch 1.2.0.416)
Nehrim (patch 2.0.2.4)
Fallout 3 (patch 1.7.0.3)
Fallout New Vegas (patch 1.4.0.525)
Skyrim (patch 1.9.36.0)
Enderal (patch 1.6.4.0)
Fallout 4 (patch 1.10.163.0)
Skyrim Special Edition (patch 1.5.97.0)
Enderal Special Edition (patch 2.0.8)

Note: The Windows Store versions of Morrowind, Oblivion, Fallout 4 and
Skyrim Special Edition are supported as well.
Download

Oblivion Nexus
Nehrim Nexus
Fallout 3 Nexus
Fallout New Vegas Nexus
Skyrim Nexus
Enderal Nexus
Fallout 4 Nexus
Skyrim Special Edition Nexus
Enderal Special Edition Nexus
Github (all releases)

Docs are included in the download but we are setting them up also online
here.
Installation

Short version: just use the installer, and install everything to their
default locations.
Long version: see the General Readme for information, and the
Advanced Readme for even more details.

To run Wrye Bash from the latest dev code (download from here)
you need:

A game to manage from the supported games.
Python 2.7 64-bit (latest 2.7 is recommended)

NB: the 64-bit version is required. 32-bit operating systems are no
longer supported.
Once you have those, install the required packages by running:
path/to/python.exe -m pip install -r requirements.txt
Refer to the readmes linked above for detailed instructions. In short:

Install one of the supported games (Oblivion, Skyrim, Fallout).
Install Python and plugins above.
Extract the downloaded Wrye Bash archive into your game folder.
Run Wrye Bash by double-clicking ""Wrye Bash Launcher.pyw"" in the new Mopy
folder.

WINE
Since 306, Wrye Bash runs on WINE - with some hiccups. Please see our
wiki article for a detailed guide.
Relevant issue: #240
Questions ? Feedback ?
We are currently monitoring this thread at the AFK Mods forum and
the Wrye Bash Discord.
Please be sure to ask there first before reporting an issue here. If asking for
help please provide the info detailed in our Reporting a bug wiki page.
In particular it is essential you produce a bashbugdump.log.
Latest betas
In the second post of the AFK Mods thread, as well as in the
#wip-builds channel on Discord, there are links to the latest python
and standalone (exe) builds. Be sure to check those out for bleeding edge
bugfixes and enhancements. Feedback appreciated!
Contributing
Please see our dedicated Contributing.md document for information on how
to contribute.
Main Branches

dev: the main development
branch - approved commits end up here. Do not directly push to this branch -
push to your branches and contact someone from the owners team in the relevant
issue.
master: the production
branch, contains stable releases. Use it only as reference.
nightly:
bleeding edge branch. Commits land here for testing.

",batch2,8:09:15,Done
267,thornbird/dot-oh-my-zsh,"
Oh My Zsh is an open source, community-driven framework for managing your zsh configuration. That sounds boring. Let's try this again.
Oh My Zsh is a way of life! Once installed, your terminal prompt will become the talk of the town or your money back! Each time you interace with your command prompt, you'll be able take advantage of the hundreds of bundled plugins and pretty themes. Strangers will come up to you in cafés and ask you, ""that is amazing. are you some sort of genius?"" Finally, you'll begin to get the sort of attention that you always felt that you deserved. ...or maybe you'll just use the time that you saved to start flossing more often.
To learn more, visit http://ohmyz.sh and/or follow ohmyzsh on twitter.
Getting Started
Prerequisites
Disclaimer: Oh My Zsh works best on Mac OS X and Linux.

Unix-based operating system (Mac OS X or Linux)
Zsh should be installed (v4.3.9 or more recent)

This is commonly pre-installed. (zsh --version to confirm)


curl or wget should be installed

Basic Installation
Oh My Zsh is installed by running one of the following commands in your terminal. You can install this via the command-line with either curl or wget.
via curl
curl -L https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh | sh
via wget
wget https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O - | sh
Using Oh My Zsh
Plugins
Oh My Zsh comes with a shit load of plugins to take advantage of. You can take a look in the plugins directory and/or the wiki to see what's currently available.
Enabling Plugins
If you spot a plugin (or several) that you would like to use with Oh My Zsh, you will need to edit the ~/.zshrc file. Once you open it with your favorite editor, you'll see a spot to list all the plugins that you'd like Oh My Zsh to load in initialization.
For example, this line might begin to look like...
plugins=(git bundler osx rake ruby)
Using Plugins
Most plugins (should! we're working on this) include a README, which documents how to use them.
Themes
We'll admit it. Early in the Oh My Zsh world... we may have gotten a far too theme happy. We have over one hundred themes now bundled. Most of them have screenshots on the wiki. Check them out!
Selecting a Theme
Robby's theme is the default one. It's not the fanciest one. It's not the simplest one. It's just right (for him).
Once you find a theme that you want to use, you will need to edit the ~/.zshrc file. You'll see an environment variable (all caps) in there that looks like:
ZSH_THEME=""robbyrussell""
To use a different theme, simple change the value to match the name of your desired theme. For example:
ZSH_THEME=""agnoster"" (this is one of the fancy ones)
Open up a new terminal window and your prompt should look something like...
Advanced Topics
If you're the type that likes to get their hands dirty... these sections might resonate.
Advanced Installation
For those who want to install this manually and/or set custom paths.
Custom Directory
The default location is ~/.oh-my-zsh (hidden in your home directory)
If you'd like to change the install directory with the ZSH environment variable, either by running export ZSH=/your/path before installing, or by setting it before the end of the install pipeline like this:
curl -L https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh | ZSH=~/.dotfiles/zsh sh
Manual Installation
1. Clone the repository:
git clone git://github.com/robbyrussell/oh-my-zsh.git ~/.oh-my-zsh
2. Optionally, backup your existing @~/.zshrc@ file:
cp ~/.zshrc ~/.zshrc.orig
3. Create a new zsh configuration file
You can create a new zsh config file by copying the template that we included for you.
cp ~/.oh-my-zsh/templates/zshrc.zsh-template ~/.zshrc
4. Change your default shell
chsh -s /bin/zsh
5. Initialize your new zsh configuration
Once you open up a new terminal window, it should load zsh with Oh My Zsh's configuration.
Installation Problems
If you have any hiccups installing, here are a few common fixes.

You might need to modify your PATH in ~/.zshrc if you're not able to find some commands after switching to oh-my-zsh.
If you installed manually or changed the install location, check the ZSH environment variable in ~/.zshrc.

Custom Plugins and Themes
If you want to override any of the default behaviors, just add a new file (ending in .zsh) in the custom/ directory.
If you have many functions that go well together, you can put them as a abcyzeae.plugin.zsh file in the custom/plugins/ directory and then enable this plugin.
If you would like to override the functionality of a plugin distributed with Oh My Zsh, create a plugin of the same name in the custom/plugins/ directory and it will be loaded instead of the one in plugins/.
Getting Updates
By default, you will be prompted to check for upgrades every few weeks. If you would like oh-my-zsh to automatically upgrade itself without prompting you, set the following in your ~/.zshrc:
DISABLE_UPDATE_PROMPT=true
To disable automatic upgrades, set the following in your ~/.zshrc:
DISABLE_AUTO_UPDATE=true
Manual Updates
If you'd like to upgrade at any point in time (maybe someone just released a new plugin and you don't want to wait a week?)... you just need to run:
upgrade_oh_my_zsh
Magic!
Uninstalling Oh My Zsh
Oh My Zsh isn't for everyone. We'll miss you, but we want to make this an easy breakup.
If you want to uninstall oh-my-zsh, just run uninstall_oh_my_zsh from the command-line. It will remove itself and revert your previous bash or zsh configuration.
Contributing
I'm far from being a Zsh expert and suspect there are many ways to improve – if you have ideas on how to make the configuration easier to maintain (and faster), don't hesitate to fork and send pull requests!
We also need people to test out pull-requests. So take a look through the open issues and help where you can.
Do NOT Send Us Themes
We have (more than) enough themes for the time being. Please fork the project and add one in there – you can let people know how to grab it from there.
Contributors
Oh My Zsh has a vibrant community of happy users and delightful contributors. Without all the time and help from our contributors, it wouldn't be so awesome.
Thank you so much!
Follow Us
We have an ohmyzsh account. You should follow it.
Merchandise
We have stickers and shirts for you to show off your love of Oh My Zsh. Again, this will help you become the talk of the town!
LICENSE
Oh My Zsh is released under the MIT license.
",batch2,8:09:16,Done
268,jave/xwidget-emacs,"-*-org-*-
Xwidgets
This is an experimental branch to enable embedding of GTK widgets
  inside an Emacs window. The Emacs abstraction is called an Xwidget,
  for eXternal widget, and also in reference to the Xembed protocoll.
There is a demo file called xwidget-test.el which shows some of the
  possibilities. There are some screnshots at the emacswiki.
Currently its possible to insert buttons, sliders, xembed widgets, and
  webkit in the buffer. It works similar to the support for images in
  Emacs.  Adding more types of widgets should be fairly straightforward,
  but will require adapter code for each type.
A difference from images is that xwidgets live their own life. You
  create them with an api, get a reference, and tie them to a particular
  buffer with a display spec.
Each xwidget can have several views. In MVC terms, an xwidget is the
  model, and an xwidget-view is a view of the xwidget in a particular
  Emacs window.
The xwidget code attempts to keep the visual appearance of the views
  in sync with through an Observer pattern implementation. This is
  necessary to support the Emacs window paradigm.
building
bzr co bzr+ssh://bzr.savannah.gnu.org/emacs/xwidget/
  #or:
  #git clone https://github.com/jave/xwidget-emacs.git
  #the below compiler flags shouldn’t be strictly necessary
  export CFLAGS=” -g”
  ./configure –with-xwidgets –enable-asserts –with-x-toolkit=gtk3
  make -j4
  gdb -ex run src/emacs
try it out
If you have GTK3 and gtk-webkit installed, you should be able to
  start the embedded webkit browser now:
M-X xwidget-webkit-browse-url
If that didnt work out try the minimal demonstration instead:
(load-library “xwidget-test”)
  (xwidget-demo-a-button)
It looks unimpressive, but it’s a gtk button inside an Emacs buffer!
webkit hints
If you got webkit working, great! Please note, though, that the
  current support is far from a full fledged browser. My focus is on
  delivering a component that can be used to build a full emacs based
  browser on. Since I implement a browse-url function you can get quite
  far just by:
(setq browse-url-browser-function ‘xwidget-webkit-browse-url)
then all Emacs browser interface systems work to a degree.
  heres stuff I use currenly

m-x anything-surfraw interfaces to search engines
C-o in org mode opens links inside org
m-x ffap opens links anywhere in a buffer
m-x gtk-lookup-symbol searches gtk docs
m-x bookmark-jump

etc..
I’ll add more examples as I go along.
However theres lots of support missing, see TODO list for
  information. Briefly:

download handling
keyboard field navigation
isearch
history
sites that use flash. I dont really care about this issue so its

unlikely to be fixed. Just a heads up.
Stability
Beginning with Summer 2011 I am now able to use Xwidget Emacs as my
  primary Emacs. That is good for the project and the stability of the
  code.
At the time of writing I have 24 hour Emacs uptime with several
  embedded webkit browsers, Gnus, org-mode, tramp, etc. etc.
That said, there are still many improvements that needs to be done,
  particularily in memory management. Expect xwidget emacs to leak
  heavily for now.
timeline for inclusion in trunk
The Emacs 24 feature freeze is passed, so xwidgets won’t probably be merged
  until Emacs 25. OTOH since I now use xwidget emacs as my primary
  emacs, I will merge from trunk much more often than in the past.
reporting bugs
Emacs xwidgets uses the same tracker as mainline emacs, but a
  different package. To report a bug:
  M-x report-xwidget-bug
browse bugs:
  http://debbugs.gnu.org/cgi/pkgreport.cgi?package=emacs-xwidgets
Thanks
emacs-devel@gnu.org. There are very helpful people there. When I
  started the xwidget project I had no clue about the Emacs internals.
Brief overview of how xwidgets work
Xwidgets work in one way like images in Emacs. You bind a display spec very
  similar to an image display spec to buffer contents. The display engine will
  notice the display spec and try to display the xwidget there. The display engine
  prepares space at the right place for the xwidget and so on for free, as long as
  we provide proper sizes and so on back to the redisplay engine.
Issues
The problem is that Emacs cant actually draw the widgets, as it can with
  images. Emacs must notify GTK about where the widgets should be, and how they
  should be clipped and so on, and this information must be given to GTK
  synchronous with Emacs display changes. Ok, so why is that difficult then?

How do we know when a widget is NOT to be drawn? The only way I found so far
    is having a flag for each xwdiget, that is reset before a redisplay. When an
    xwidget is encountered during display, the flag is set. After redisplay,
    iterate all xwidgets and hide those which hasnt been displayed.
The gtk socket type for embedding external applications is desirable
    but presents a lot of difficulties of its own. One difficulty is
    deciding which input events to forward, and when and how to do it.

placement and clipping
the entire emacs frame is a gtk window. we use the fixed layout
  manager to place xwidgets on the frame. coordinates are supplied by
  the emacs display engine. widgets are placed inside an intermediate
  window, called the widgetwindow. the widgetwindows are placed on the
  emacs frame.
this way was chosen to simplify clipping of the widgets against emacs
  window borders.
different strategies
Integrating toolkit widgets(gtk in this case) and the emacs display
  engine is more difficult than your plain average gui application, and
  different strategies has been tested and will continue to be tested.
There was a distinction between live xwidgets and
  phantom xwidgets, previous to the change to MVC.

the first aproach was to have the live xwidget on-screen, and move
    them about. the phantoms were generated by snapshoting the live
    xwidget.

the drawback of that aproach was that the gtk toolkit is admirably
  lazy and doesnt draw the widget if its not actualy shown, meaning that
  the snapshots for the phantoms will show garbage.

the second aproach was to use composition support. that tells gtk
    that the widget should be drawn in an off-screen buffer and drawn on
    screen by the application.

this has the primary advantage that the snapshot is always
  available, and enables the possibility of more eye-candy like drawing
  live and phantom widgets in different colors.
the drawback is that its our own responsibility to handle drawing,
  which puts more of the display optimization burden on us.
this is aproach worked so-so.

another aproach is to have both live and phantom widgets drawn
    on-screen by proxy gtk objects. the live xwidget will be entirely
    handled in an off-screen window, and the proxy objects will redirect
    events there.
combine on-screen and off-screen aproaches. maybe composition is the
    way to go for most cases, but on-screen xembeding is the way to go
    for particular special cases, like showing video in a
    window. off-screen rendering and whatnot, is not efficient in that
    particular case, and the user will simply have to accept that the
    phantom of a video widget isnt particularily beautiful.
The current and seemingly sanest aproach implements a MVC pattern.

Testing
;;test like:
  ;; cd /path/to/xwidgets-emacs-dir
  ;; make   all&&  src/emacs -q –eval “(progn (load ""`pwd`/lisp/xwidget-test.el"") (xwidget-demo-basic))”
MVC and Xembedd
The MVC approach appears to be at least in principle robust for plain gtk
  widgets. For the interesting case of gtk sockets which implements an
  xembed host widget that allows for embedding other applications inside
  an Emacs window, the story gets more complex.
The problem is that xembed is designed to plug an application window
  inside a socket and thats it. You can’t move a plug between
  sockets. I tried numerous hacks to get around this but there is
  nothing that works really well.
Therefore the Emacs part of the code will only expose well-defined
  interfaces. cooperating applications will be able to use the interface
  in a well defined manner. The problem is that there is no known xembeddable
  application that implement the needed type of functionality, which is
  allowing for creating new windows on the fly that plug into new
  sockets.
Therefore I will attempt to provide an external application that wraps
  another application and through hacks attempts to provide the needed
  multi view xembed function. That way Emacs is sane and the insanity
  contained.
This app will work by providing a socket that an app plugs into. The
  socket window is copied efficiently by means of composition to a
  number of other windows, that are then plugged into the different
  Emacs sockets.
old notes from x_draw_xwidget_glyph_string
BUG it seems this method for some reason is called with bad s->x and s->y sometimes.
  When this happens the xwidget doesnt move on screen as it should.
  This might be because of x_scroll_run. Emacs decides to scroll the screen by blitting sometimes.
  then emacs doesnt try to actualy call the paint routines, which means this here code will never
  run so the xwidget wont know it has been moved.
Solved temporarily by never optimizing in try_window_reusing_current_matrix().
BUG the phantoming code doesnt work very well when the live xwidget is off screen.
  you will get weirdo display artefacts. Composition ought to solve this, since that means the live window is
  always available in an off-screen buffer. My current attempt at composition doesnt work properly however.
//allocation debugging. the correct values cant be expected to show upp immediately, but eventually they should get to be ok
  // this is because we dont know when the container gets around to do layout
  //GtkAllocation galloc;
  //gtk_widget_get_allocation(GTK_WIDGET (xv->widgetwindow), &galloc);
  //printf(“allocation %d %d , %d %d\n”, galloc.x,galloc.y,galloc.width,galloc.height);
old notes about the old live/phantom scheme
//TODO:
  // 1) always draw live xwidget in selected window
  // (2) if there were no live instances of the xwidget in selected window, also draw it live)
  // 3) if there was a live xwidget previously, now phantom it.
else
  {
  //ok, we are painting the xwidgets in non-selected window, so draw a phantom
  //printf(“draw phantom xwidget at:%d %d\n”,x,y);
  //xwidget_composite_draw_phantom (xw, x, y, clipx, clipy); //TODO MVC there will be very few cases of phantoming
  }
atm this works as follows: only check if xwidgets are displayed in the
  “selected window”. if not, hide them or phantom them.
this means valid cases like xwidgets being displayed only once in
  non-selected windows, does not work well. they should also be visible
  in that case not phantomed.
ToDo:s
webkit crash
[2013-04-13 Sat] seems to crash a lot on http://www.dilbert.com
  Not always, but enough to be annoying.
optimize drawing off large offscreen widgets
Currently I just allocate as large an area as the offscreen webkit
  widget desires. This works well most of the time. But a HTML page
  might in principle be of infinite height so there are cases where this
  doesn’t work too well.
Heres a proposed strategy:

never grow the offscreen webkit over xwidget-webkit-max-height
allow for webkit to handle its own scrolling internally as well
be more clever about when you have more than one emacs window
    showing the same webkit instance.
allow to grow the offscreen instance in steps rather than just
    allocate the entire height at once

again a trace[2011-08-23 Tue]
  the hunch is that since I still hand-wave the view storage the array
  can get out of synchronous. so maybe switching to a lisp structure
  will help as it did for the model. Anyway, doesnt happen at all often.
the trace
(gdb) bt
  #0  0x0000000000685304 in xwidget_touch (xv=0x0) at xwidget.c:1225
  #1  0x00000000006853e7 in xwidget_end_redisplay (w=0x11b42ca0, matrix=
  0xff9bf40) at xwidget.c:1272
  #2  0x000000000041cc31 in update_window (w=0x11b42ca0, force_p=0)
  at dispnew.c:3705
  #3  0x000000000041c0e5 in update_window_tree (w=0x11b42ca0, force_p=0)
  at dispnew.c:3331
  #4  0x000000000041be8b in update_frame (f=0x1682a50, force_p=0,
  inhibit_hairy_id_p=0) at dispnew.c:3258
  #5  0x000000000045066f in redisplay_internal () at xdisp.c:12931
  #6  0x000000000044e210 in redisplay () at xdisp.c:12110
  #7  0x0000000000567e65 in read_char (commandflag=1, nmaps=7, maps=
  0x7fffffffc040, prev_event=12708226, used_mouse_menu=0x7fffffffc254,
  end_time=0x0) at keyboard.c:2447
  #8  0x000000000057613c in read_key_sequence (keybuf=0x7fffffffc4a0, bufsize=
  30, prompt=12708226, dont_downcase_last=0, can_return_switch_frame=1,
  fix_current_buffer=1) at keyboard.c:9299
  #9  0x0000000000565d45 in command_loop_1 () at keyboard.c:1448
  #10 0x0000000000601008 in internal_condition_case (bfun=
  0x565962 <command_loop_1>, handlers=12760466, hfun=0x565259 <cmd_error>)
  at eval.c:1490
  #11 0x0000000000565659 in command_loop_2 (ignore=12708226) at keyboard.c:1159
  #12 0x0000000000600992 in internal_catch (tag=12873826, func=
  —Type <return> to continue, or q <return> to quit—
  0x565633 <command_loop_2>, arg=12708226) at eval.c:1247
  #13 0x00000000005655bd in command_loop () at keyboard.c:1124
  #14 0x0000000000564da7 in recursive_edit_1 () at keyboard.c:759
  #15 0x0000000000564f43 in Frecursive_edit () at keyboard.c:823
  #16 0x000000000060444f in Ffuncall (nargs=1, args=0x7fffffffca20)
  at eval.c:2986
  #17 0x00000000006507f8 in exec_byte_code (bytestr=145172929, vector=145179445,
  maxdepth=116, args_template=12708226, nargs=0, args=0x0) at bytecode.c:785
  #18 0x0000000000604eec in funcall_lambda (fun=140575909, nargs=2, arg_vector=
  0x7fffffffcfe8) at eval.c:3220
  #19 0x000000000060467e in Ffuncall (nargs=3, args=0x7fffffffcfe0)
  at eval.c:3038
  #20 0x00000000006035fc in Fapply (nargs=2, args=0x7fffffffd0b0) at eval.c:2494
  #21 0x0000000000603b43 in apply1 (fn=12874242, arg=301666310) at eval.c:2732
  #22 0x00000000005feb25 in call_debugger (arg=301666310) at eval.c:220
  #23 0x0000000000601ca9 in maybe_call_debugger (conditions=9431542, sig=
  12761282, data=301666742) at eval.c:1893
  #24 0x0000000000601785 in Fsignal (error_symbol=12761282, data=301666742)
  at eval.c:1714
  #25 0x0000000000601898 in xsignal (error_symbol=12761282, data=301666742)
  at eval.c:1749
  #26 0x0000000000601926 in xsignal2 (error_symbol=12761282, arg1=102756373,
  arg2=0) at eval.c:1770
  —Type <return> to continue, or q <return> to quit—
  #27 0x0000000000604d6e in funcall_lambda (fun=102756373, nargs=0, arg_vector=
  0x7fffffffd398) at eval.c:3189
  #28 0x000000000060467e in Ffuncall (nargs=1, args=0x7fffffffd390)
  at eval.c:3038
  #29 0x00000000006507f8 in exec_byte_code (bytestr=54783137, vector=109656229,
  maxdepth=12, args_template=12708226, nargs=0, args=0x0) at bytecode.c:785
  #30 0x0000000000604eec in funcall_lambda (fun=109656517, nargs=0, arg_vector=
  0x7fffffffd890) at eval.c:3220
  #31 0x000000000060467e in Ffuncall (nargs=1, args=0x7fffffffd888)
  at eval.c:3038
  #32 0x0000000000603b08 in apply1 (fn=109656517, arg=12708226) at eval.c:2725
  #33 0x00000000005fc8c9 in Fcall_interactively (function=109656517, record_flag=
  12708226, keys=12754549) at callint.c:379
  #34 0x00000000006044c2 in Ffuncall (nargs=4, args=0x7fffffffdc60)
  at eval.c:2996
  #35 0x0000000000603c57 in call3 (fn=12893554, arg1=109656517, arg2=12708226,
  arg3=12708226) at eval.c:2789
  #36 0x00000000005784cd in Fcommand_execute (cmd=109656517, record_flag=
  12708226, keys=12708226, special=12708226) at keyboard.c:10290
  #37 0x00000000005661fb in command_loop_1 () at keyboard.c:1575
  #38 0x0000000000601008 in internal_condition_case (bfun=
  0x565962 <command_loop_1>, handlers=12760466, hfun=0x565259 <cmd_error>)
  at eval.c:1490
  —Type <return> to continue, or q <return> to quit—
  #39 0x0000000000565659 in command_loop_2 (ignore=12708226) at keyboard.c:1159
  #40 0x0000000000600992 in internal_catch (tag=12756258, func=
  0x565633 <command_loop_2>, arg=12708226) at eval.c:1247
  #41 0x000000000056560c in command_loop () at keyboard.c:1138
  #42 0x0000000000564da7 in recursive_edit_1 () at keyboard.c:759
  #43 0x0000000000564f43 in Frecursive_edit () at keyboard.c:823
  #44 0x0000000000563052 in main (argc=1, argv=0x7fffffffe678) at emacs.c:1711
Lisp Backtrace:
  “recursive-edit” (0xffffca28)
  “debug” (0xffffcfe8)
  “image-bol” (0xffffd398)
  0x68939c0 PVEC_COMPILED
  “call-interactively” (0xffffdc68)
  (gdb)
new annoying tracemaybe related to scroll inhibiting or cursor inhibiting code.
  It appears actually to be related to GLYPH_DEBUG=1. this flag is no
  longer needed.
the trace
Breakpoint 1, abort () at emacs.c:383
  383       kill (getpid (), SIGABRT);
  Missing separate debuginfos, use: debuginfo-install hunspell-1.2.15-2.fc15.x86_64 nss-mdns-0.10-9.fc15.x86_64
  (gdb)
  (gdb)
  (gdb) bt
  #0  abort () at emacs.c:383
  #1  0x0000000000418f01 in matrix_row (matrix=0xac29400, row=-1)
  at dispnew.c:1477
  #2  0x000000000046e113 in draw_glyphs (w=0x18235c0, x=198, row=0xa3af100, area=
  TEXT_AREA, start=17, end=18, hl=DRAW_CURSOR, overlaps=0) at xdisp.c:22550
  #3  0x000000000047869f in draw_phys_cursor_glyph (w=0x18235c0, row=0xa3af100,
  hl=DRAW_CURSOR) at xdisp.c:24882
  #4  0x00000000005083bb in x_draw_window_cursor (w=0x18235c0, glyph_row=
  0xa3af100, x=180, y=361, cursor_type=0, cursor_width=1, on_p=1, active_p=1)
  at xterm.c:7440
  #5  0x00000000004790cd in display_and_set_cursor (w=0x18235c0, on=1, hpos=17,
  vpos=19, x=180, y=361) at xdisp.c:25098
  #6  0x00000000004fa31f in x_update_window_end (w=0x18235c0, cursor_on_p=1,
  mouse_face_overwritten_p=0) at xterm.c:644
  #7  0x000000000041ccb9 in update_window (w=0x18235c0, force_p=0)
  at dispnew.c:3694
  #8  0x000000000041c165 in update_window_tree (w=0x18235c0, force_p=0)
  at dispnew.c:3331
  #9  0x000000000041beee in update_frame (f=0x1658460, force_p=0,
  inhibit_hairy_id_p=0) at dispnew.c:3258
  #10 0x0000000000450a2e in redisplay_internal () at xdisp.c:12983
  #11 0x000000000044e2a6 in redisplay () at xdisp.c:12099
  #12 0x000000000056a60d in read_char (commandflag=1, nmaps=6, maps=
allow xwidgets to report their sizenow we just hard code sizes. but webkit widgets for instance can
  report sizes that suit the content. support that.
BUG xwidget view ghosts(havent seen this in quite a while)

xwidget-webkit-browse-url somewhere
split window.

now theres 2 webkit views

c-x 1

now theres 2 views but one is a ghost!
  one should have been deleted when its window died but that didnt work
  for some reason here.

m-x xwidget-cleanup

the ghost goes away because we killed explicitly but this is just a workaround.
xwidget_view_delete_all_in_window(w); in delete-window-internal is not sufficient.
  delete-other-windows-internal
  delete_all_subwindows
  unshow_buffer
Added cleanup those window configuration hook which works in practice
  but feels kludgy.
code looks like this
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
  (defun xwidget-cleanup ()
  “Delete zombie xwidgets.”
  ;;its still pretty easy to trigger bugs with xwidgets.
  ;;this function tries to implement a workaround
  (interactive)
  (xwidget-delete-zombies) ;;kill xviews who should have been deleted but stull linger
  (redraw-display);;redraw display otherwise ghost of zombies  will remain to haunt the screen
  )
;;this is a workaround because I cant find the right place to put it in C
  ;;seems to work well in practice though
  (add-hook ‘window-configuration-change-hook ‘xwidget-cleanup)
but it ought rather to work like this
xwidget-delete-zombies should be called from C after window
  configuration has changed but before redisplay. redisplay should not
  be called.
BUG annoying backtrace(this no longer seems to happen even under heavy usage. seems merging
  from trunk helped. lots were happening in redisplay at this time in trunk.)
sadly happens a lot.

happens even with no initialized xwidgets
+ row->glyphs[area][i].face_id

or similar code, so row is invalid for some reason.
  xwidgets currently disable some redisplay opimizations so it might be
  an actual emacs bug manifesting without optimizations.
bt 1
* Compute the width of this line.  *
  row->pixel_width = row->x;
  for (i = 0; i < row->used[TEXT_AREA]; ++i)
  row->pixel_width += row->glyphs[TEXT_AREA][i].pixel_width;
(gdb) bt
  #0  0x000000000045c340 in compute_line_metrics (it=0x7fffffff8a20)
  at xdisp.c:17549
  #1  0x00000000004603da in display_line (it=0x7fffffff8a20) at xdisp.c:18792
  #2  0x0000000000457646 in try_window (window=23403045, pos=…, flags=1)
  at xdisp.c:15399
  #3  0x00000000004559c9 in redisplay_window (window=23403045, just_this_one_p=0)
  at xdisp.c:14944
  #4  0x0000000000450247 in redisplay_window_0 (window=23403045) at xdisp.c:13152
  #5  0x00000000005fdcd9 in internal_condition_case_1 (bfun=
  0x450208 <redisplay_window_0>, arg=23403045, handlers=12691046, hfun=
  0x4501d9 <redisplay_window_error>) at eval.c:1538
  #6  0x00000000004501ba in redisplay_windows (window=23403045) at xdisp.c:13132
  #7  0x000000000044f19c in redisplay_internal () at xdisp.c:12706
  #8  0x000000000044f9f2 in redisplay_preserve_echo_area (from_where=7)
  at xdisp.c:12964
  #9  0x0000000000568525 in swallow_events (do_display=1) at keyboard.c:4197
  #10 0x0000000000422554 in sit_for (timeout=40, reading=1, do_display=1)
  at dispnew.c:5963
  #11 0x000000000056512c in read_char (commandflag=1, nmaps=8, maps=
  0x7fffffffd3f0, prev_event=12720514, used_mouse_menu=0x7fffffffd604,
  end_time=0x0) at keyboard.c:2689
  #12 0x0000000000572c59 in read_key_sequence (keybuf=0x7fffffffd850, bufsize=
  30, prompt=12720514, dont_downcase_last=0, can_return_switch_frame=1,
  —Type <return> to continue, or q <return> to quit—
  fix_current_buffer=1) at keyboard.c:9291
  #13 0x0000000000562897 in command_loop_1 () at keyboard.c:1446
  #14 0x00000000005fdb52 in internal_condition_case (bfun=
  0x5624b4 <command_loop_1>, handlers=12772898, hfun=0x561dab <cmd_error>)
  at eval.c:1493
  #15 0x00000000005621ab in command_loop_2 (ignore=12720514) at keyboard.c:1157
  #16 0x00000000005fd4ce in internal_catch (tag=12768770, func=
  0x562185 <command_loop_2>, arg=12720514) at eval.c:1247
  #17 0x000000000056215e in command_loop () at keyboard.c:1136
  #18 0x00000000005618f9 in recursive_edit_1 () at keyboard.c:757
  #19 0x0000000000561a95 in Frecursive_edit () at keyboard.c:821
  #20 0x000000000055fba2 in main (argc=1, argv=0x7fffffffe188) at emacs.c:1704
bt 2
Examine using XComposite rather than GTK off-screen
rendering. This would make xembed widgets work much better. This
  would probably be rathter difficult, but could open up other
  interesting possibilities for Emacs. There is an early attempt in
  xwidget.c, but the X call to redirect to offscreen rendering fails
  for unknown reasons.
the attempt was further worked on, and the xlib calls replaced with
  gdk calls, this works better.
In the end I abandoned this aproach. Xwidget-osr is the new aproach.
make the keyboard event code propagation code work.
There is an attempt to provide an api to send keyboard events to an
  xwidget, but it doesnt currently work very well.
try gtk event creation instead
since that works fine in the webkit osr code.
  but, oh no, that didn’t work for some reason.
  the widgets seems to receive the event but then the embedded widgets
  hangs.
http://kegel.com/gtk/button.c
examine some library to synthesize events
xdotool
  xte xautomation
  crikey
  libxdo
webkit raw keyboard event escape
c-c tab could send a raw tab to the webkit instance.
remove the special-case for when the minibuffer is
active.  I added some code to reduce the annoying problem display artefacts
  when making the minibuffer the selected window. This made xwidgets in the
  buffer go grey or black whenever one did m-x to activate the minibuffer. The
  coded tried to handle the minibuffer as a special case. That simply wasnt a
  good idea. Special-casing will never work properly. It is much better to spend
  time finding solutions that work acceptably in the general case.
disable emacs cursor drawing on top of an active xwidget.
This ought to be rather simple and should improve the visuals a lot.
improve the xwidgets programming interface
so its less of hand-waving affair. This shouldnt be too hard, but I
  have deliberatley not spent any time on it, since getting the
  visuals right is much harder. Anyway, I sort of think the interface
  should be somewhat like it is, except symbols is used instead of
  integers.
use symbols for xwidget types rather than ints
better lisp based structure for xwidgets
the lisp interface woud be like this:

make-xwidget returns an xwidget object, similar to a process
    object. this object is used when creating the display spec(instead of
    the user defined id now used)

the data structure would be something like this:

a “process” like aproach to create the xwidgets. xwidgets are
    coupled to buffers, somewhat like processes, except a buffer can
    hold several xwidgets
an xwidget has a plist to hold the model, like a process
an xwidget has an assoc list of xwidget views

there are some things that arent clear:

an xwidget doesnt necessarily need to be coupled to a buffer but it
    seems to be the clearest model. xwidgets would be buffer local
xwidget-views are by necessity coupled to a emacs window so it might
    be better to store them window locally rather than in an assoc
    coupled to the xwidget model
for some gtk widgets that resist an mvc approach, like the webkit
    widgets, special operations are needed, similar to the old phantom
    widgets aproach. so we need to differentiate live and phantom
    instances for these troublesome widgets and let lisp manage all the trickery.

stuff that needs to work:

do something for all views of a xwidget(resize, value change)
do something for all xw-views in an emacs window(deletion etc)
lookup xw-view for xwidget in emacs window(during redisplay)

(- do something for all siblings of a xw-view. not atm)
xwidget creation interfacexwidgets are a little bit like emacs processes but also a little bit
  like emacs images. Therefore its not perfectly obvious how to handle
  creation. Currently I just use hardcoded identifiers. the real scheme
  needs to be something else.
Heres a tentative approach:

xwidget-create returns a xwidget object, like process creation
    functions. the xwidget will be largely uninitialized until
    discovered by redisplay. an xw belongs to a buffer
xwidget-insert inserts the xwidget in a buffer. when discovered by
    redisplay it will be initialized and a xwidget-view allocated
an event will be emitted when initialization is finished when
    relevant like for sockets

the problem with this aproach is that its not really legal to reuse
  xwidget objects by writing several display specs who reference the
  same xwidget. It could presumably be done but it would just become
  weird for no real benefit. the big preblem is that the display spec
  decides the on-screen size, and its not sane to have xwidget views
  with different sizes. therefore such display specs would need to be
  catched and dissallowed. Except it is hard because AFAIK the specs
  don’t have an identity as such. A flag in the structure could be set
  by lookup so the first display attempt would win. but then you can’t
  rewrite the spec to change the size. hmmm. A third approach would be
  to just allow the 1st spec refering an xw during a redisplay to take
  effect, the rest are signaled as errors. this wouldnt be too bad.
the other aproach would be to work more like images:

write the display spec with all information needed to create the
    xwidget.
retrieve the xwidget objet from the spec with an xwidget-at-point function. It
    can be uninitalized which client code must handle. Unlike
    asynchronous process creation we dont get back a handle, because
    there is none yet.
emitted event on initialization, when needed. Many widgets don’t
    need this. for instance, a button sends an event when pressed. but
    you can’t press it unless its on screen, and then its initialized
    properly.

This approach seemed good, but how do I know which instance
  generates an event if I cant set the id beforehand?
so, therefore, the first two aproach is used.
xwidget creation interface actuallyconclusion of above ramblings:

should be similar to make-text-button
don’t init from display spec, instead during make-xwidget call

callbacks would be nice
but they need to be handled initially with events for technical
  reasons. C code can’t call Lisp easily. The event handler can call the
  callback though.
more documentation
There should be user docs, and xwidget contributor docs. The current README
  is all contributor docs there is now, apart from the code.
CANCELLED look into more ways of displaying xwidgets, like binding them to awindow rather than a point in a buffer. This was suggested by Chidong.
  This would be a useful addition to Emacs in itself, and would avoid nearly all
  display issues. I still think the general case is more interesting, but this
  special case should also be added. The xwidget would then be bound to
  replace the view of a particular window, and it would only show in
  that window.
I got the webkit xwidget to work well enough so I dont see the need
  for this now, except for sockets and I think it can better be dealt
  with at the lisp level.
MVC mode for xwidgetsIt appears unfruitful to chase using the same display mode for all
  types of xwidgets. Composition is fun but not robust the way I
  tried to do it.
Instead there should be a set of MVC xwidgets. Each on-screen instance
  of an MVC widget would be a real GTK widget. The instances would
  communciate state using signals.
There are drawbacks. There is no inbuilt support for MVC in GTK, so we
  have to roll our own, which is tedious if not much work for the few
  cases.
MVC for xembedded application will need support from the applications
  themselves. Inkscape supports multiple views to the same document,
  other programs don’t. In practice it might not be a big drawback.
figure out what to do with the multiple frames case.This should be easier to solve with MVC.
  Surprisingly, this just worked!
how to propagate changes in views to other views?I used gtk signals, the implementation for sliders works well!
canvas support
heres an interesting comparision of gtk canvases
  http://live.gnome.org/ProjectRidley/CanvasOverview
ATM there are small hardcoded demos in the code, these should be
  removed and replaced with working xwgir counterparts.
goocanvas
goocanvas is a gtk canvas implemented using cairo. investigate.
pros:

it has a MVC model aproach out of the box which is nice.

http://developer.gnome.org/goocanvas/unstable/goocanvas-model-view-canvas.html
export CFLAGS=”`pkg-config –cflags goocanvas` -DHAVE_GOOCANVAS”
  export LDFLAGS=`pkg-config –libs goocanvas`
  ./configure
  make
I made a hello goo world xwidget so seems doable.
  I wanted to load a SVG which wasnt immediately straightforward, so I
  tried clutter. but it turns out the exact same strategy could be used
  with goocanvas.
clutter
maybe clutter can be used as a canvas?
  pros:

seems to have a lot of traction atm. many examples
potentialy fast and cool vector graphics

cons:

no out of the box MVC support, but seems doable. no worse than the
    other home brew mvc support I have in xwidgets

(media-explorer in an application that employes the MVC pattern)
http://www.openismus.com/documents/clutter_tutorial/0.9/docs/tutorial/html/sec-stage-widget.html
there is also cool stuff like this:
  http://gitorious.org/webkit-clutter/webkit-clutter which is an webkit actor for
  clutter! hmmmmm.
I want to render svg. aparently:
  librsvg rsvg_handle_render_cairo(h, cr);
  ClutterCairoTexture
  Clutter
export CFLAGS=”`pkg-config –cflags clutter-gtk-1.0` -DHAVE_CLUTTER”
  export LDFLAGS=`pkg-config –libs clutter-gtk-1.0`
  ./configure
  make
compiles but I get:
  Gtk-ERROR **: GTK+ 2.x symbols detected. Using GTK+ 2.x and GTK+ 3 in
  the same process is not supported
export CFLAGS=”`pkg-config –cflags clutter-gtk-0.10` -DHAVE_CLUTTER”
  export LDFLAGS=`pkg-config –libs clutter-gtk-0.10`
  ./configure
  make
webkit html 5
expose the DOM to lisp or something. The webkit xwidget works pretty
  well now, so this might be the way ahead.
mvc code crashes after a whileseemingly only when compiling with optimizations.
  I have no idea why.
Doesn’t seem to happen after some code cleanups.
xwidget-resize-atreimplement so display spec is not involved
display spec validationit is an error to reuse xwidgets in several buffers or in the same
  buffer. how do we catch these errors?

showing the same xwidget twice in a buffer is no more wrong than
    showing in several emacs windows, just conceptually wrong, so ignore
    this case for now
xwidgets now store a reference to the buffer they were created in,
    so use that to invalidate xwidget references in oher buffers. but
    thats not really an error either
xwidgets should now be proper lisp objects so you dont delete them
    you await their garbage collection. so therefore there can never be
    invalid display specs

so turned out this got solved by using proper lisp objects for
  xwidgets. yay!
clipping of controllers
Emacs uses a big GTK window and does its own clipping against Emacs
  windows inside this area. So, in order to layout gtk widgets in emacs
  windows we must clip thim ourselves.
The following method worked well for a long time:

make a gtk widget, say a button, xw
make a clipping area, of type gtkfixed(many types have been tested)
put the clip area in the main emacs gtk window
figure out clip area changes during emacs redisplay

the only weirdness was that one has to tell gtk the clip area has a
  window in order to get clipping. This is weird because all gtkwidgets
  are windows in a sense and a window is almost by definition also a
  clipping area.
Anyway, in GTK3 the   gtk_widget_set_has_window(GTK_WIDGET (
  xv->widgetwindow), TRUE); call is ignored.
The gtkeventbox which is documented to have its own window doesnt work
  either.
http://www.lanedo.com/~carlos/gtk3-doc/chap-drawing-model.html
anyway clipping is rather complicated but seems to finally work okay.
subclass my own clipping widgethttp://www.lanedo.com/~carlos/gtk3-doc/GtkWidget.html#gtk-widget-set-has-window
  mentions that it has_window can only be called inside a widget
  implementation.
this wasnt really the issue. allocation was the problem
try scrolled windowclipping does in fact work with
  gtk_scrolled_window_add_with_viewport (xv->widgetwindow, xv->widget);
  !!
I get unwanted scrollbars in the widget though.
gtk_scrolled_window_set_policy      (  xv->widgetwindow,
  GTK_POLICY_NEVER, GTK_POLICY_NEVER);
stops clipping from working!
try viewportgtkviewport is used in scrolled window so in order to remove
  scrollbars it should be possible to use viewport directly. however,
  viewport ignores size requests. or rather the container does.
debug allocationthe container determines how much size to allocate to child widgets.
  GtkAllocation galloc;
    gtk_widget_get_allocation(GTK_WIDGET (xv->widgetwindow), &galloc);
    printf(“allocation %d %d , %d %d\n”, galloc.x,galloc.y,galloc.width,galloc.height);
after my clipping attemp shows that my size request is ignored! this
  might be logical, since the container provided by emacs is a
  gtkfixed. gtkfixed might choose to heed the widgets size desires and
  allocate the entire widget size. but we want clipping!
since i cant reasonably expect to change the emacs main container, i
  can maybe overide the setallocation method in gwfixed, and adjust
  allocation to clipping if its an xwidget asking for allocation.
subclass gtkfixedpossibly i need to subclass gtkfixed and override
void                gtk_widget_size_allocate            (GtkWidget *widget,
                                                         GtkAllocation *allocation);
http://developer.gnome.org/gobject/stable/howto-gobject.html
turns out emacs already does this for gtk3 according to jan D:
  >>For GTK3, Emacs already subclasses GtkFixed, see emacsgtkfixed.[ch].

widgets may not be underallocated, aparently

http://mail.gnome.org/archives/commits-list/2011-April/msg10950.html

how to call base class method/chain up

http://developer.gnome.org/gobject/stable/howto-gobject-chainup.html

the allocation modification could happen in the container or the
    child. it feels more apropiate in the container

it is however unexpectedy inconvenient to modify allocation because
  the needed data is private to the base class. to overcome this:

run base class method 1st.
then, iterate all children, and modify allocation  for xwidget
    children only. x y will then be set.

JanD pointed out the GTK3 port already has its own subclass, so I
  modified that one.
clip topthere are four controller edges that potentialy need clipping. I begun
  with right and bottom edges. clipping them is just a matter of setting
  the right size of the widgetwindow and also ensure it gets the right
  allocation from the container.
clipping top (and left) is not equally straightforward. I’m using a
  viewport now and scroll it the amount that needs to be clipped.
  however, the viewport is sensitive to changes in allocation, which
  makes it harder to use the allocation workarounds.
see:

gtk_widget_set_size_request
gtkscrolledwindow

I returned to using a simple gtkfixed for the widgetwindow. with
  allocation hack and set_has_window it works. Idea prefer not to have
  the allocatien hack and it wasnt needed it gtk3 only gtk2. needs
  further investigation.
various code cleanups
There are many cleanups necessary before any hope of inclusion in
  Emacs trunk. To begin with, the part of the patch that touches other
  parts of emacs must be very clean.
use FRAME_GTK_WIDGET (f)rather than gwfixed.
support configure
ifdef all xwidget codeso you can reliably disable the code at compiletime
translate clickson onscreen webkit peer to offscreen
maybe
  http://developer.gnome.org/gdk/stable/gdk-Windows.html#GdkWindow-from-embedder
turned out to be not so hard, captured events, copied them and
  forwarded them offscreen!
CANCELLED investigate gdk_window_redirect_to_drawable(cancelled this, the current approach seems okay)
  http://developer.gnome.org/gdk/stable/gdk-Windows.html#gdk-offscreen-window-set-embedder
  maybe could be used in place of my own copy hacks? to work it must
  support a chain of redirects, which seems unlikely. the benefit would
  be that I dont have to spend time optimizing redrawing.
remove xwidget_views when emacs window is deletedremoving xwidget views when an Emacs window closes is not reliable.

switching buffers in a window seems to hide the corresponding
    xwidget-views properly, but they might as well be deleted.
patching delete-window-internal could be used to delete the xwidget-views

this seems to work
browser xwidget
although embedding a browser is not my primary concern many are
  interested in this. some suitable browser component needs to be found
  supporting gtk.
webkitthere is a webkit gtk port. there is no obvious mvc support.
  http://live.gnome.org/WebKitGtk
http://webkitgtk.org/
it might be possible to keep a set of webxits in artificial
  synchronisation by recursive deep copy of the DOM from one webkit to
  another. This will be error prone at best though. Another way might be
  to just use bitmap copy of the “live”instance to the “phantom”
  instances. the problem of transfering the live view remains though.
export CFLAGS=”`pkg-config –cflags webkit-1.0` -DHAVE_WEBKIT -g”
  export LDFLAGS=`pkg-config –libs webkit-1.0`
  ./configure
  make
off screen rendering
export CFLAGS=”`pkg-config –cflags webkit-1.0` -DHAVE_WEBKIT_OSR -g”
  export LDFLAGS=`pkg-config –libs webkit-1.0`
  ./configure
  make
works a little bit  but i get errors like:
(emacs:8362): GLib-GObject-WARNING **: invalid cast from `GdkOffscreenWindow’ to `GdkDrawableImplX11’
set a breakpoint in g_log, backtrace seems to indicate
  webkitViewportAttributesRecompute is the offender.
maybe try gtk3 variants?
export CFLAGS=""`pkg-config --cflags webkitgtk-3.0 ` -DHAVE_WEBKIT_OSR ""
export LDFLAGS=`pkg-config --libs webkitgtk-3.0 `
./configure   --with-x-toolkit=gtk3
make
crash in gtk_window_get_size instead. great.
http://gtkplus-p3.0.sourcearchive.com/documentation/2.91.5-0ubuntu1/testoffscreenwindow_8c-source.html
after many attempts, the basic issue remains. for some reason the
  offscreen widget isnt ok when I want to snapshot it, so i simply get
  emptiness. the surface is only ok sometimes.
here is a useful debugging snippets:
// debugging redraw:
//  - the bg colors always change, so theres no error in signal handling
//  - i get this error now and then:
//(emacs:7109): GLib-GObject-WARNING **: invalid cast from `GdkOffscreenWindow' to `GdkDrawableImplX11'
// seems to happen in webkit actually. see README

if(0){ //redraw debug hack. helped a lot in fact. use the with alpha painter below also
  cairo_set_source_rgb(cr, osr_dbg_color, 1.0, 0.2);
  cairo_rectangle(cr, 0,0, xw->width, xw->height);
  cairo_fill(cr);
  osr_dbg_color+=0.1;
  if(osr_dbg_color>1.0)
    osr_dbg_color=0.0;
  
}
you need to terminate drawing like this:
//cairo_set_source_surface (cr, src_pixmap, 0,0); 
//cairo_set_operator (cr, CAIRO_OPERATOR_OVER);

//cairo_paint_with_alpha (cr, 1.0);
//cairo_paint(cr);
the snippets change background color on oach redraw.
on-screen rendering to separate window
an alternative might be to open a separate window and snapshot it. the
  idea is that whatever oddness webkit does so that offscreen rendering
  doesnt work, doesnt happen on-screen. the window could be opened
  somewhere not in the way.
CANCELLED firefoxhttp://www-archive.mozilla.org/unix/gtk-embedding.html
  seems to be severly bitrotted
heres a newer aproach
  http://hg.mozilla.org/incubator/embedding/file/29ac0fe51754/gtk/tests/test.cpp
while webkit clearly has the best traction as an embedded, the
  offscreen rendering issues makes it interesting to see what ff brings
  to the table.
turned out webkit has as good offscreen support as anyone, see I went
  with that in the end.
text field supportEmacs captures all keyboard events so text field support isn’t super
  straightforward.
propagate keyboard events
I have some old hacks for this and they are not good.
use the DOM model
expose document.activeElement to lisp. This is potentially more
  interesting than just forwarding keyboard events.
webkit_web_view_get_dom_document ()
this is hard it seems. an idea might be to hack elisp support for swig
  to machine generate the bindings.
inject javascriptwebkit_web_view_execute_script ()
this works now:
  (xwidget-webkit-execute-script 5 “document.activeElement.value=’test’”)
so it should be possible to do some interesting stuff.
  execute-script does however not return anything at the interface level
  so satisfaction is not total:
http://markmail.org/message/4yowmdgras73z3x5
maybe
  https://launchpad.net/gnome-seed
or this funny hack:
  <jave> im trying to understand how to interact via javascript to an embedded
  webkit gtk instance  [23:38]
  <jave> i use webkit_web_view_execute_script() which is nice but doesnt return
  a value, by design aparently  [23:39]
  <jave> any hints?
  <lucian> jave: afaik, webkit still doesn’t have full gobject bindings  [23:48]
  <lucian> jave: you can hack it up by making the JS modify the title, and read
  the title from gtk-side
  <jave> lucian: that was a pretty cool idea!
webkit_web_view_load_string ()
I would like preview of html in a buffer rather than from uri.
simple xwidget-webkit wrapperso that it could be used for actual browsing :)
  I dont want to reinvent too many wheels so i’d like to use existing
  emacs facilities here possible. use bindings similar to w3m(or info)

m-x xwidget-webkit starts a session
‘g’ goes to a url
use bookmark-jump i suppose. I mostly use org for bookmarks myself
browse-url support so webkit can be the default browser
some way of getting around the quirky keyboard interaction since
    xwidgets dont receive keyboard events because I hawe no idea how to
    do that in a sane way

… and one can of course go on bikeshedding forever. lets keep it
  simple and extensible, and compatible with other Emacs packages.
the really cool ideas would need Emacs DOM integration, which is not
  easy.
webkit related
webkit support webkit signals
particularly document-load-finishedhttp://webkitgtk.org/reference/webkitgtk-webkitwebview.html#WebKitWebView-document-load-finished
  because one might need tell set a title and sizes and things when it loads.
event bug
Debugger entered–Lisp error: (error “Two bases given in one event”)
hapens sometimes with xwidget events. appears to be when the
  originating xwidget is offscreen so that the event doesn’t get caught
  by the correct emacs event map.
maybe I need to set the originating window in the event structure.
  event.frame_or_window = Qnil;	//frame; //how to get the frame here? //TODO i store it in the xwidget now
since its an offscreen xwidget the buffer local keymap isnt the right
  place for the handler. some global map should be used.
onscreen widgets don’t have the same issue.
anyway, seems it’ll turn out like this:

xwidget-osr stores a callback and user data
the event is an implementation detail only and get caught in the
    topmost event map
the event map calls the callback in the xw with the right args.

we need the event handler at some level because we can’t call lisp
  asynchronously.
navigation signal
new window signal
console messages
http://webkitgtk.org/reference/webkitgtk-webkitwebview.html#WebKitWebView-console-message
http://getfirebug.com/wiki/index.php/Console_API#console.count.28.5Btitle.5D.29
  because maybe we can make a simple JS REPL that way.
  (xwidget-webkit-execute-script ( xwidget-webkit-last-session)
  “console.log(‘hello’)”)
  prints hello to stdout but theres no way to catch stdout from webkit I
  think other than receiving the signal.
webkit flashkiller by default
while its possible to support plugins in the webkit xwidget, flash has
  issues on 64 bit, and slows down emacs to a halt with off screen
  rendering, and of course is not free software. its in the way for real
  world usage even if its interesting to watch flash animations inside
  emacs. which should be achieved with Gnash or other free software
  instead.
http://stackoverflow.com/questions/4885513/prevent-flash-in-cocoa-webview
simply use this api:
  http://webkitgtk.org/reference/WebKitWebPluginDatabase.html
theres an implementation now but it’s not robust enough webkit often
  crashes taking emacs with it.
webkit downloads
when clicking a download link in Webkit Emacs should take over and handle it
  from there. Probably need signals. There are Emacs libraries to
  download things, with wget etc. an url.el facility should be made.
  “download-requested”
webkit alt-text not handled
XKCD use image-title to display a cartoon comment. These mysteriously
  don’t work ATM. Other mouseovers work though. Maybe webkit tries to
  open a new window or something, which wont work.
webkit isearch in webkit buffers
have a look at how docview solves it
  webkit_web_view_search_text ()
webkit relative references doesn’t work
because we handle scrolling in a non-standard way. It does
  work sort of when theres a html frameset and webkit scrolls by itself.
internal links (page.html#section) do not work
  see xwidget-webkit-show-named-element
also did some  webkit signal work for this.
now it actually works! except for I need to know the Y coordinate of
  the element to navigate to, and that can either be by “name” or “id”
  attribute, currently “id”  works.
webkit width adjustment handling issue
since there are so many levels of clipping and whatnot in xwidgets
  sizing issues are difficult.

an xwidget is told how large it can be by emacs. thats the end of
    it. if the xwidget thinks otherwise it will be clipped.
but emacs can ask the xwidget how large it wants to be. it can then
    resize the reserved area and inform the xwidget thusly.

That should have been enough. but webkit never reports less than what
  it already has. So currently a webkit view will only growth and not
  adjust to smaller sizes.
This is not a big problem in practice but is still annoying.
to see the problem surface to http://www.slashdot.org

xwidget-webkit-adjust-size
xwidget-webkit-adjust-size-to-content

and then compare by resizing in Epiphany, which is also webkit based.
try putting webkit osr inside a scrolling window
it seems webkit is supposed to behave differently while embedded in a
  scrolling window. This is a bit cumbersome because the container stack
  is already deep.
xwidget webkit allow loading from string from emacs
xwidget-webkit-last-sessionwas rather hurried. end result is that the lisp layer only really
  allows for one webkit session.
extract DOM to lisp
then the SHR html renderer from Gnus could render the DOM as created
  by Webkit.
made a simple oxperimental DOM tree traverser. It can be expanded to
  return a lisp representation, LDOM.
in order to bring lisp and DOM closer together the LDOM can include a
  mapping to the originating DOM node. so, find a node in LDOM, and the
  cell maps to the original DOM. but since the LDOM is a copy it can get
  out of sync. DOM events might help.
C-X b in other buffer from webkitbafflingly resets the webkit view to the top. Maybe the window
  reconfiguration hook code? further mystification is added because it
  only seems to happen with ido mode enabled.
in comparison with image-mode which does the right thing, I discovered
  that image-mode has special code to handle scrolling. the browser mode
  and image mode has some similarities.
I made some delegation code frrom webkit mode to image mode.
url-browse improvement
sindikat: site.com and http://site.com should be equivalent (simple site.com
  throws error)
Yes, but its unclear at what level in Emacs to do this
  properly. I added a url-tidy function as a start.
this should be further improved:

change the call to url-tidy so its a hook
provide a couple of demonstration hooks:
    
url-tidy, which just prepends http://
youtube which appends &html5=1 to urls looking like http://www.youtube.com/watch?v=DZdUgjEx_dQ
history which logs all visited urls like a traditional browser



sindicat notes
Here are some comments from user “sindikat” and my replies

http://ya.ru renders inadequatly (compare with any other browser) -
    the search text-input is way below

The problem is the size communication between Emacs and Webkit.

doing PageDown is endless; so if you do 100 PageDowns, you have to
    do 100 PageUps to retun to the header of the page

True, I hadn’t noticed. Thanks.

http://linux.org.ru (just an example) renders incorrectly too - it
    should stretch horizontally

Size communication.

obviously, pointing of mouse over some link should change it to
    pointing hand cursor

Need to verify with some other webkit browser.

when you are somewhere on the middle of a long page, than go to some
    other page, you are still in the middle, instead of being again on
    the top

This is because I inherit from Image view mode. I kind of like it so
  we can add an option for it.

changing dropdown menus cause flickering


string entering is incorrect - by default it enters the title of the
    page, while it should be empty

The cause is the lack of return value in the webkit evaluation
  API. Ive made some fixes.

internal links (page.html#section) do not work
    ive added a rudimentary function “xwidget-webkit-show-named-element” for this

maybe it’s a good idea to implement Conkeror or some other
    keybindings, where you press ‘f’ then select the exact <input
    type=”text”> where you want to enter text, without using mouse,
    etc.;

Indeed, this would require better DOM integration.

pressing ‘home’ and ‘end’ puts nonsense into minibuffer

Probably because the Image mode derivative is mostly a hack.
  fixed now I think.

implement search (emacs internal isearch obviously doesn’t work)

Either use the webkit search but that doesn’t feel right. It would be
  better to expose the DOM and search that.

some sites intercept with keyboard; example -
    http://www.artlebedev.ru/kovodstvo/business-lynch/2011/10/03/ uses
    Ctrl+left/right/up/down to navigate between pages - this should be
    implemented too

Keyboard integration is the unloved step-child of xwidgets, unfortunately.
xwidget image display  spec compatibility
some history: the first version of the xwidget display spec was
  the same as an image spec. This turned out not to be fantastic because
  an xwidget is both like a process and like an image. it has a separate
  existence from display. So now the xwidget display spec is just a
  pointer to a xwidget. But then some useful functionality in Emacs
  can’t be reused for xwidget, in particular image-mode.
Maybe a new image type could be added that was a wraper on an
  xwidget. Then image mode could be reused for webkit mode.
I tried some adaptor code in xwidget.el so webkit mode now delegates
  to image mode but its a kludge.
socket related
some flickering during redisplay of sockets
with gtk3 an size allocation workaround is used.
  this seems maybe to result in flickering sizewize y-axis with the
  xwidget socket type. The webkit xwidget doesn’t seem similarily
  afflicted.
the size allocation workaround works by 1st running the ordinary
  allocation then modifying the results. its done this way to minimize
  the copy paste index from the base class. it might be that the
  original allocation has a brief time window to show itself.
tried to modify the allocation hack so it doesn’t call allocate
  twice. this doesn’t seem to help flicker at all aparently so the
  hypothesis falls. Maybe then a socket simply doesn’t like being clipped
  by gtkfixed.
xwidget view reaping too agressive
hide an emacs window for a while and return to it. the xwidget might
  get reaped and a new socket thus created.
try out OSR for sockets
didn’t work too well in the inkscape case. it might be that some other
  bitmap copy method works better though.
basically sockets doesn’t like to be offscreen because they want their
  own gdk window.
synchronise emacs background with xwidget colorfine-tuning to reduce flicker.
isn’t needed if emacs bg erase disabled
xwidgets doesn’t work during bootstrap all of a suddenmight be some annoying local issues with my install because it is not
  reliably reproducible. (went away during merges)
CANCELLED low impact xwidget based image viewer(cancelled this because it no longer seems like a good idea)
  for instance to render SVG using webkit, or some other canvas.
  that way it would be possible to merge to trunk in stages.
so, webkit could be used to display the SVG. the display spec for
  images would be used. multiple webkits would be used rather than
  offscreen rendering, so it would be GTK2 compatible.
xwidget movement doesn’t work all of a suddenthis used to work great. now it doesn’t.
suspects:

XCopyArea
    
x_shift_glyphs_for_insert
x_scroll_run. this is run by the try_window* functions, and
        inhibiting them doesnt help. but also callid in scrolling_window.




try_window_reusing_current_matrix
I used to enable GLYPH_DEBUG which I currently don’t. it disables
    many optimisations. this was fixed.
lookup_xwidget then produce_xwidget_glyph gets called always but not

x_draw_xwidget_glyph_string probably because of scroll optimization.
  movement detection could possibly be moved to produce_xwidget_glyph(not)
no longer helps:
  (setq inhibit-try-window-id t)
  (setq inhibit-try-window-reusing t)
workaround:
  (run-with-timer 1 1 ‘redraw-display)
seems to work:
  inhibiting scrolling_window(). and this seem to be enough to restore the
  old behaviour, GLYPH_DEBUG doesn’t seem needed.
GLYPH_DEBUG doesn’t workwas stupid accidental line removal that was hard to spot
osc xwidget example
a couple of xwidget sliders that control a csound/supercollider song with osc.
  so, for that to work we need slider callbacks to work. when a slider
  changes send an osc message. use ocssend:
oscsend localhost 7777 /sample/address iTfs 1 3.14 hello
or better:
  http://delysid.org/emacs/osc.el
sliders could be defined in csound comments or something to illustrate
  the point. or if real fanciness is desired parse the csound source
  with Semantic and provide a control buffer corresponding to the
  defined controls.
Added: [2011-08-11 Thu 10:53]
SEBthe SEB site does something funny so I can’t insert text in
  fields. aparently document.activeElement.value doesn’t work with framesets.
seems to work using the ugly javascript in
  xwidget-webkit-activeelement-js
support downstreams
http://aur.archlinux.org/packages.php?ID=53902
http://gpo.zugaina.org/app-editors/emacs-xwidget/ChangeLog
the proof of concept canvas code should be disabled by default.
advi
active dvi viewer. investigate if it could be xwidgetified.
  advi supports embedding inside presentations.
cairo configuration support
gtk3 brings in cairo on Fedora, but apparently not on all plattforms.
  pkg-config –cflags cairo
splint
splint   -Demacs -DHAVE_CONFIG_H  -I. -I/home/joakim/build_myprojs/emacsnew/emacs.bzr/xwidget.mint/src -I../lib -I/home/joakim/build_myprojs/emacsnew/emacs.bzr/xwidget.mint/src/../lib   -DGSEAL_ENABLE  -I/usr/include/gtk-3.0 -I/usr/include/atk-1.0 -I/usr/include/cairo -I/usr/include/gdk-pixbuf-2.0 -I/usr/include/pango-1.0 -I/usr/include/glib-2.0 -I/usr/lib64/glib-2.0/include -I/usr/include/pixman-1 -I/usr/include/freetype2 -I/usr/include/libpng12   -I/usr/include/freetype2    -I/usr/include/alsa    -I/usr/include/librsvg-2.0 -I/usr/include/glib-2.0 -I/usr/lib64/glib-2.0/include -I/usr/include/gdk-pixbuf-2.0 -I/usr/include/cairo -I/usr/include/libpng12 -I/usr/include/pixman-1 -I/usr/include/freetype2    -I/usr/include/ImageMagick   -I/usr/include/libxml2   -I/usr/include/dbus-1.0 -I/usr/lib64/dbus-1.0/include   -DGSEAL_ENABLE  -I/usr/include/webkit-3.0 -I/usr/include/glib-2.0 -I/usr/lib64/glib-2.0/include -I/usr/include/gtk-3.0 -I/usr/include/libsoup-2.4 -I/usr/include/atk-1.0 -I/usr/include/cairo -I/usr/include/gdk-pixbuf-2.0 -I/usr/include/pango-1.0 -I/usr/include/pixman-1 -I/usr/include/freetype2 -I/usr/include/libpng12 -I/usr/include/libxml2  -I/usr/include/glib-2.0 -I/usr/lib64/glib-2.0/include   -DORBIT2=1  -I/usr/include/gconf/2 -I/usr/include/orbit-2.0 -I/usr/include/glib-2.0 -I/usr/lib64/glib-2.0/include   -I/usr/include/freetype2   xwidget.c
32 bit bug
user reports that xwidgets segfaults  on the 32 bit Mint distribution
  but not the 64 bit. Mint is an Ubuntu derivative. I got some
  VirtualBox images to test with.
youtubehttp://www.youtube.com/watch?v=DZdUgjEx_dQ&html5=1
  html5 makes it work without stupid flash plugins!
clicking on an webkit xwidgets
doesn’t make the window active. this leads to problems.
“g” should default to current url“g” runs xwidget-webkit-browse-url which gets its interactive argument
  from browse-url-interactive-arg. this might need a new optional argument.
http://test
anything/helm support
hook so anything/helm can filter browser history.
new relative url code sometimes fail
http://www.dilbert.com
input field enhancements
password field.
was straightforward
textarea
less straightforward. I would like it to work like emacs-w3m, where a
  new editing buffer is opened. on c-c, the buffer is closed and the
  browser field updated. however, it’s not immediately obvious how to
  store the reference to the field reliably.
furthermore the current code doesn’t seem to propagate linefeed
  properly to text areas.
bug in current navigation handleron www.dn.se
  Debugger entered–Lisp error: (args-out-of-range “http://platform.twitter.com/widgets/hub.html” 54 357)
  match-string(1 “http://platform.twitter.com/widgets/hub.html”)
  xwidget-webkit-callback(48890368 navigation-policy-decision-requested)
  xwidget-event-handler()
  call-interactively(xwidget-event-handler nil nil)
how to set the name of a webkit buffer?
not obvious because, the buffer isn’t created immediately and there is
  a callback that sets the buffer name automatically
how to find next field in tab order?
unique buffer names
the webkit xwidgets renames the buffer after load but not uniquely so
  it sometimes fails.
kill the offscreen webkit xwidgets when last view killed
The offscreen xwidgets is currently kept around even if the xwidgets
  views are all gone. this is a general problem and it requires actions
  on the behalf of the application to resolve.
In the case of webkit it is currently possible to get errors like these:
Debugger entered–Lisp error: (error “Selecting deleted buffer”)
  xwidget-webkit-callback(60925380 navigation-policy-decision-requested)
  xwidget-event-handler()
  call-interactively(xwidget-event-handler nil nil)
because the last view is gone and the offscreen widgets is still
  generating events.
In the case of webkit it is okay to kill the offscreen widgets
  completely when the user kills the last view window because it would
  be unexpected by the user to see it pop up again. This is not true in
  the general case.
xwidgets debugging logcurrently theres a lot of debugging traces using “message” which is
  annoying. Instead put them in a separate trace buffer.
  (see xwidgetbuffer)
make garbage collect work for xwidgets
when an xwidget is removed from xwidget-alist, and there are no other
  references(mostly views) the xwidget should be garbage collected.
special finalization would go into gc_sweep()
embedding evince
http://developer.gnome.org/libevview/3.2/libevview-ev-view.html
  would be useful for reading PDF:s and other document types.
  it would work the same way webkit embedding works.
support gobject introspection
https://live.gnome.org/GObjectIntrospection/
  supporting gobject introspection would mean that more gtk widgets
  could be tried out with less effort, and also that the build process
  and runtime support would be easier. The drawbacks are small: somewhat
  slower execution, and difficulty in providing elisp bindings for
  introspection.
https://live.gnome.org/GObjectIntrospection/HowToWriteALanguageBinding
http://developer.gnome.org/gi/unstable/gi-girepository.html
http://developer.gnome.org/gi/unstable/gi-overview.html
In order for GIR to work, it needs the namespace and class of a
  widget. This is used to access the typelib file, which contains the
  introspection data. The namespace and class is stored as a property
  on the lisp symbol handle used by xwidgets to identify the widget
  class.
This snippet sets the needed :xwgir-class property, and calls the
  set_zoom_level method:
M-x xwidget-webkit-browse-url RET www.emacswiki.org RET
Then eval the following:
;;load the webkit typelib
  (xwgir-require-namespace “WebKit” “2.0”)
;;provide the metadata needed so xwgir can work with the webkit-osr xwidget
  (put ‘webkit-osr :xwgir-class ‘(“WebKit” “WebView”))
  ;;call the method
  (xwgir-call-method (xwidget-at 1) “set_zoom_level” ‘(3.0))
It’s also possible to create widgets dynamically, by using
  introspection to call a widget constructor(from xwidget-test.el):
(defun xwgir-test ()
  (interactive)
  (xwgir-require-namespace “Gtk” “3.0”)
  (put ‘color-selection :xwgir-class ‘(“Gtk” “ColorSelection”))
(xwgir-demo-a-xwgir-button)
  (xwgir-call-method (xwidget-at 1) “set_label” ‘( “xwgir set label!”))
  )
Current limitation:

Only 0 arg constructors are supported at the moment. Since xwidgets
    defer construction, the args needs to be stored with the xwidget.
xwgir-call-method does indeed lisp to gobject conversion for the
    arguments, but only some primitive types are supported atm.
next to no argument checking. If wrong type args are used with the
    xwgir methods, emacs crashes.

xwgir create components with more advanced constructor
so this opens up an entire new can of beans.
explain by example:
  lets say we want to create agtkhscale on screen. its a slider.
  https://developer.gnome.org/gtk3/stable/GtkHScale.html
  we can already create buttons, so sliders shouldnt be much more
  advanced right? wrong.
the simplest slider constructor looks like:
  GtkWidget *         gtk_hscale_new
  (GtkAdjustment *adjustment);
so in order to call it, we must be able to forward arguments to the
  constructor. this is almost already done, but we lack the ability to
  pass object instances, only simple types atm.
we need to be able to create GtkAdjustment
  https://developer.gnome.org/gtk3/stable/GtkAdjustment.html
this we can already almost do, because an xwidget is a gir object
  with some decorations. we also store the decorated gir object in an
  array, retrievable from lisp.
In order for this to be usable in practice, we need some changes:

lightweight objects should be stored un-decorated. they have no
    need for the entire graphical machinery of xwidgets
lightweight objects should be garbage collectable, but this is the
    same for all the xwidget objects, and isnt really resolved atm.

investigate gdk_offscreen_window_set_embedder()https://developer.gnome.org/gdk/unstable/gdk-Windows.html
,—-

Offscreen windows are more general than composited windows, since they
allow not only to modify the rendering of the child window onto its
parent, but also to apply coordinate transformations.
To integrate an offscreen window into a window hierarchy, one has to
call gdk_offscreen_window_set_embedder() and handle a number of
signals. The “pick-embedded-child” signal on the embedder window is
used to select an offscreen child at given coordinates, and the
“to-embedder” and “from-embedder” signals on the offscreen window are
used to translate coordinates between the embedder and the offscreen
window.
For rendering an offscreen window onto its embedder, the contents of
the offscreen window are available as a pixmap, via
gdk_offscreen_window_get_pixmap().

`—-
okay, [2013-04-03 Wed] I finally suceeded in this approach!
  it was pretty  hard to make it work and currently works like this:

the on screen dravwing area is the embedder
you must implement “pick child”

event forwarding is done automatically!
BUT its not really super, because it only works well with a single
  embedder.
perhaps the strategy could be refined:

the window frame would be the embedder for all xwidgets. (but what
    about several frames then?)
in the from-embedder signal handler, which maps container coords to
    embedded widget coords, find out which xw-view i clicked on, and
    compute the coords.

[2013-04-04 Thu] I had a strategy working for a xwgir button but not
  a webkit. set_embedded in the motion event handler for the xv. it
  even works for 2 frames! but not webkit :(
[2013-04-05 Fri] it works for xwgir osr components, but not for
  webkit. Webkit retains the previous event forwarding system.
Now it works like this:

the offscreen widget is created as before
the on-screen views also as before, painting and copying as before.
gdk_offscreen_window_set_embedder() is now used to embedd the
    offscreen widget in the onscreen one, upon view creation
only one widget can embedd one other. This means that the embedding
    widget must be switched between the onscreen ones. This is now done
    in the mouse motion event handler.

The above approach has been tested for xwgir created buttons and seems
  to work. it doesnt work for webkit, so the old scheme is preserved
  for webkit.
investigate git-remote-bzr
",batch1,16:59:01,Done
269,RPCS3/llvm-mirror,,batch1,16:57:58,Done
270,glen77777/remacs,"Rust ❤️ Emacs

A community-driven port of Emacs to Rust.
GPLv3 license.
Table of Contents

Rust ❤️ Emacs

Why Emacs?
Why Rust?
Why A Fork?
Getting Started

Requirements
Building Remacs
Running Remacs
Rustdoc builds


Porting Elisp Primitive Functions: Walkthrough

Porting Widely Used C Functions


Design Goals
Non-Design Goals
Contributing
Help Needed
Rust Porting Tips

C Functions
C Macros
Assertions
Safety





Why Emacs?
Emacs will change how you think about programming.
Emacs is totally introspectable. You can always find out 'what
code runs when I press this button?'.
Emacs is an incremental programming environment. There's no
edit-compile-run cycle. There isn't even an edit-run cycle. You can
execute snippets of code and gradually turn them into a finished
project. There's no distinction between your editor and your
interpreter.
Emacs is a mutable environment. You can set variables, tweak
functions with advice, or redefine entire functions. Nothing is
off-limits.
Emacs provides functionality without applications. Rather than
separate applications, functionality is all integrated into your Emacs
instance. Amazingly, this works. Ever wanted to use the same snippet
tool for writing C++ classes as well as emails?
Emacs is full of incredible software concepts that haven't hit the
mainstream yet. For example:

Many platforms have a single item clipboard. Emacs has an infinite
clipboard.
If you undo a change, and then continue editing, you can't redo the
original change. Emacs allows undoing to any historical state, even
allowing tree-based exploration of history.
Emacs supports a reverse variable search: you can find variables
with a given value.
You can perform structural editing of code, allowing you to make
changes without breaking syntax. This works for lisps (paredit) and
non-lisps (smartparens).
Many applications use a modal GUI: for example, you can't do other
edits during a find-and-replace operation. Emacs provides
recursive editing that allow you to suspend what you're
currently doing, perform other edits, then continue the original
task.

Emacs has a documentation culture. Emacs includes a usage manual,
a lisp programming manual, pervasive docstrings and even an
interactive tutorial.
Emacs has a broad ecosystem. If you want to edit code in a
niche language, there's probably an Emacs package for it.
Emacs doesn't have a monopoly on good ideas, and there are other great
tools out there. Nonetheless, we believe the Emacs learning curve pays
off.
Why Rust?
Rust is a great alternative to C.
Rust has a fantastic learning curve. The documentation is superb,
and the community is very helpful if you get stuck.
Rust has excellent tooling. The compiler makes great suggestions,
the unit test framework is good, and rustfmt helps ensure formatting
is beautiful and consistent.
The Rust packaging story is excellent. It's easy to reuse
the great libraries available, and just as easy to factor out code for
the benefit of others. We can replace entire C files in Emacs with
well-maintained Rust libraries.
Code written in Rust easily interoperates with C. This means we
can port to Rust incrementally, and having a working Emacs at each
step of the process.
Rust provides many compile-time checks, making it much easier to write
fast, correct code (even when using multithreading). This also makes
it much easier for newcomers to contribute.
Give it a try. We think you'll like it.
Why A Fork?
Emacs is a widely used tool with a long history, broad platform
support and strong backward compatibility requirements. The core team
is understandably cautious in making far-reaching changes.
Forking is a longstanding tradition in the Emacs community for trying
different approaches. Notable Emacs forks include XEmacs,
Guile Emacs,
and emacs-jit.
There have also been separate elisp implementations, such as
Deuce,
JEmacs and
El Compilador.
By forking, we can explore new development approaches. We can
use a pull request workflow with integrated CI.
We can drop legacy platforms and compilers. Remacs will never run
on MS-DOS, and that's OK.
There's a difference between the idea of Emacs and the current
implementation of Emacs. Forking allows us to explore being even
more Emacs-y.
Getting Started
Requirements


You will need Rust installed. If you're on macOS, you will need Rust
nightly.


You will need a C compiler and toolchain. On Linux, you can do
something like apt-get install build-essential automake. On
macOS, you'll need Xcode.


You will need some C libraries. On Linux, you can install
everything you need with:
 apt-get install texinfo libjpeg-dev libtiff-dev \
   libgif-dev libxpm-dev libgtk-3-dev libgnutls-dev \
   libncurses5-dev libxml2-dev

On macOS, you'll need libxml2 (via xcode-select --install) and
gnutls (via brew install gnutls).


Building Remacs
$ ./autogen.sh
$ ./configure --enable-rust-debug
$ make

For a release build, don't pass --enable-rust-debug.
The Makefile obeys cargo's RUSTFLAGS variable and additional options
can be passed to cargo with CARGO_FLAGS.
For example:
$ make CARGO_FLAGS=""-vv"" RUSTFLAGS=""-Zunstable-options --pretty""
Running Remacs
You can now run your shiny new Remacs build!
# Using -q to ignore your .emacs.d, so Remacs starts up quickly.
# RUST_BACKTRACE is optional, but useful if your instance crashes.
$ RUST_BACKTRACE=1 src/remacs -q

Rustdoc builds
You can use rustdoc to generate API docs:
# http://stackoverflow.com/a/39374515/509706
$ cargo rustdoc -- \
    --no-defaults \
    --passes strip-hidden \
    --passes collapse-docs \
    --passes unindent-comments \
    --passes strip-priv-imports
You can then open these docs with:
$ cargo doc --open
Porting Elisp Primitive Functions: Walkthrough
Let's look at porting numberp to Rust.
First, make sure you have configured and built Remacs on your
system. You'll probably want to generate TAGS too, so you can jump to
definitions of C functions.
Emacs C uses a lot of macros, so it's also useful to look at the expanded
version of the code.
Define a little file src/dummy.c with the C source of numberp, along
with the lisp.h header file:
#include ""lisp.h""

DEFUN (""numberp"", Fnumberp, Snumberp, 1, 1, 0,
       doc: /* Return t if OBJECT is a number (floating point or integer).  */
       attributes: const)
  (Lisp_Object object)
{
  if (NUMBERP (object))
    return Qt;
  else
    return Qnil;
}
Then expand it with GCC:
$ cd /path/to/remacs
$ gcc -Ilib -E src/dummy.c > dummy_exp.c

This gives us a file that ends with:
static struct Lisp_Subr 
# 3 ""src/dummy.c"" 3 4
_Alignas 
# 3 ""src/dummy.c""
(8) Snumberp = { { PVEC_SUBR << PSEUDOVECTOR_AREA_BITS }, { .a1 = Fnumberp }, 1, 1, ""numberp"", 0, 0}; Lisp_Object Fnumberp


  (Lisp_Object object)
{
  if (NUMBERP (object))
    return Qt;
  else
    return builtin_lisp_symbol (0);
}
We can see we need to define a Snumberp and a Fnumberp. We define
a numberp function that does the actual work, then defun! handles
these definitions for us:
// This is the function that gets called when 
// we call numberp in elisp.
fn numberp(object: LispObject) -> LispObject {
    if lisp::NUMBERP(object) {
        LispObject::constant_t()
    } else {
        LispObject::constant_nil()
    }
}

// defun! defines a wrapper function that calls numberp with
// LispObject values. It also declares a struct that we can pass to
// defsubr so the elisp interpreter knows about our function.
defun!(""numberp"", // the name of our primitive function inside elisp
       Fnumberp(object), // the signature of the wrapper function
       Snumberp, // the name of the struct that describes our function
       numberp, // the rust function we want to call
       1, 1, // min and max number of arguments
       ptr::null(), // our function is not interactive
       // docstring, the last line ensures that *Help* shows the
       // correct calling convention
       ""Return t if OBJECT is a number (floating point or integer).

(fn OBJECT)"");
Finally, we need to delete the old C definition and call defsubr
inside rust_init_syms:
pub extern ""C"" fn rust_init_syms() {
    unsafe {
        // ...
        defsubr(&*yourmodule::Snumberp);
    }
}
You're done! Compile Remacs, try your function with M-x ielm, and
open a pull request. Fame and glory await!
Porting Widely Used C Functions
If your Rust function replaces a C function that is used elsewhere in
the C codebase, you will need to export it. The wrapper function needs
to be exported in lib.rs:
pub use yourmodulename::Fnumberp;
and add a declaration in the C where the function used to be:
// This should take the same number of arguments as the Rust function.
Lisp_Object Fnumberp(Lisp_Object);
Design Goals
Compatibility: Remacs should not break existing elisp code, and
ideally provide the same FFI too.
Similar naming conventions: Code in Remacs should use the same
naming conventions for elisp namespaces, to make translation
straightforward.
This means that an elisp function do-stuff will have a corresponding
Rust function Fdo_stuff, and a declaration struct Sdo_stuff. A
lisp variable do-stuff will have a Rust variable Vdo_stuff and a
symbol 'do-stuff will have a Rust variable Qdo_stuff.
Otherwise, we follow Rust naming conventions, with docstrings noting
equivalent functions or macros in C. When incrementally porting, we
may define Rust functions with the same name as their C predecessors.
Leverage Rust itself: Remacs should make best use of Rust to
ensure code is robust and performant.
Leverage the Rust ecosystem: Remacs should use existing Rust
crates wherever possible, and create new, separate crates where our
code could benefit others.
Great docs: Emacs has excellent documentation, Remacs should be no
different.
Non-Design Goals
etags: The
universal ctags project
supports a wider range of languages and we recommend it instead.
Contributing
Pull requests welcome, no copyright assignment required. This project is under the
Rust code of conduct.
Help Needed
There's lots to do! We keep a list of low hanging fruit here so you can easily choose
one. If you do, please open a new issue to keep track of the task and link to it.
Easy tasks:

 Find a small function in lisp.h and write an equivalent in lisp.rs.
 Improve our unit tests. Currently we're passing Qnil to test
functions, which isn't very useful.
 Add docstrings to public functions in lisp.rs.
 Tidy up messy Rust that's been translated directly from C. Run
rustfmt, add or rename internal variables, run clippy, and so
on.
 Add Rust-level unit tests to elisp functions defined in lib.rs.

Medium tasks:

 Choose an elisp function you like, and port it to rust. Look at
rust-mod for an example.
 Teach describe-function to find functions defined in Rust.
 Expand our Travis configuration to run 'make check', so we know
remacs passes Emacs' internal test suite.
 Expand our Travis configuration to ensure that Rust code has been
formatted with rustfmt
 Set up bors/homu.
 Set up a badge tracking pub struct/function coverage using
cargo-doc-coverage.
 Search the Rust source code for TODO comments and fix them.
 Teach Emacs how to jump to definition for Rust functions.

Big tasks:

 Find equivalent Rust libraries for parts of Emacs, and replace all
the relevant C code. Rust has great libraries for regular
expressions, GUI, terminal UI, managing processes, amongst others.
 Change the elisp float representation to use
nan boxing
rather than allocating floats on the heap.

Rust Porting Tips
C Functions
When writing a Rust version of a C function, give it the same name and
same arguments. If this isn't appropriate, docstrings should say the
equivalent C function to help future porters.
For example, make_natnum mentions that it can be used
in place of XSETFASTINT.
C Macros
For C macros, we try to define a fairly equivalent Rust
function. The docstring should mention the original macro name.
Since the Rust function is not a drop-in replacement, we prefer Rust
naming conventions for the new function.
For the checked arithmetic macros (INT_ADD_WRAPV,
INT_MULTIPLY_WRAPV and so on), you can simply use .checked_add,
.checked_mul from the Rust stdlib.
Assertions
eassert in Emacs C should be debug_assert! in Rust.
emacs_abort() in Emacs C should be panic!(""reason for panicking"")
in Rust.
Safety
LispObject values may represent pointers, so the usual safety
concerns of raw pointers apply.
If you can break memory safety by passing a valid value to a function,
then it should be marked as unsafe. For example:
// This function is unsafe because it's dereferencing the car
// of a cons cell. If `object` is not a cons cell, we'll dereference
// an invalid pointer.
unsafe fn XCAR(object: LispObject) -> LispObject {
    (*XCONS(object)).car
}

// This function is safe because it preserves the contract
// of XCAR: it only passes valid cons cells. We just use
// unsafe blocks instead.
fn car(object: LispObject) -> LispObject {
    if CONSP(object) {
        unsafe {
            XCAR(object)
        }
    } else if NILP(object) {
        Qnil
    } else {
        unsafe {
            wrong_type_argument(Qlistp, object)
        }
    }
}
",batch1,16:59:00,Done
271,wso2/andes,"andes



Branch
Build Status




master




",batch2,8:10:18,Done
272,tonlabs/TON-Compiler,"C and C++ compiler for TVM
Getting C++ toolchain in binary form
This README is mostly about building C++ for TVM which is the most appropriate way to get the compiler and the library for contributors. If your intent is only to use the compiler, we provide binary package you can simply download:

For Ubuntu
For Mac OS X

The binaries are updated on every commit in master, so they are always up to date.
Clang for TVM based on LLVM 7.0.0.
This repository contain

Clang for TVM C and C++ compiler
C++ runtime and SDK headers-only library
C runtime and SDK library
The following guide is about building the compilers and installing them, to find a user guide and example contracts, please refer to the samples repository.

Prerequisites
To build the toolchain, you need a recent C++ toolchain supporting C++17:

MSVC 2017 or newer
Clang 6.0.0 or newer
GCC 7.3.0 or newer
Rust 1.47.0 or newer
Cargo

Stable operation of older toolchains is not guaranteed.
You also need zlib 1.2.3.4 or newer. Python 2.7 is required to run tests. Optionally, you can use ninja-build.
For more info about LLVM software requirements visit: https://llvm.org/docs/GettingStarted.html.
Supported operation systems
We expect the project to build successfully on

Ubuntu 16.04, 18.04
Mac OS Mojave 10.14.3 or higher
Windows 10

Building and installing
To build and to install the compiler use the following script:
$ git clone git@github.com:tonlabs/TON-Compiler.git
$ mkdir build
$ cd build
$ cmake -DCMAKE_INSTALL_PREFIX=/path/to/install -C /path/to/TON-Compiler/cmake/Cache/ton-compiler.cmake ../llvm
$ cmake --build . --target install-distribution

Notes:

/path/to/install must be complete path to the installation folder, otherwise Clang might be unable to find all the libraries, headers and tools by default.
We strongly recommend to use the installed version of the compiler, otherwise it might be unable to find the required headers and tools by itself, so they have to be specified by hands.
A complete C and C++ toolchain require tvm_linker to be built. We recommend to put the liker binary to /path/to/install/bin directory. Otherwise you might need to use -fuse-ld option to spicify full name of the linker.
install-distribution installs only required minimum of tools, while install target copies all the LLVM tools to the installation folder. These additional tools doesn't necessary work with TVM target properly.

Troubleshooting and speeding up the build
Building Clang takes quite a bit of time, below we list some options to speed it up:

Use faster build system via -GXXX option. We recommend to choose ninja-build (you might need to install it first) using -GNinja.
Use a faster linker via -DCMAKE_LINKER=<linker name>. It's known that lld is faster than gold, and gold is faster than ld. On Linux, however, lld might not be a part of the default toolchain, so you might have to install it first.
Use a faster compiler via -DCMAKE_C_COMPILER=<compiler> and -DCMAKE_CXX_COMPILER=<compiler>. Clang might be slightly faster than GCC.
Build dynamically linked version of Clang via -DBUILD_SHARED_LIB=On.
Note that building Clang also require a tens of gigabytes of disk space (especially for a static build) and several gigabytes of RAM to be build. In case, you run out of memory -DLLVM_PARALLEL_LINK_JOBS=N might be of help. This option limits the number of link jobs running in parallel does the footpring. There is also a separate option -DLLVM_PARALLEL_COMPILE_JOBS=N to limit number of compilation jobs running in parallel.
To learn more about possible configurations of LLVM build, please refer to LLVM documentation. In case you are experiencing problems with build, we would appreciete raising an issue in this repository.

Running tests
To run tests for TVM, execute
$ cmake --build . --target check-llvm-codegen-tvm

To run tests for other platforms (to ensure that LLVM itself is not broken), you have to create a separate build without using /path/to/TON-Compiler/cmake/Cache/ton-compiler.cmake config and run
$ cmake --build . --target check all

For more details see testing.md.
Example of usage
You can learn more about C++ for TVM and find examples of usage of C++ toolchain here. C toolchain is mostly for geeks who want to follow TVM assembly closely, but doesn't want to work with stack. C examples might be found here.
Getting support
C and C++ for TVM, being similar to conventional C and C++, has their own extensions and limitations, so if you are getting started with programming for TVM, we recommend to first refer to the examples repository.
Texts, videos and samples illustrating how to use the compiler will soon appear at https://ton.dev/ and https://www.youtube.com/channel/UC9kJ6DKaxSxk6T3lEGdq-Gg. Stay tuned.
You can also get support in TON Dev Telegram channel.
In case you found a bug, raise an issue in the repository. Please attach the source file, the command to reproduce the failure and your machine description.
Contribution policy
The project strives to follow LLVM coding standards and policies as well as C++ Core Guidelines, so before contributing, we recommend you to familiarize with the following documents:

LLVM Developer Policy
LLVM Coding Standards
LLVM Programmer’s Manual
C++ Core Guidelines

Note: Since TVM backend uses C++17 which is not yet fully supported in LLVM, the guidelines could be reasonably adjusted for cases of C++17 usages. C++17 data structures are preferred over LLVM counterparts, for instance, std::optional<T> is better to use w.r.t. llvm::Optional<T>.
All changes in LLVM (excluding lib/Target/TVM subdirectory) must be marked as local changes in the following way:
// TVM local begin
<changed LLVM code>
// TVM local end

The reason is to help resolving merge conflicts when updating LLVM to a new version.
All removals from LLVM must be commented out instead:
#if 0
<removed LLVM code>
#endif

The reason is to minimize number of merge conflicts when updating LLVM to a new version.
To learn more about development a downstream LLVM project, refer to https://llvm.org/devmtg/2015-10/slides/RobinsonEdwards-LivingDownstreamWithoutDrowning.pdf.
Upstreaming
We believe that LLVM community would benefit from getting TVM backend upstream. It's a very distinct architecture, that break several assumptions. For instance, Clang front-end and the optimizer relies on 8-bit bytes byte which is not true for TVM as well as for some non-mainstream processors. Furthermore, target independent code generator is designed for a register machine, so stack machine support is benefitial at least for WebAssembly. So, with some constraints removed, LLVM will be better placed to downstream development and we believe it is feasible without damaging existing targets.
We would like to implement, upstream and maintain the following features:

Byte size specified in data layout, removing magical number 8 from the optimizer.
memset, memcopy, memmove configured with the byte size.
DAG scheduler for a stack machine.
Generic analysis and transformation passes to optimize for stack machine on MIR level.
The current version of Clang for TVM is based on LLVM 7.0.0, it's planned update LLVM, remove hardcoded byte size and adopt opaque types insted of types we introduced to LLVM.

",batch1,16:57:57,Done
273,grayarea11235/enki2,"Enki: A text editor for programmers
Official site
Installation
For most Linux and Windows users: use the pre-built binaries from the official site. But if you feel brave..
master branch can be unstable or even broken. Use releases if you are not going to hack Enki
1. Install dependencies
Mandatory:

Python 3
PyQt5. With SVG support.
Qutepart

Optional:

Python-Markdown. For Markdown preview
python-docutils. For reStructuredText preview
ctags. For navigation in file
regex. For preview synchronization
CodeChat. For source code to HTML translation (literate programming)
Sphinx. To build Sphinx documentation.
Flake8. To lint your Python code.

Debian and Debian based
   apt-get install python3 libqt5svg5 python3-pyqt5 python3-markdown python3-docutils ctags
   pip3 install -r requirements.txt

Install Qutepart from sources.
Other Unixes
Find and install listed packages with your package manager.
Install Qutepart from sources.
Other systems
Go to official pages of the projects, download packages and install according to instructions.
2. Get the sources
Download source archive
3. Install Enki
python3 setup.py install

4. Enjoy
Don't forget to send a bug report if you are having some problems
Running from the source tree
python3 bin/enki

License
GPL v2
Authors

Andrei Kopats (aka hlamer) ported core and some plugins to Python, reworked it and released the result as Enki
Filipe Azevedo, Andrei Kopats and Monkey Studio v2 team developed Monkey Studio v2

The Team
Contacts
enki-editor@googlegroups.com
hlamer@tut.by
",batch2,8:10:17,Done
274,adamprocter/microblog,"Neo Cactus for Jekyll
Demo: https://mmarfil.com/
Screenshot

This Jekyll theme started as a port of Cactus to my own needs, but I ended up performing a lot more modifications than expected. Some people reached me out and asked if I could share it, so here we are.
Disclaimer: I'm only a designer, so please don't expect the code to be pretty.
Usage
To start your project, fork this respository, put in your content, and go!
",batch1,16:57:57,Done
275,djbender/homebrew-tmux,,batch1,16:58:59,Done
276,hathach/tinyusb,"TinyUSB

 
TinyUSB is an open-source cross-platform USB Host/Device stack for embedded system, designed to be memory-safe with no dynamic allocation and thread-safe with all interrupt events are deferred then handled in the non-ISR task function.

.
├── docs            # Documentation
├── examples        # Sample with Makefile build support
├── hw
│   ├── bsp         # Supported boards source files
│   └── mcu         # Low level mcu core & peripheral drivers
├── lib             # Sources from 3rd party such as freeRTOS, fatfs ...
├── src             # All sources files for TinyUSB stack itself.
├── test            # Unit tests for the stack
└── tools           # Files used internally

Contributors
Special thanks to all the people who spent their precious time and effort to help this project so far. Check out the
CONTRIBUTORS.md file for the list of all contributors and their awesome work for the stack.
Supported MCUs
The stack supports the following MCUs:

Dialog: DA1469x
Espressif: ESP32-S2, ESP32-S3
MicroChip: SAMD11, SAMD21, SAMD51, SAME5x, SAMG55
NordicSemi: nRF52833, nRF52840
Nuvoton: NUC120, NUC121/NUC125, NUC126, NUC505
NXP:

iMX RT Series: RT1011, RT1015, RT1021, RT1052, RT1062, RT1064
Kinetis: KL25
LPC Series: 11u, 13, 15, 17, 18, 40, 43, 51u, 54, 55


Raspberry Pi: RP2040
Renesas: RX63N
Silabs: EFM32GG12
Sony: CXD56
ST: STM32 series: L0, F0, F1, F2, F3, F4, F7, H7 both FullSpeed and HighSpeed
TI: MSP430
ValentyUSB eptri

Here is the list of supported Boards that can be used with provided examples.
Device Stack
Supports multiple device configurations by dynamically changing usb descriptors. Low power functions such like suspend, resume, and remote wakeup. Following device classes are supported:

Audio Class 2.0 (UAC2) still work in progress
Bluetooth Host Controller Interface (BTH HCI)
Communication Class (CDC)
Device Firmware Update (DFU): DFU mode (WIP) and Runtinme
Human Interface Device (HID): Generic (In & Out), Keyboard, Mouse, Gamepad etc ...
Mass Storage Class (MSC): with multiple LUNs
Musical Instrument Digital Interface (MIDI)
Network with RNDIS, CDC-ECM (work in progress)
USB Test and Measurement Class (USBTMC)
Vendor-specific class support with generic In & Out endpoints. Can be used with MS OS 2.0 compatible descriptor to load winUSB driver without INF file.
WebUSB with vendor-specific class

If you have special need, usbd_app_driver_get_cb() can be used to write your own class driver without modifying the stack. Here is how RPi team add their reset interface raspberrypi/pico-sdk#197
Host Stack
Most active development is on the Device stack. The Host stack is under rework and largely untested.

Human Interface Device (HID): Keyboard, Mouse, Generic
Mass Storage Class (MSC)
Hub currently only supports 1 level of hub (due to my laziness)

OS Abstraction layer
TinyUSB is completely thread-safe by pushing all ISR events into a central queue, then process it later in the non-ISR context task function. It also uses semaphore/mutex to access shared resources such as CDC FIFO. Therefore the stack needs to use some of OS's basic APIs. Following OSes are already supported out of the box.

No OS
FreeRTOS
Mynewt Due to the newt package build system, Mynewt examples are better to be on its own repo

Getting Started
Here are the details for getting started with the stack.
Porting
Want to help add TinyUSB support for a new MCU? Read here for an explanation on the low-level API needed by TinyUSB.
License
MIT license for all TinyUSB sources src folder, Full license is here. However, each file is individually licensed especially those in lib and hw/mcu folder. Please make sure you understand all the license term for files you use in your project.
Uses
TinyUSB is currently used by these other projects:

Adafruit nRF52 Arduino
Adafruit nRF52 Bootloader
Adafruit SAMD Arduino
CircuitPython
Espressif IDF
MicroPython
mynewt
Raspberry Pi Pico SDK
TinyUF2 Bootloader
TinyUSB Arduino Library

",batch2,8:10:17,Done
277,weigun/llvm,,batch1,16:57:58,Done
278,ajaysharma00/mastodon,"


Mastodon is a free, open-source social network server based on open web protocols like ActivityPub and OStatus. The social focus of the project is a viable decentralized alternative to commercial social media silos that returns the control of the content distribution channels to the people. The technical focus of the project is a good user interface, a clean REST API for 3rd party apps and robust anti-abuse tools.
Click on the screenshot below to watch a demo of the UI:

Ruby on Rails is used for the back-end, while React.js and Redux are used for the dynamic front-end. A static front-end for public resources (profiles and statuses) is also provided.
If you would like, you can support the development of this project on Patreon or Liberapay.

Resources

Frequently Asked Questions
Use this tool to find Twitter friends on Mastodon
API overview
List of Mastodon instances
List of apps
List of sponsors

Features
No vendor lock-in: Fully interoperable with any conforming platform
It doesn't have to be Mastodon, whatever implements ActivityPub or OStatus is part of the social network!
Real-time timeline updates
See the updates of people you're following appear in real-time in the UI via WebSockets. There's a firehose view as well!
Federated thread resolving
If someone you follow replies to a user unknown to the server, the server fetches the full thread so you can view it without leaving the UI
Media attachments like images and short videos
Upload and view images and WebM/MP4 videos attached to the updates. Videos with no audio track are treated like GIFs; normal videos are looped - like vines!
OAuth2 and a straightforward REST API
Mastodon acts as an OAuth2 provider so 3rd party apps can use the API
Fast response times
Mastodon tries to be as fast and responsive as possible, so all long-running tasks are delegated to background processing
Deployable via Docker
You don't need to mess with dependencies and configuration if you want to try Mastodon, if you have Docker and Docker Compose the deployment is extremely easy

Development
Please follow the development guide from the documentation repository.
Deployment
There are guides in the documentation repository for deploying on various platforms.
Contributing
You can open issues for bugs you've found or features you think are missing. You can also submit pull requests to this repository. Here are the guidelines for code contributions
IRC channel: #mastodon on irc.freenode.net
License
Copyright (C) 2016-2018 Eugen Rochko & other Mastodon contributors (see AUTHORS.md)
This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.
This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Affero General Public License for more details.
You should have received a copy of the GNU Affero General Public License along with this program. If not, see https://www.gnu.org/licenses/.

Extra credits
The elephant friend illustrations are created by Dopatwo
",batch2,8:09:15,Done
279,kfogel/emacs,,batch1,16:58:59,Done
280,rirror/bugs.debian.org,"Debbugs
Debian Bug-Tracking System

What is Debbugs?
Debbugs is a stable, scaleable bug reporting and issue tracking system. Debbugs has a web interface for viewing and searching issues in the database but unlike other bug tracking systems, Debbugs has no web interface for editing bug reports - all modification is done via email.
The most notable deployment of Debbugs is on the Debian project
System Requirements

GNU date
GNU gzip
Perl 5 (5.005 is known to work)
Mailtools and MIME-tools perl modules to manipulate email
Lynx 2.7 or later
The bug system requires its own mail domain. It comes with code
which understands how exim, qmail and sendmail deliver mail for such a
domain to a script.
A webserver. For the old system of static HTML pages generated for
bug reports and index pages, this is easiest if the bug system can
write directly to the webspace; for the new system of CGI scripts
that generate web pages on the fly, write access is not required.
Somewhere to run CGI scripts (unless you don't need the web forms for
searching for bugs by number, package, maintainer or submitter).

Where do I get the Source?
Debbugs is managed in git. You can clone the repository into your local
workspace as follows:
    git clone http://bugs-master.debian.org/debbugs-source/debbugs.git

Additional branches are available from:

Don Armstrong

Installation Instructions
Install the Debian package and read /usr/share/doc/debbugs/README.Debian file.
If you can't use the .deb, do the following:


Clone the repo
git clone http://bugs-master.debian.org/debbugs-source/debbugs.git



Create version and spool directory
cd
mkdir version spool



Optional - Retrieve a partial database of bugs for testing


Get a list of rsync targets from Debbugs
 rsync --list-only rsync://bugs-mirror.debian.org



Grab bugs ending in 00
 mkdir -p splool/db-h/00;
 cd spool/db-h;
 rsync -av rsync://bugs-mirror.debian.org/bts-spool-db/00 .;





Optional - Retrieve bts-versions directory for testing purposes
The database obtained in step 3 requires associated version and index information.


Pull versions directory
 rsync -av rsync://bugs-mirror.debian.org/bts-versions/ versions/



Pull index directory
 rsync -av rsync://bugs-mirror.debian.org/bts-spool-index index





Configure Debbugs config


Create a config directory for Debbugs
 sudo mkdir /etc/debbugs



Copy sample configuration to config directory
 sudo cp ~/debbugs/scripts/config.debian /etc/debbugs/config



Update the following variables

$gConfigDir
$gSpoolDir
$gIndicesDir
$gWebDir
$gDocDir

as follows:
 70,72c70,72
 < $gConfigDir = ""/org/bugs.debian.org/etc""; # directory where this file is
 < $gSpoolDir = ""/org/bugs.debian.org/spool""; # working directory
 < $gIndicesDir = ""/org/bugs.debian.org/indices""; # directory where the indices are
 ---
 > $gConfigDir = ""/etc/debbugs""; # directory where this file is
 > $gSpoolDir = ""/path/to/directory/spool""; # working directory
 > $gIndicesDir = ""/path/to/directory/spool/indices""; # directory  where the indices are
 
 74,75c74,75
 < $gWebDir = ""/org/bugs.debian.org/www""; # base location of web pages
 < $gDocDir = ""/org/ftp.debian.org/ftp/doc""; # location of text doc files
 ---
 > $gWebDir = ""/path/to/directory/debbugs/html""; # base location of web pages
 > $gDocDir = ""/path/to/directory/debbugs/doc""; # location of text doc files





Configure Webserver


Copy example apache config
 sudo cp /path/to/directory/debbugs/examples/apache.conf  /etc/apache2/sites-available/debbugs.conf



Update the directory entries and the DocumentRoot and ScriptAlias variables
 5c5
 < DocumentRoot /var/lib/debbugs/www/
 ---
 > DocumentRoot /path/to/directory/debbugs/html/

 10c10
 < <Directory /var/lib/debbugs/www>
 ---
 > <Directory /path/to/directory/debbugs/html>

 16,17c16,17
 < ScriptAlias /cgi-bin/ /var/lib/debbugs/www/cgi/
 < <Directory ""/var/lib/debbugs/www/cgi/"">
 ---
 > ScriptAlias /cgi-bin/ /path/to/directory/debbugs/cgi/
 > <Directory ""/path/to/directory/debbugs/cgi/"">



Enable required apache mods
 sudo a2enmod rewrite
 sudo a2enmod cgid



Install site
 sudo a2ensite debbugs



Reload apache
 sudo service apache2 reload





Install dependencies
 sudo apt-get install libmailtools-perl ed libmime-tools-perl libio-stringy-perl libmldbm-perl liburi-perl libsoap-lite-perl libcgi-simple-perl libparams-validate-perl libtext-template-perl libsafe-hole-perl libmail-rfc822-address-perl liblist-moreutils-perl libtext-template-perl libfile-libmagic-perl libgravatar-url-perl libwww-perl imagemagick libapache2-mod-perl2



Set up libraries


Create symlinks to link source to their expected locations
 sudo mkdir -p /usr/local/lib/site_perl
 sudo ln -s /path/to/directory/debbugs/Debbugs /usr/local/lib/site_perl/

 sudo mkdir -p /usr/share/debbugs/
 sudo ln -s /path/to/directory/debbugs/templates /usr/share/debbugs/





Create required files


Create files
 touch /etc/debbugs/pseudo-packages.description
 touch /etc/debbugs/Source_maintainers
 touch /etc/debbugs/pseudo-packages.maintainers
 touch /etc/debbugs/Maintainers
 touch /etc/debbugs/Maintainers.override
 mkdir /etc/debbugs/indices
 touch /etc/debbugs/indices/sources



Test
 cd /path/to/directory/debbugs
 perl -c cgi/bugreport.cgi
 REQUEST_METHOD=GET QUERY_STRING=""bug=775300"" perl cgi/bugreport.cgi; 





Install MTA. See README.mail for details.


Note that each line of /etc/debbugs/Maintainers file needs to be formatted as
follows:
package    maintainer name <email@address>

If you need a template, look in /usr/share/doc/debbugs/examples/ directory.
How do I contribute to Debbugs?
Debbugs bugs
Bugs in debbugs are tracked on the Debian bugtracker. The web interface is available at
bugs.debian.org
Start contributing
Make a working branch for your code and check it out to start working:
    git checkout -b example-branch

Stage and commit your changes using appropriate commit messages
    git add example-file

    git commit -m ""Created an example file to demonstrate basic git commands.""

Submitting a Patch
Submitting a patch can be done using git format-patch.
For example
    git format-patch origin/master

Creates .patch files for all commits since the branch diverged from master.
Debbugs bugs are tracked using debbugs (what else). Patches should therefore be
attached to the bug report for the issue. This can be done by emailing the
.patch files to xxxx@bugs.debian.org (where xxxx is the bug number).
Feature patches can also be emailed to the maintaining list at
Debugs mailing list
Further Information and Assistance
Email


Mailing List debian-debbugs@lists.debian.org


To subscribe to the mailing list, email
debian-debbugs-request@lists.debian.org with the word ""subscribe"" in the
subject line.


Website

Code
Debbugs Team

IRC
Join the #debbugs channel on irc.oftc.net
Developers
This bug tracking system was developed by Ian Jackson from 1994-1997,
with assistance from nCipher Corporation Limited in 1997. nCipher allowed
Ian to redistribute modifications he made to the system while working as an
employee of nCipher.
Since then, it has been developed by the various administrators of
bugs.debian.org, including Darren Benham, Adam Heath, Josip Rodin, Anthony
Towns, and Colin Watson. As in the case of Ian, nCipher allowed Colin to
redistribute modifications he made while working as an employee of nCipher.
Copyright and Lack-of-Warranty Notice

Copyright 1999 Darren O. Benham
Copyright 1994-1997 Ian Jackson
Copyright 1997,2003 nCipher Corporation Limited

This bug system is free software; you can redistribute it and/or modify it
under the terms of the GNU General Public License as published by the Free
Software Foundation; version 2 of the License.
This program and documentation is distributed in the hope that it will be
useful, but without any warranty; without even the implied warranty of
merchantability or fitness for a particular purpose. See the GNU General
Public License for more details.
You should have received a copy of the GNU General Public License along
with this program, or one should be available above; if not, write to the
Free Software Foundation, 59 Temple Place - Suite 330, Boston, MA
02111-1307, USA.
",batch2,8:10:18,Done
281,ruby-compiler-survey/llvm,"The LLVM Compiler Infrastructure
This directory and its sub-directories contain source code for LLVM,
a toolkit for the construction of highly optimized compilers,
optimizers, and run-time environments.
The README briefly describes how to get started with building LLVM.
For more information on how to contribute to the LLVM project, please
take a look at the
Contributing to LLVM guide.
Getting Started with the LLVM System
Taken from https://llvm.org/docs/GettingStarted.html.
Overview
Welcome to the LLVM project!
The LLVM project has multiple components. The core of the project is
itself called ""LLVM"". This contains all of the tools, libraries, and header
files needed to process intermediate representations and converts it into
object files.  Tools include an assembler, disassembler, bitcode analyzer, and
bitcode optimizer.  It also contains basic regression tests.
C-like languages use the Clang front end.  This
component compiles C, C++, Objective-C, and Objective-C++ code into LLVM bitcode
-- and from there into object files, using LLVM.
Other components include:
the libc++ C++ standard library,
the LLD linker, and more.
Getting the Source Code and Building LLVM
The LLVM Getting Started documentation may be out of date.  The Clang
Getting Started page might have more
accurate information.
This is an example work-flow and configuration to get and build the LLVM source:


Checkout LLVM (including related sub-projects like Clang):


git clone https://github.com/llvm/llvm-project.git


Or, on windows, git clone --config core.autocrlf=false  https://github.com/llvm/llvm-project.git




Configure and build LLVM and Clang:


cd llvm-project


mkdir build


cd build


cmake -G <generator> [options] ../llvm
Some common build system generators are:

Ninja --- for generating Ninja
build files. Most llvm developers use Ninja.
Unix Makefiles --- for generating make-compatible parallel makefiles.
Visual Studio --- for generating Visual Studio projects and
solutions.
Xcode --- for generating Xcode projects.

Some Common options:


-DLLVM_ENABLE_PROJECTS='...' --- semicolon-separated list of the LLVM
sub-projects you'd like to additionally build. Can include any of: clang,
clang-tools-extra, libcxx, libcxxabi, libunwind, lldb, compiler-rt, lld,
polly, or debuginfo-tests.
For example, to build LLVM, Clang, libcxx, and libcxxabi, use
-DLLVM_ENABLE_PROJECTS=""clang;libcxx;libcxxabi"".


-DCMAKE_INSTALL_PREFIX=directory --- Specify for directory the full
path name of where you want the LLVM tools and libraries to be installed
(default /usr/local).


-DCMAKE_BUILD_TYPE=type --- Valid options for type are Debug,
Release, RelWithDebInfo, and MinSizeRel. Default is Debug.


-DLLVM_ENABLE_ASSERTIONS=On --- Compile with assertion checks enabled
(default is Yes for Debug builds, No for all other build types).




cmake --build . [-- [options] <target>] or your build system specified above
directly.


The default target (i.e. ninja or make) will build all of LLVM.


The check-all target (i.e. ninja check-all) will run the
regression tests to ensure everything is in working order.


CMake will generate targets for each tool and library, and most
LLVM sub-projects generate their own check-<project> target.


Running a serial build will be slow.  To improve speed, try running a
parallel build.  That's done by default in Ninja; for make, use the option
-j NNN, where NNN is the number of parallel jobs, e.g. the number of
CPUs you have.




For more information see CMake




Consult the
Getting Started with LLVM
page for detailed information on configuring and compiling LLVM. You can visit
Directory Layout
to learn about the layout of the source code tree.
",batch1,16:57:58,Done
282,mbrukman/llvm-2,,batch1,16:57:58,Done
283,sagpant/forgottenserver,"forgottenserver   
The Forgotten Server is a free and open-source MMORPG server emulator written in C++. It is a fork of the OpenTibia Server project. To connect to the server, you can use OTClient or OpenTibiaUnity.
Getting Started

Compiling, alternatively download AppVeyor builds for Windows
Scripting Reference

Support
If you need help, please visit the support forum on OTLand. Our issue tracker is not a support forum, and using it as one will result in your issue being closed. If you were unable to get assistance in the support forum, you should consider becoming a premium user on OTLand which grants you access to the premium support forum and supports OTLand financially.
Issues
We use the issue tracker on GitHub. Keep in mind that everyone who is watching the repository gets notified by e-mail when there is activity, so be thoughtful and avoid writing comments that aren't meaningful for an issue (e.g. ""+1""). If you'd like for an issue to be fixed faster, you should either fix it yourself and submit a pull request, or place a bounty on the issue.
",batch2,8:10:17,Done
284,MICommunity/psi-jami,"PSI-JAMI



Master
Develop
Latest release










Introduction
The aim of JAMI is to provide a single java library and framework which unifies the standard formats such as PSI-MI XML and PSI-MITAB.
The JAMI model interfaces are abstracted from both formats to hide the complexity/requirements of each format. The development of softwares and tools on top of this framewrok simplifies the integration and support of the two standard formats.
It avoids endless conversions to different formats and avoid code/unit test duplicate as the code becomes more modular.
The utils package contains a collection of various helper classes. We're not sure if all of this classes or really necessary, but we need to use them for now.

",batch2,8:09:16,Done
285,grahamc/notpkgs,"

Nixpkgs is a collection of packages for the Nix package
manager. It is periodically built and tested by the Hydra
build daemon as so-called channels. To get channel information via git, add
nixpkgs-channels as a remote:
% git remote add channels https://github.com/NixOS/nixpkgs-channels.git

For stability and maximum binary package support, it is recommended to maintain
custom changes on top of one of the channels, e.g. nixos-18.09 for the latest
release and nixos-unstable for the latest successful build of master:
% git remote update channels
% git rebase channels/nixos-18.09

For pull-requests, please rebase onto nixpkgs master.
NixOS Linux distribution source code is located inside
nixos/ folder.

NixOS installation instructions
Documentation (Nix Expression Language chapter)
Manual (How to write packages for Nix)
Manual (NixOS)
Community maintained wiki
Continuous package builds for unstable/master
Continuous package builds for 18.09 release
Tests for unstable/master
Tests for 18.09 release

Communication:

Discourse Forum
IRC - #nixos on freenode.net

Note: MIT license does not apply to the packages built by Nixpkgs, merely to
the package descriptions (Nix expressions, build scripts, and so on). It also
might not apply to patches included in Nixpkgs, which may be derivative works
of the packages to which they apply. The aforementioned artifacts are all
covered by the licenses of the respective packages.
",batch1,16:58:59,Done
286,kiang/TainanWaterMap,,batch1,16:58:59,Done
287,myokomu/emacs-mac,,batch1,16:59:00,Done
288,KhronosGroup/LLVM-SPIRV-Backend,"The LLVM Compiler Infrastructure
This directory and its sub-directories contain source code for LLVM,
a toolkit for the construction of highly optimized compilers,
optimizers, and run-time environments.
The README briefly describes how to get started with building LLVM.
For more information on how to contribute to the LLVM project, please
take a look at the
Contributing to LLVM guide.
Getting Started with the LLVM System
Taken from https://llvm.org/docs/GettingStarted.html.
Overview
Welcome to the LLVM project!
The LLVM project has multiple components. The core of the project is
itself called ""LLVM"". This contains all of the tools, libraries, and header
files needed to process intermediate representations and converts it into
object files.  Tools include an assembler, disassembler, bitcode analyzer, and
bitcode optimizer.  It also contains basic regression tests.
C-like languages use the Clang front end.  This
component compiles C, C++, Objective-C, and Objective-C++ code into LLVM bitcode
-- and from there into object files, using LLVM.
Other components include:
the libc++ C++ standard library,
the LLD linker, and more.
Getting the Source Code and Building LLVM
The LLVM Getting Started documentation may be out of date.  The Clang
Getting Started page might have more
accurate information.
This is an example work-flow and configuration to get and build the LLVM source:


Checkout LLVM (including related sub-projects like Clang):


git clone https://github.com/llvm/llvm-project.git


Or, on windows, git clone --config core.autocrlf=false  https://github.com/llvm/llvm-project.git




Configure and build LLVM and Clang:


cd llvm-project


mkdir build


cd build


cmake -G <generator> [options] ../llvm
Some common build system generators are:

Ninja --- for generating Ninja
build files. Most llvm developers use Ninja.
Unix Makefiles --- for generating make-compatible parallel makefiles.
Visual Studio --- for generating Visual Studio projects and
solutions.
Xcode --- for generating Xcode projects.

Some Common options:


-DLLVM_ENABLE_PROJECTS='...' --- semicolon-separated list of the LLVM
sub-projects you'd like to additionally build. Can include any of: clang,
clang-tools-extra, libcxx, libcxxabi, libunwind, lldb, compiler-rt, lld,
polly, or debuginfo-tests.
For example, to build LLVM, Clang, libcxx, and libcxxabi, use
-DLLVM_ENABLE_PROJECTS=""clang;libcxx;libcxxabi"".


-DCMAKE_INSTALL_PREFIX=directory --- Specify for directory the full
path name of where you want the LLVM tools and libraries to be installed
(default /usr/local).


-DCMAKE_BUILD_TYPE=type --- Valid options for type are Debug,
Release, RelWithDebInfo, and MinSizeRel. Default is Debug.


-DLLVM_ENABLE_ASSERTIONS=On --- Compile with assertion checks enabled
(default is Yes for Debug builds, No for all other build types).




cmake --build . [-- [options] <target>] or your build system specified above
directly.


The default target (i.e. ninja or make) will build all of LLVM.


The check-all target (i.e. ninja check-all) will run the
regression tests to ensure everything is in working order.


CMake will generate targets for each tool and library, and most
LLVM sub-projects generate their own check-<project> target.


Running a serial build will be slow.  To improve speed, try running a
parallel build.  That's done by default in Ninja; for make, use the option
-j NNN, where NNN is the number of parallel jobs, e.g. the number of
CPUs you have.




For more information see CMake




Consult the
Getting Started with LLVM
page for detailed information on configuring and compiling LLVM. You can visit
Directory Layout
to learn about the layout of the source code tree.
",batch1,16:57:58,Done
289,conceptslearningmachine/llvm-2,,batch1,16:57:58,Done
290,npc-sw/cmssw,,batch1,16:57:58,Done
291,adsharma/libunwind,,batch2,8:09:16,Done
292,iquiw/emacs-Win32-IME,,batch1,16:58:59,Done
293,palmermarc/vampirewars,"
ranvier
Node.js-based MUD engine
Ranvier is a MUD game engine whose goal is to be a simple but powerful way to build whatever MUD you want with special care given to extensibility. The core code strives to be completely unopinionated toward any specific style of game while using the bundle system to build the game you want without having to dig through the engine's code.
Special Features

Robust bundle system: Nearly every aspect of the game can be modified without changing the core and allows for easy
packaging and sharing of commands/areas/items/npcs/channels/behaviors
Unopinionated network layer: easily swap out telnet for any network layer you like. No need to gut the whole codebase
just to support a different transport type, just drop in a file.
Customizable data layer: You are not tied to saving in any particular database or file storage sytem
Optional coordinate based room system allowing for the flexibilty of a standard MUD world with the easy mappability of
a strict 3D world.
Scripting for all entities in the game for any event along with behaviors to create shared, composable scripts
Skill system with passive/active skills
Effects e.g., buffs/debuffs
Quest system allowing for starting/progress/completion from any event in the game
Communication channels with custom audiences

Documentation
Ranvier prides itself on having thorough documentation which is available on our website: ranviermud.com
Slack
We have a Slack channel you can use to ask questions, suggest features, or just keep up to date with the project: https://ranviermud.slack.com
Get an invite
Requirements

Node.js >= v10.12.0

Demo
Point your favorite client or telnet to ranviermud.com port 4000. This demo server is wiped and updated from the master branch every hour.
",batch2,8:10:18,Done
294,sauravkumar2014/ASCEND,,batch2,8:09:16,Done
295,thornjad/thornmacs,"aero
Emacs, Rust, Spacemacs and more
     

Table of Contents

What aero Is
What aero Is Not
Difference With Remacs+Spacemacs
Tailored Use-Cases
Why Emacs?
Why Remacs?
Why Spacemacs?
Getting Started

Requirements
Dockerized development environment
Building aero
Running aero


Design Goals
Contributing
License

What aero is
aero is, simply put, a fusion of Spacemacs and Remacs, yielding a
day-to-day editor and reliable sidekick.
aero is:

Emacs at heart
Integrated Spacemacs
Increasingly Rust-powered
Heavily tailored
Emacs-ily extensible
Smaller

What aero is not
aero is not, and is not intended to be, a widespread editor. I
(thornjad) make this for myself and my own use-cases. While I try to
leave it open for extensiblity via Emacs Lisp, aero cannot be
counted on to have all the bells and whistles that come with GNU
Emacs.
aero is not:

Intended for mass use
Supportive of things I (thornjad) don't like
Backed by a team
Guranteed to work

Difference With Remacs+Spacemacs
aero is a fusion of Remacs and Spacemacs, but goes further. Lambda:

Is smaller
Does not support the NT kernel
Does not use X by default (check out the with-gui branch)
Does not contain many default packages which Emacs and Spacemacs
provide

Tailored Use-Cases
aero is tailored to the use-case of my (thornjad) life. As such, it
is designed with these uses in mind:

TCL backend and Apache Rivet frontend
Lisp, primarily Common Lisp (SBCL), also HCL, Orson, and of course
Emacs lisp
Javascript, for Node and Web, with Handlebars and other frameworks
Coffeescript for Node
Occasional C, C++, Make, SQL
Heavy git usage, both Magit and CLI (via gitsh)
Heavy SSH usage via TRAMP
Documentation in Markdown, Org-mode and other formats
Terminal-oriented development (Alacritty + Tmux)
Unix-only (GNU/Linux and MacOS)

Why Emacs?
Emacs will change how you think about programming. Emacs is entirely
introspectable, and runs in a fully mutable and extensible
environment. It also has a fantastic culture, built around great
documentation, even providing an interactive tutorial. It also comes
with a broad ecosystem, with support running back decades.
Additionally, Emacs is, essentially, a Lisp machine, and so bringing
all the power and might of Lisp with it, inherently. This makes
working with Lisp ridiculously easy.
Besides, check out the learning
curve.
Why Remacs?
Remacs offers Rust as an alternative to C. Though C is arguably
timeless, Remacs may make Emacs development more accessible to a
younger generation, allowing them to eventually take the torch from
the current core and bring Emacs into the future.
Additionally, a full port to Rust may offer a speedup, in run time as
well as compile time and developer time. It may also speed along the
full development of needed features, such as fully-asynchronous
operation.
See the [Remacs project](https://github.com/Wilfred/remacs* for more
information.
Why Spacemacs?
Spacemacs is a beautiful and thriving project that provides aero
with two major benefits:
Key bindings

Emacs key bindings suck, Spacemacs bindings (inspired by Vim are
good
No need to constantly press modifier keys
Space as a leader key is brilliant, simple, and ergonomical

Extensibility

Emacs is highly extensible. This is good, but...
Spacemacs makes it easier. The single .spacemacs configuration
file is intuitive in a way pure Emacs doesn't quite achieve
Spacemacs also provides sensible default packages. Emacs does as
well, but many are antiquated and unused

Getting Started
Requirements


You will need
Rust installed.
The file rust-toolchain indicates the version that gets installed.
This happens automatically, so don't override the toolchain manually.


You will need a C compiler and toolchain. On Linux, you can do
something like:
 apt install build-essential automake clang libclang-dev

On macOS, you'll need Xcode.


Linux:
 apt install texinfo libjpeg-dev libtiff-dev \
   libgif-dev libxpm-dev libgtk-3-dev libgnutls28-dev \
   libncurses5-dev libxml2-dev libxt-dev

macOS:
 brew install gnutls texinfo autoconf

To use the installed version of makeinfo instead of the built-in
(/usr/bin/makeinfo) one, you'll need to make sure
/usr/local/opt/texinfo/bin is before /usr/bin in
PATH. Mojave install libxml2 headers with: open /Library/Developer/CommandLineTools/Packages/macOS_SDK_headers_for_macOS_10.14.pkg


Dockerized development environment
If you don't want to bother with the above setup you can use the
provided Docker environment. Make sure you have
docker 1.12+ and
docker-compose 1.8+ available.
To spin up the environment run
docker-compose up -d
The first time you run this command, Docker will build the image. After
that any subsequent startups will happen in less than a second. If
this command fails because of needing absolute paths, make sure to set
the PWD environment variable before calling the command like so:
PWD=$(pwd) docker-compose up -d
The working directory with aero will be mounted under the same path
in the container so editing the files on your host machine will
automatically be reflected inside the container. To build aero use
the steps from Building aero prefixed with
docker-compose exec aero, this will ensure the commands are
executed inside the container.
Building aero
$ ./autogen.sh
$ ./configure --enable-rust-debug
$ make

For a release build, don't pass --enable-rust-debug.
The Makefile obeys cargo's RUSTFLAGS variable and additional options
can be passed to cargo with CARGO_FLAGS.
For example:
$ make CARGO_FLAGS=""-vv"" RUSTFLAGS=""-Zunstable-options --cfg MARKER_DEBUG""
Running aero
You can now run your shiny new aero build!
# Using -q to ignore your .emacs.d, so aero starts up quickly.
# RUST_BACKTRACE is optional, but useful if your instance crashes.
$ RUST_BACKTRACE=1 src/aero -q
Design Goals
Tailored experience: aero is highly tailored to me
(thornjad). This is the primary purpose of this project; if you think
this is dumb, I encourage you to fork this project and build it the
way you want.
Documentation: Emacs and Spacemacs have a culture of great
documentation. aero should be no different.
Power of Rust: Through Remacs, aero should benefit from the
performance and robustness of Rust, and the ecosystem that comes with
it.
Contributing

Pull requests are welcome, but keep in mind the target audience
here. Your contribution is likely to have more impact in
Remacs or
Spacemacs.
License

aero is covered under a GPLv3 License, exactly the same as its
constituent software, GNU
Emacs,
Remacs and
Spacemacs. Please see the
license file (LICENSE) and the accompanying
etc directory. Additionally, some software packages
shipped in this repository carry their own copyright, in which case you can
refer to that package and/or its header(s).
",batch1,16:59:00,Done
296,aquamacs-emacs/aquamacs-emacs,"Aquamacs Emacs
This project is Aquamacs Emacs, a modified distribution of Emacs for Mac OS X.
Aquamacs tracks GNU Emacs, merging from the latest release branch.
Aquamacs is not a fork.
Aquamacs is designed to make it easy for users to get started with Emacs.
It also makes it easy to switch between different programs on the Mac.
Further, Aquamacs comes with many packages pre-installed and configured
so that users can, for example, start editing LaTeX documents with AUCTeX
or statistical programs in R using ESS-Mode.  Many programming languages
are supported, and sometimes Aquamacs provides, by default, other major
modes for certain programming languages than what comes with GNU Emacs.
Development on Aquamacs contributes back to GNU Emacs where appropriate.
Aquamacs Emacs is licensed under the GNU General Public License, v3.
Project website:  http://aquamacs.org
Discussion of Aquamacs takes place on the email list macosx-emacs@email.esm.psu.edu
Building Aquamacs
Run ./build-aquamacs
For more details about building Aquamacs, see ./aquamacs/build/BUILD-AQUAMACS.txt
Contributing to Aquamacs
Discussion about Aquamacs development takes place on the email list  aquamacs-devel@googlegroups.com, or in issues filed at https://github.com/aquamacs-emacs/aquamacs-emacs/issues.
Author
Aquamacs Emacs started in 2005.
Until 2019, it was maintained by David Reitter, david.reitter@gmail.com.
Aquamacs is currently maintained by Win Treese, treese@acm.org.
Many developers have contributed to its success.
",batch1,16:59:01,Done
297,asaf53409/aa,"

    Simple yet flexible JavaScript charting for designers & developers








Documentation

Introduction
Getting Started
General
Configuration
Charts
Axes
Developers
Popular Extensions
Samples

Contributing
Instructions on building and testing Chart.js can be found in the documentation. Before submitting an issue or a pull request, please take a moment to look over the contributing guidelines first. For support, please post questions on Stack Overflow with the chartjs tag.
License
Chart.js is available under the MIT license.
",batch2,8:10:18,Done
298,SuperElastix/SimpleElastix,"What is SimpleElastix?
Image registration is the process of transforming images into a common coordinate system so corresponding pixels represent homologous biological points. SimpleElastix is an extension of SimpleITK that offers a user-friendly API to the popular image registration algorithms of the elastix C++ library. This makes state-of-the-art medical image registration really easy to do in languages like Python, Java, C# and R. This package provides

elastix and transformix bindings for C++, Python, Java, R, Ruby, Octave, Lua, Tcl and C# (see elastix manual for a list of supported registration algorithms).
Pre-configured registration methods that work well in many cases and serve as starting points for tuning elastix to domain-specific applications.
Installation guides, examples, and introductory material at simpleelastix.readthedocs.org.
A user-friendly API that aligns with the design philosophy of SimpleITK developed specifically for rapid prototyping. If you are interested, The Design of SimpleITK is a great read.
The complete set of SimpleITK image processing algorithms.

Enough talk, time for some examples! We will use Python for the following code. Say you need to register two images. This can be accomplished with a single line of code:
resultImage = SimpleITK.Elastix(sitk.ReadImage(""fixedImage.dcm""), sitk.ReadImage(""movingImage.dcm""))
Under the hood, Elastix will use stochastic optimization for maximum speed, a multi-resolution strategy and several different transforms of increasing complexity for maximum robustness. All aspects of the registration procedure can be customized via parameter maps.
SimpleElastix can also be used for more complex image processing pipelines. Say you want to compare the volume, mean intensity and standard deviation of the intensity of anatomical structures across a population of images using an atlas segmentation. We can accomplish this task with the following lines of Python code:
import SimpleITK as sitk

# The atlas and associated segmentation is loaded once and held in memory
movingImage = sitk.ReadImage('atlasImage.hdr')
movingLabel = sitk.ReadImage('atlasLabel.hdr')

# Images are loaded from disk one at a time. Here we specify an array of 
# paths to images which we will loop over. 
population = ['image1.dcm', 'image2.dcm', ... , 'imageN.dcm']

selx = sitk.ElastixImageFilter()
selx.SetMovingImage(movingImage)
selx.SetParameterMap(selx.GetDefaultParameterMap('nonrigid'))

for filename in population
  # Register images
  fixedImage = sitk.ReadImage(filename)
  selx.SetFixedImage(fixedImage)
  selx.Execute()

  # Transform label map using the deformation field from above
  resultLabel = sitk.Transformix(movingLabel, selx.GetTransformParameterMap())

  # Compute statistics for label 1
  LabelStatistics = sitk.LabelStatisticsImageFilter()
  LabelStatistics.Execute(fixedImage, sitk.Cast(resultLabel, sitk.sitkInt8))
  LabelStatistics.GetCount(1)
  LabelStatistics.GetMean(1)
  LabelStatistics.GetVariance(1)
  # etc etc
This example demonstrates the efficiency of combining SimpleElastix's object oriented interface (the way we used elastix to register images) and procedural interface (the way we used transformix to warp labels) with SimpleITK (the way we computed statistics). Previously, using elastix and transformix on large datasets would incur a significant overhead, from scripting command line invocations and arguments to copying images and transform parameter files across folders. With SimpleElastix this complexity is easier to manage and more memory and disk I/O efficient. For more examples see the documentation.
Building with the SuperBuild
SimpleElastix integrates elastix and transformix with the SimpleITK SuperBuild. Simply clone this repository and invoke the SuperBuild as outlined in the documentation. The SuperBuild will download and install dependencies (elastix, ITK, SimpleITK and SWIG) and compile SimpleElastix. Target language dependencies need to be pre-installed, e.g. sudo apt-get install cmake swig monodevelop r-base r-base-dev ruby ruby-dev python python-dev tcl tcl-dev tk tk-dev. Note that this project takes around an hour to build on a quad-core machine.
The documentation further describes how to build SimpleElastix on Windows and how to build SimpleElastix manually without the SuperBuild.
SimpleElastix has been tried and tested on Ubuntu 14.10 using GCC 4.9.2 and Clang 3.4.0, Mac OSX Yosemite using Apple Clang 600.0.56 and Windows 8.1 using Microsft Visual Studio 2012 C++ compiler.
About
If you are interested in my work you are most welcome to visit my website.
",batch2,8:09:15,Done
299,kingdavid72/deepchem,"DeepChem


DeepChem aims to provide a high quality open-source toolchain that
democratizes the use of deep-learning in drug discovery, materials science, quantum chemistry, and biology.
Table of contents:

Requirements
Installation

Conda Environment
Direct from Source
Docker


FAQ
Getting Started

Input Formats
Data Featurization
Performances


Contributing to DeepChem

Code Style Guidelines
Documentation Style Guidelines
Gitter


DeepChem Publications
Corporate Supporters

Schrödinger
DeepCrystal


Examples
About Us

Requirements

pandas
rdkit
boost
joblib
sklearn
numpy
six
mdtraj
tensorflow

Installation
Installation from source is the only currently supported format. deepchem currently supports both Python 2.7 and Python 3.5, but is not supported on any OS'es except 64 bit linux. Please make sure you follow the directions below precisely. While you may already have system versions of some of these packages, there is no guarantee that deepchem will work with alternate versions than those specified below.
Note that when using Ubuntu 16.04 server or similar environments, you may need to ensure libxrender is provided via e.g.:
sudo apt-get install -y libxrender-dev
Using a conda environment
You can install deepchem in a new conda environment using the conda commands in scripts/install_deepchem_conda.sh
git clone https://github.com/deepchem/deepchem.git      # Clone deepchem source code from GitHub
cd deepchem
bash scripts/install_deepchem_conda.sh deepchem
source activate deepchem
pip install tensorflow-gpu==1.3.0                      # If you want GPU support
python setup.py install                                 # Manual install
nosetests -v deepchem --nologcapture                    # Run tests
This creates a new conda environment deepchem and installs in it the dependencies that
are needed. To access it, use the source activate deepchem command.
Check this link for more information about
the benefits and usage of conda environments. Warning: Segmentation faults can still happen
via this installation procedure.
Easy Install via Conda
conda install -c deepchem -c rdkit -c conda-forge -c omnia deepchem=1.2.0
Installing Dependencies Manually


Download the 64-bit Python 2.7 or Python 3.5 versions of Anaconda for linux here.
Follow the installation instructions


rdkit
conda install -c rdkit rdkit


joblib
conda install joblib


six
pip install six


networkx
conda install -c anaconda networkx=1.11


mdtraj
conda install -c omnia mdtraj


pdbfixer
conda install -c omnia pdbfixer=1.4


tensorflow: Installing tensorflow on older versions of Linux (which
have glibc < 2.17) can be very challenging. For these older Linux versions,
contact your local sysadmin to work out a custom installation. If your
version of Linux is recent, then the following command will work:
pip install tensorflow-gpu==1.3.0



deepchem: Clone the deepchem github repo:
git clone https://github.com/deepchem/deepchem.git
cd into the deepchem directory and execute
python setup.py install


To run test suite, install nosetests:


pip install nose
Make sure that the correct version of nosetests is active by running
which nosetests
You might need to uninstall a system install of nosetests if
there is a conflict.

If installation has been successful, all tests in test suite should pass:
nosetests -v deepchem --nologcapture
Note that the full test-suite uses up a fair amount of memory.
Try running tests for one submodule at a time if memory proves an issue.

Using a Docker Image
For major releases we will create docker environments with everything pre-installed.
In order to get GPU support you will have to use the
nvidia-docker plugin.
# This will the download the latest stable deepchem docker image into your images
docker pull deepchemio/deepchem

# This will create a container out of our latest image with GPU support
nvidia-docker run -i -t deepchemio/deepchem

# You are now in a docker container whose python has deepchem installed
# For example you can run our tox21 benchmark
cd deepchem/examples
python benchmark.py -d tox21

# Or you can start playing with it in the command line
pip install jupyter
ipython
import deepchem as dc
FAQ


Question: I'm seeing some failures in my test suite having to do with MKL
Intel MKL FATAL ERROR: Cannot load libmkl_avx.so or libmkl_def.so.
Answer: This is a general issue with the newest version of scikit-learn enabling MKL by default. This doesn't play well with many linux systems. See BVLC/caffe#3884 for discussions. The following seems to fix the issue
conda install nomkl numpy scipy scikit-learn numexpr
conda remove mkl mkl-service


Getting Started
The first step to getting started is looking at the examples in the examples/ directory. Try running some of these examples on your system and verify that the models train successfully. Afterwards, to apply deepchem to a new problem, try starting from one of the existing examples and modifying it step by step to work with your new use-case.
Input Formats
Accepted input formats for deepchem include csv, pkl.gz, and sdf files. For
example, with a csv input, in order to build models, we expect the
following columns to have entries for each row in the csv file.

A column containing SMILES strings [1].
A column containing an experimental measurement.
(Optional) A column containing a unique compound identifier.

Here's an example of a potential input file.



Compound ID
measured log solubility in mols per litre
smiles




benzothiazole
-1.5
c2ccc1scnc1c2



Here the ""smiles"" column contains the SMILES string, the ""measured log
solubility in mols per litre"" contains the experimental measurement and
""Compound ID"" contains the unique compound identifier.
[2] Anderson, Eric, Gilman D. Veith, and David Weininger. ""SMILES, a line
notation and computerized interpreter for chemical structures."" US
Environmental Protection Agency, Environmental Research Laboratory, 1987.
Data Featurization
Most machine learning algorithms require that input data form vectors.
However, input data for drug-discovery datasets routinely come in the
format of lists of molecules and associated experimental readouts. To
transform lists of molecules into vectors, we need to subclasses of DeepChem
loader class dc.data.DataLoader such as dc.data.CSVLoader or
dc.data.SDFLoader. Users can subclass dc.data.DataLoader to
load arbitrary file formats. All loaders must be
passed a dc.feat.Featurizer object. DeepChem provides a number of
different subclasses of dc.feat.Featurizer for convenience.
Performances

Classification

Index splitting



Dataset
Model
Train score/ROC-AUC
Valid score/ROC-AUC




clintox
Logistic regression
0.969
0.683



Random forest
0.995
0.763



XGBoost
0.879
0.890



IRV
0.762
0.811



MT-NN classification
0.929
0.832



Robust MT-NN
0.948
0.840



Graph convolution
0.961
0.812



DAG
0.997
0.660



Weave
0.937
0.887


hiv
Logistic regression
0.861
0.731



Random forest
0.999
0.720



XGBoost
0.917
0.745



IRV
0.841
0.724



NN classification
0.712
0.676



Robust NN
0.740
0.699



Graph convolution
0.888
0.771



Weave
0.880
0.758


muv
Logistic regression
0.957
0.754



XGBoost
0.895
0.714



MT-NN classification
0.900
0.746



Robust MT-NN
0.937
0.765



Graph convolution
0.890
0.804



Weave
0.749
0.764


pcba
Logistic regression
0.807
0.773



XGBoost
0.931
0.847



MT-NN classification
0.819
0.792



Robust MT-NN
0.812
0.782



Graph convolution
0.886
0.851


sider
Logistic regression
0.932
0.622



Random forest
1.000
0.669



XGBoost
0.829
0.639



IRV
0.649
0.643



MT-NN classification
0.781
0.630



Robust MT-NN
0.805
0.634



Graph convolution
0.744
0.593



DAG
0.908
0.558



Weave
0.622
0.599


tox21
Logistic regression
0.902
0.705



Random forest
0.999
0.736



XGBoost
0.891
0.753



IRV
0.811
0.767



MT-NN classification
0.854
0.768



Robust MT-NN
0.857
0.766



Graph convolution
0.903
0.814



DAG
0.871
0.733



Weave
0.844
0.797


toxcast
Logistic regression
0.724
0.577



XGBoost
0.738
0.621



IRV
0.662
0.643



MT-NN classification
0.830
0.684



Robust MT-NN
0.825
0.681



Graph convolution
0.849
0.726



Weave
0.796
0.725



Random splitting



Dataset
Model
Train score/ROC-AUC
Valid score/ROC-AUC




bace_c
Logistic regression
0.952
0.860



Random forest
1.000
0.882



IRV
0.876
0.871



NN classification
0.868
0.838



Robust NN
0.892
0.853



Graph convolution
0.849
0.793



DAG
0.873
0.810



Weave
0.828
0.847


bbbp
Logistic regression
0.978
0.905



Random forest
1.000
0.908



IRV
0.912
0.889



NN classification
0.857
0.822



Robust NN
0.886
0.857



Graph convolution
0.966
0.870



DAG
0.986
0.888



Weave
0.935
0.898


clintox
Logistic regression
0.968
0.734



Random forest
0.996
0.730



XGBoost
0.886
0.731



IRV
0.793
0.751



MT-NN classification
0.946
0.793



Robust MT-NN
0.958
0.818



Graph convolution
0.965
0.908



DAG
0.998
0.529



Weave
0.927
0.867


hiv
Logistic regression
0.855
0.816



Random forest
0.999
0.850



XGBoost
0.933
0.841



IRV
0.831
0.836



NN classification
0.699
0.695



Robust NN
0.726
0.726



Graph convolution
0.876
0.824



Weave
0.872
0.819


muv
Logistic regression
0.954
0.722



XGBoost
0.874
0.696



IRV
0.690
0.630



MT-NN classification
0.906
0.737



Robust MT-NN
0.940
0.732



Graph convolution
0.889
0.734



Weave
0.757
0.714


pcba
Logistic regression
0.808
0.775



MT-NN classification
0.811
0.787



Robust MT-NN
0.809
0.776



Graph convolution
0.888
0.850


sider
Logistic regression
0.931
0.639



Random forest
1.000
0.682



XGBoost
0.824
0.635



IRV
0.636
0.634



MT-NN classification
0.782
0.662



Robust MT-NN
0.807
0.661



Graph convolution
0.732
0.666



DAG
0.919
0.555



Weave
0.597
0.610


tox21
Logistic regression
0.900
0.735



Random forest
0.999
0.763



XGBoost
0.874
0.773



IRV
0.807
0.770



MT-NN classification
0.849
0.754



Robust MT-NN
0.854
0.755



Graph convolution
0.901
0.832



DAG
0.888
0.766



Weave
0.844
0.812


toxcast
Logistic regression
0.719
0.538



XGBoost
0.738
0.633



IRV
0.659
0.662



MT-NN classification
0.836
0.676



Robust MT-NN
0.828
0.680



Graph convolution
0.843
0.732



Weave
0.785
0.718



Scaffold splitting



Dataset
Model
Train score/ROC-AUC
Valid score/ROC-AUC




bace_c
Logistic regression
0.957
0.726



Random forest
0.999
0.728



IRV
0.899
0.700



NN classification
0.884
0.710



Robust NN
0.906
0.738



Graph convolution
0.921
0.665



DAG
0.839
0.591



Weave
0.736
0.593


bbbp
Logistic regression
0.980
0.957



Random forest
1.000
0.955



IRV
0.914
0.962



NN classification
0.884
0.955



Robust NN
0.905
0.959



Graph convolution
0.972
0.949



DAG
0.940
0.855



Weave
0.953
0.969


clintox
Logistic regression
0.962
0.687



Random forest
0.994
0.664



XGBoost
0.873
0.850



IRV
0.793
0.715



MT-NN classification
0.923
0.825



Robust MT-NN
0.949
0.821



Graph convolution
0.973
0.847



DAG
0.991
0.451



Weave
0.936
0.930


hiv
Logistic regression
0.858
0.793



Random forest
0.946
0.562



XGBoost
0.927
0.830



IRV
0.847
0.811



NN classification
0.719
0.718



Robust NN
0.740
0.730



Graph convolution
0.882
0.797



Weave
0.880
0.793


muv
Logistic regression
0.950
0.756



XGBoost
0.875
0.705



IRV
0.666
0.708



MT-NN classification
0.908
0.785



Robust MT-NN
0.934
0.792



Graph convolution
0.899
0.787



Weave
0.762
0.764


pcba
Logistic regression
0.810
0.748



MT-NN classification
0.823
0.773



Robust MT-NN
0.818
0.758



Graph convolution
0.894
0.826


sider
Logistic regression
0.926
0.594



Random forest
1.000
0.611



XGBoost
0.796
0.560



IRV
0.638
0.598



MT-NN classification
0.771
0.555



Robust MT-NN
0.795
0.567



Graph convolution
0.751
0.546



DAG
0.902
0.541



Weave
0.640
0.509


tox21
Logistic regression
0.901
0.676



Random forest
0.999
0.665



XGBoost
0.881
0.703



IRV
0.823
0.708



MT-NN classification
0.863
0.725



Robust MT-NN
0.861
0.724



Graph convolution
0.913
0.764



DAG
0.888
0.658



Weave
0.864
0.763


toxcast
Logistic regression
0.717
0.511



XGBoost
0.741
0.587



IRV
0.677
0.612



MT-NN classification
0.835
0.612



Robust MT-NN
0.832
0.609



Graph convolution
0.859
0.646



Weave
0.802
0.657




Regression




Dataset
Model
Splitting
Train score/R2
Valid score/R2




bace_r
Random forest
Random
0.958
0.680



NN regression
Random
0.895
0.732



Graphconv regression
Random
0.328
0.276



DAG regression
Random
0.370
0.271



Weave regression
Random
0.555
0.578



Random forest
Scaffold
0.956
0.203



NN regression
Scaffold
0.894
0.203



Graphconv regression
Scaffold
0.321
0.032



DAG regression
Scaffold
0.304
0.000



Weave regression
Scaffold
0.594
0.044


chembl
MT-NN regression
Index
0.828
0.565



Graphconv regression
Index
0.192
0.293



MT-NN regression
Random
0.829
0.562



Graphconv regression
Random
0.198
0.271



MT-NN regression
Scaffold
0.843
0.430



Graphconv regression
Scaffold
0.231
0.294


clearance
Random forest
Index
0.953
0.244



NN regression
Index
0.884
0.211



Graphconv regression
Index
0.696
0.230



Weave regression
Index
0.261
0.107



Random forest
Random
0.952
0.547



NN regression
Random
0.880
0.273



Graphconv regression
Random
0.685
0.302



Weave regression
Random
0.229
0.129



Random forest
Scaffold
0.952
0.266



NN regression
Scaffold
0.871
0.154



Graphconv regression
Scaffold
0.628
0.277



Weave regression
Scaffold
0.228
0.226


delaney
Random forest
Index
0.954
0.625



XGBoost
Index
0.898
0.664



NN regression
Index
0.869
0.585



Graphconv regression
Index
0.969
0.813



DAG regression
Index
0.976
0.850



Weave regression
Index
0.963
0.872



Random forest
Random
0.955
0.561



XGBoost
Random
0.927
0.727



NN regression
Random
0.875
0.495



Graphconv regression
Random
0.976
0.787



DAG regression
Random
0.968
0.899



Weave regression
Random
0.955
0.907



Random forest
Scaffold
0.953
0.281



XGBoost
Scaffold
0.890
0.316



NN regression
Scaffold
0.872
0.308



Graphconv regression
Scaffold
0.980
0.564



DAG regression
Scaffold
0.968
0.676



Weave regression
Scaffold
0.971
0.756


hopv
Random forest
Index
0.943
0.338



MT-NN regression
Index
0.725
0.293



Graphconv regression
Index
0.307
0.284



Weave regression
Index
0.046
0.026



Random forest
Random
0.943
0.513



MT-NN regression
Random
0.716
0.289



Graphconv regression
Random
0.329
0.239



Weave regression
Random
0.080
0.084



Random forest
Scaffold
0.946
0.470



MT-NN regression
Scaffold
0.719
0.429



Graphconv regression
Scaffold
0.286
0.155



Weave regression
Scaffold
0.097
0.082


kaggle
MT-NN regression
User-defined
0.748
0.452


lipo
Random forest
Index
0.960
0.485



NN regression
Index
0.829
0.508



Graphconv regression
Index
0.867
0.702



DAG regression
Index
0.957
0.483



Weave regression
Index
0.726
0.607



Random forest
Random
0.960
0.514



NN regression
Random
0.833
0.476



Graphconv regression
Random
0.867
0.631



DAG regression
Random
0.967
0.412



Weave regression
Random
0.747
0.598



Random forest
Scaffold
0.959
0.330



NN regression
Scaffold
0.830
0.308



Graphconv regression
Scaffold
0.875
0.608



DAG regression
Scaffold
0.937
0.368



Weave regression
Scaffold
0.761
0.575


nci
XGBoost
Index
0.441
0.066



MT-NN regression
Index
0.690
0.062



Graphconv regression
Index
0.123
0.053



XGBoost
Random
0.409
0.106



MT-NN regression
Random
0.698
0.117



Graphconv regression
Random
0.117
0.076



XGBoost
Scaffold
0.445
0.046



MT-NN regression
Scaffold
0.692
0.036



Graphconv regression
Scaffold
0.131
0.036


pdbbind(core)
Random forest
Random
0.921
0.382



NN regression
Random
0.764
0.591



Graphconv regression
Random
0.774
0.230



Random forest(grid)
Random
0.970
0.401



NN regression(grid)
Random
0.986
0.180


pdbbind(refined)
Random forest
Random
0.901
0.562



NN regression
Random
0.766
0.442



Graphconv regression
Random
0.694
0.508



Random forest(grid)
Random
0.963
0.530



NN regression(grid)
Random
0.982
0.484


pdbbind(full)
Random forest
Random
0.879
0.475



NN regression
Random
0.311
0.307



Graphconv regression
Random
0.183
0.186



Random forest(grid)
Random
0.966
0.524



NN regression(grid)
Random
0.961
0.492


ppb
Random forest
Index
0.951
0.235



NN regression
Index
0.902
0.333



Graphconv regression
Index
0.673
0.442



Weave regression
Index
0.418
0.301



Random forest
Random
0.950
0.220



NN regression
Random
0.903
0.244



Graphconv regression
Random
0.646
0.429



Weave regression
Random
0.408
0.284



Random forest
Scaffold
0.943
0.176



NN regression
Scaffold
0.902
0.144



Graphconv regression
Scaffold
0.695
0.391



Weave regression
Scaffold
0.401
0.373


qm7
Random forest
Index
0.942
0.029



NN regression
Index
0.782
0.038



Graphconv regression
Index
0.982
0.036



NN regression(CM)
Index
0.997
0.989



DTNN
Index
0.998
0.997



Random forest
Random
0.935
0.429



NN regression
Random
0.643
0.554



Graphconv regression
Random
0.892
0.740



NN regression(CM)
Random
0.997
0.997



DTNN
Random
0.998
0.995



Random forest
Stratified
0.934
0.430



NN regression
Stratified
0.630
0.563



Graphconv regression
Stratified
0.894
0.725



NN regression(CM)
Stratified
0.998
0.997



DTNN
Stratified
0.999
0.998


qm7b
MT-NN regression(CM)
Index
0.900
0.783



DTNN
Index
0.926
0.869



MT-NN regression(CM)
Random
0.891
0.849



DTNN
Random
0.925
0.902



MT-NN regression(CM)
Stratified
0.892
0.862



DTNN
Stratified
0.922
0.905


qm8
Random forest
Index
0.972
0.616



MT-NN regression
Index
0.939
0.604



Graphconv regression
Index
0.866
0.704



MT-NN regression(CM)
Index
0.770
0.625



DTNN
Index
0.856
0.696



Random forest
Random
0.971
0.706



MT-NN regression
Random
0.934
0.717



Graphconv regression
Random
0.848
0.780



MT-NN regression(CM)
Random
0.753
0.699



DTNN
Random
0.842
0.754



Random forest
Stratified
0.971
0.690



MT-NN regression
Stratified
0.934
0.712



Graphconv regression
Stratified
0.846
0.767



MT-NN regression(CM)
Stratified
0.761
0.696



DTNN
Stratified
0.846
0.745


qm9
MT-NN regression
Index
0.839
0.708



Graphconv regression
Index
0.754
0.768



MT-NN regression(CM)
Index
0.803
0.800



DTNN
Index
0.911
0.867



MT-NN regression
Random
0.849
0.753



Graphconv regression
Random
0.700
0.696



MT-NN regression(CM)
Random
0.822
0.823



DTNN
Random
0.913
0.867



MT-NN regression
Stratified
0.839
0.687



Graphconv regression
Stratified
0.724
0.696



MT-NN regression(CM)
Stratified
0.791
0.827



DTNN
Stratified
0.911
0.874


sampl
Random forest
Index
0.967
0.737



XGBoost
Index
0.884
0.784



NN regression
Index
0.923
0.758



Graphconv regression
Index
0.970
0.897



DAG regression
Index
0.970
0.871



Weave regression
Index
0.992
0.915



Random forest
Random
0.966
0.729



XGBoost
Random
0.906
0.745



NN regression
Random
0.931
0.689



Graphconv regression
Random
0.964
0.848



DAG regression
Random
0.973
0.861



Weave regression
Random
0.992
0.885



Random forest
Scaffold
0.967
0.465



XGBoost
Scaffold
0.918
0.439



NN regression
Scaffold
0.901
0.238



Graphconv regression
Scaffold
0.963
0.822



DAG regression
Scaffold
0.961
0.846



Weave regression
Scaffold
0.992
0.837




General features

Number of tasks and examples in the datasets



Dataset
N(tasks)
N(samples)




bace_c
1
1522


bbbp
1
2053


clintox
2
1491


hiv
1
41913


muv
17
93127


pcba
128
439863


sider
27
1427


tox21
12
8014


toxcast
617
8615


bace_r
1
1522


chembl(5thresh)
691
23871


clearance
1
837


delaney
1
1128


hopv
8
350


kaggle
15
173065


lipo
1
4200


nci
60
19127


pdbbind(core)
1
195


pdbbind(refined)
1
3706


pdbbind(full)
1
11908


ppb
1
1614


qm7
1
7165


qm7b
14
7211


qm8
16
21786


qm9
15
133885


sampl
1
643



Time needed for benchmark test(~20h in total)



Dataset
Model
Time(loading)/s
Time(running)/s




bace_c
Logistic regression
10
10



NN classification
10
10



Robust NN
10
10



Random forest
10
80



IRV
10
10



Graph convolution
15
70



Weave
15
120


bbbp
Logistic regression
20
10



NN classification
20
20



Robust NN
20
20



Random forest
20
120



IRV
20
10



Graph convolution
20
150



Weave
20
100


clintox
Logistic regression
15
10



XGBoost
15
33



MT-NN classification
15
20



Robust MT-NN
15
30



Random forest
15
200



IRV
15
10



Graph convolution
20
130



Weave
20
90


hiv
Logistic regression
180
40



XGBoost
180
1000



NN classification
180
350



Robust NN
180
450



Random forest
180
2800



IRV
180
200



Graph convolution
180
1300



Weave
180
2000


muv
Logistic regression
600
450



XGBoost
600
3500



MT-NN classification
600
400



Robust MT-NN
600
550



Graph convolution
800
1800



Weave
800
4400


pcba
Logistic regression
1800
10000



XGBoost
1800
470000



MT-NN classification
1800
9000



Robust MT-NN
1800
14000



Graph convolution
2200
14000


sider
Logistic regression
15
80



XGBoost
15
660



MT-NN classification
15
75



Robust MT-NN
15
150



Random forest
15
2200



IRV
15
150



Graph convolution
20
50



Weave
20
200


tox21
Logistic regression
30
60



XGBoost
30
1500



MT-NN classification
30
60



Robust MT-NN
30
90



Random forest
30
6000



IRV
30
650



Graph convolution
30
160



Weave
30
300


toxcast
Logistic regression
80
2600



XGBoost
80
30000



MT-NN classification
80
2300



Robust MT-NN
80
4000



Graph convolution
80
900



Weave
80
2000


bace_r
NN regression
10
30



Random forest
10
50



Graphconv regression
10
110



Weave regression
10
150


chembl
MT-NN regression
200
9000



Graphconv regression
250
1800


clearance
NN regression
10
20



Random forest
10
10



Graphconv regression
10
60



Weave regression
10
70


delaney
NN regression
10
40



XGBoost
10
50



Random forest
10
30



graphconv regression
10
40



Weave regression
10
40


hopv
MT-NN regression
10
20



Random forest
10
50



Graphconv regression
10
50



Weave regression
10
60


kaggle
MT-NN regression
2200
3200


lipo
NN regression
30
60



Random forest
30
60



Graphconv regression
30
240



Weave regression
30
280


nci
MT-NN regression
400
1200



XGBoost
400
28000



graphconv regression
400
2500


pdbbind(core)
NN regression
0(featurized)
30


pdbbind(refined)
NN regression
0(featurized)
40


pdbbind(full)
NN regression
0(featurized)
60


ppb
NN regression
20
30



Random forest
20
30



Graphconv regression
20
100



Weave regression
20
120


qm7
MT-NN regression
10
400



DTNN
10
600


qm7b
MT-NN regression
10
600



DTNN
10
600


qm8
MT-NN regression
60
1000



DTNN
10
2000


qm9
MT-NN regression
220
10000



DTNN
10
14000


sampl
NN regression
10
30



XGBoost
10
20



Random forest
10
20



graphconv regression
10
40



Weave regression
10
20



Gitter
Join us on gitter at https://gitter.im/deepchem/Lobby. Probably the easiest place to ask simple questions or float requests for new features.
DeepChem Publications

Computational Modeling of β-secretase 1 (BACE-1) Inhibitors using
Ligand Based
Approaches
Low Data Drug Discovery with One-Shot Learning
MoleculeNet: A Benchmark for Molecular Machine Learning
Atomic Convolutional Networks for Predicting Protein-Ligand Binding Affinity

About Us
DeepChem is possible due to notable contributions from many people including Peter Eastman, Evan Feinberg, Joe Gomes, Karl Leswing, Vijay Pande, Aneesh Pappu, Bharath Ramsundar and Michael Wu (alphabetical ordering).  DeepChem was originally created by Bharath Ramsundar with encouragement and guidance from Vijay Pande.
DeepChem started as a Pande group project at Stanford, and is now developed by many academic and industrial collaborators. DeepChem actively encourages new academic and industrial groups to contribute!
Corporate Supporters
DeepChem is supported by a number of corporate partners who use DeepChem to solve interesting problems.
Schrödinger


DeepChem has transformed how we think about building QSAR and QSPR models when very large data sets are available; and we are actively using DeepChem to investigate how to best combine the power of deep learning with next generation physics-based scoring methods.

DeepCrystal


DeepCrystal was an early adopter of DeepChem, which we now rely on to abstract away some of the hardest pieces of deep learning in drug discovery. By open sourcing these efficient implementations of chemically / biologically aware deep-learning systems, DeepChem puts the latest research into the hands of the scientists that need it, materially pushing forward the field of in-silico drug discovery in the process.

Version
1.2.0
",batch2,8:10:17,Done
300,17712484466/intellij-community,"IntelliJ IDEA Community Edition
Building and Running from the IDE
To develop IntelliJ IDEA, you can use either IntelliJ IDEA Community Edition or IntelliJ IDEA Ultimate not older than 15.0. To build and run the code:

Run getPlugins.sh / getPlugins.bat from the project root directory to check out additional modules.
If this git repository is not on 'master' branch you need to checkout the same branches/tags in android and android/tools-base git repositories.
Open the project.
If an error notification about a missing required plugin (e.g. Kotlin) is shown enable or install that plugin.
Configure a JSDK named ""IDEA jdk"" (case sensitive), pointing to an installation of JDK 1.6.
Unless you're running on a Mac with an Apple JDK, add <JDK_HOME>/lib/tools.jar to the set of ""IDEA jdk"" jars.
Configure a JSDK named ""1.8"", pointing to an installation of JDK 1.8.
Add <JDK_18_HOME>/lib/tools.jar to the set of ""1.8"" jars.
Use Build | Make Project to build the code.
To run the code, use the provided shared run configuration ""IDEA"".

You can find other useful information at http://www.jetbrains.org. Contribute section of that site describes how you can contribute to IntelliJ IDEA.
",batch1,16:57:59,Done
301,tokzk/emacs,,batch1,16:59:00,Done
302,RanvierMUD/tiny,"RanvierMUD Tiny is a barebones starter kit for people who ""know what they're doing"" and just want to have a telnet
connection, login, command parsing, and movement. There is almost nothing here which may be preferable for those that
know what they want and don't want to spend time tearing everything down before they can build it back up.
This setup includes account creation, multiplayer players per account, a very basic command parser that understands
room exits and exact command entry.  That is to say the player needs to type 'look' if they want to look, not 'l'.
This gives you freedom to do whatever kind of handling you want without any code to tear down.
There are exactly 2 commands included: look and quit and there is one starter area with 3 rooms to demonstrate that
movement actually works.
The basic command parser does not support skills or channels, if you're using this bundle you know what you're doing and
can decide how you want those to be parsed.  The input events for the login/character creation flow are nearly identical
to bundle-example-input-events but pared down to the bare minimum. There are no character classes. There is only one
default attribute: health.
Installation
git clone --recursive https://github.com/ranviermud/tiny
cd tiny
npm install
git submodule foreach npm install

",batch2,8:10:17,Done
303,sarahemm/llvm-eclair,,batch1,16:57:59,Done
304,minxiyang/PrepareforTrainingDataforTrack,,batch1,16:57:58,Done
305,ubolonton/emacs,,batch1,16:58:59,Done
306,sisirkoppaka/fluent,,batch1,16:57:58,Done
307,irfan-en/cloned-repo,"gRPC-Go



The Go implementation of gRPC: A high performance, open
source, general RPC framework that puts mobile and HTTP/2 first. For more
information see the gRPC Quick Start:
Go guide.
Installation
To install this package, you need to install Go and setup your Go workspace on
your computer. The simplest way to install the library is to run:
$ go get -u google.golang.org/grpc

With Go module support (Go 1.11+), simply import ""google.golang.org/grpc"" in
your source code and go [build|run|test] will automatically download the
necessary dependencies (Go modules
ref).
If you are trying to access grpc-go from within China, please see the
FAQ below.
Prerequisites
gRPC-Go requires Go 1.9 or later.
Documentation

See godoc for package and API
descriptions.
Documentation on specific topics can be found in the Documentation
directory.
Examples can be found in the examples directory.

Performance
Performance benchmark data for grpc-go and other languages is maintained in
this
dashboard.
Status
General Availability Google Cloud Platform Launch
Stages.
FAQ
I/O Timeout Errors
The golang.org domain may be blocked from some countries.  go get usually
produces an error like the following when this happens:
$ go get -u google.golang.org/grpc
package google.golang.org/grpc: unrecognized import path ""google.golang.org/grpc"" (https fetch: Get https://google.golang.org/grpc?go-get=1: dial tcp 216.239.37.1:443: i/o timeout)

To build Go code, there are several options:


Set up a VPN and access google.golang.org through that.


Without Go module support: git clone the repo manually:
git clone https://github.com/grpc/grpc-go.git $GOPATH/src/google.golang.org/grpc

You will need to do the same for all of grpc's dependencies in golang.org,
e.g. golang.org/x/net.


With Go module support: it is possible to use the replace feature of go mod to create aliases for golang.org packages.  In your project's directory:
go mod edit -replace=google.golang.org/grpc=github.com/grpc/grpc-go@latest
go mod tidy
go mod vendor
go build -mod=vendor

Again, this will need to be done for all transitive dependencies hosted on
golang.org as well.  Please refer to this
issue in the golang repo regarding
this concern.


Compiling error, undefined: grpc.SupportPackageIsVersion
Please update proto package, gRPC package and rebuild the proto files:

go get -u github.com/golang/protobuf/{proto,protoc-gen-go}
go get -u google.golang.org/grpc
protoc --go_out=plugins=grpc:. *.proto

How to turn on logging
The default logger is controlled by the environment variables. Turn everything
on by setting:
GRPC_GO_LOG_VERBOSITY_LEVEL=99 GRPC_GO_LOG_SEVERITY_LEVEL=info

The RPC failed with error ""code = Unavailable desc = transport is closing""
This error means the connection the RPC is using was closed, and there are many
possible reasons, including:

mis-configured transport credentials, connection failed on handshaking
bytes disrupted, possibly by a proxy in between
server shutdown

It can be tricky to debug this because the error happens on the client side but
the root cause of the connection being closed is on the server side. Turn on
logging on both client and server, and see if there are any transport
errors.
",batch2,8:09:16,Done
308,hhy37/homebrew-core,"Homebrew Core
Core formulae for the Homebrew package manager.
How do I install these formulae?
Just brew install <formula>. This is the default tap for Homebrew and is installed by default.
More Documentation, Troubleshooting, Contributing, Security, Community, Donations, License and Sponsors
See these sections in Homebrew/brew's README.
",batch1,16:59:00,Done
309,bcgov/dbcrss,"

dbcrss
DataBC Application Feeds Service
Visualizations
DataBC Web Services
https://uptime.apps.gov.bc.ca 
Purpose
Service Status Page
License
Copyright 2016 Province of British Columbia

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

",batch1,16:58:59,Done
310,dcgavril/Booked,,batch2,8:10:18,Done
311,zklhp/emacs-w64,,batch1,16:59:01,Done
312,tyler-dodge/emacs-mac,,batch1,16:58:59,Done
313,leanprover-community/mathlib,"Lean mathlib




Mathlib is a user maintained library for the Lean theorem prover.
It contains both programming infrastructure and mathematics,
as well as tactics that use the former and allow to develop the latter.
Installation
You can find detailed instructions to install Lean, mathlib, and supporting tools on our website.
Experimenting
Got everything installed? Why not start with the tutorial project?
For more pointers, see Learning Lean.
Documentation
Besides the installation guides above and Lean's general
documentation, the documentation
of mathlib consists of:

The mathlib docs: documentation generated
automatically from the source .lean files.
In addition to the pages generated for each file in the library, the docs also include pages on:

tactics,
commands,
hole commands, and
attributes.


A description of currently covered theories,
as well as an overview for mathematicians.
A couple of tutorial Lean files
Some extra Lean documentation not specific to mathlib (see ""Miscellaneous topics"")
Documentation for people who would like to contribute to mathlib

Much of the discussion surrounding mathlib occurs in a
Zulip chat room. Since this
chatroom is only visible to registered users, we provide an
openly accessible archive
of the public discussions. This is useful for quick reference; for a
better browsing interface, and to participate in the discussions, we strongly
suggest joining the chat. Questions from users at all levels of expertise are
welcomed.
Maintainers:

Jeremy Avigad (@avigad): analysis
Anne Baanen (@Vierkantor): algebra, number theory, tactics
Reid Barton (@rwbarton): category theory, topology
Mario Carneiro (@digama0): all
Bryan Gin-ge Chen (@bryangingechen): documentation, infrastructure
Johan Commelin (@jcommelin): algebra
Floris van Doorn (@fpvandoorn): all
Gabriel Ebner (@gebner): all
Sébastien Gouëzel (@sgouezel): topology, calculus
Markus Himmel (@TwoFX): category theory
Simon Hudon (@cipher1024): all
Chris Hughes (@ChrisHughes24): group theory, ring theory, field theory
Yury G. Kudryashov (@urkud): analysis, topology
Robert Y. Lewis (@robertylewis): all
Heather Macbeth (@hrmacbeth): geometry, analysis
Patrick Massot (@patrickmassot): documentation, topology
Bhavik Mehta (@b-mehta): category theory, combinatorics
Scott Morrison (@semorrison): category theory
Adam Topaz (@adamtopaz): algebra, category theory
Eric Wieser (@eric-wieser): algebra, infrastructure

",batch2,8:09:15,Done
314,didichuxing2/myMariaDB,,batch1,16:57:59,Done
315,quytelda/cemacs,,batch1,16:59:00,Done
316,veshboo/emacs,"nsxwidget - NS Cocoa backend for Emacs xwidgets
This repo supports Emacs feature xwidgets on native macOS X Cocoa.
Though original Emacs xwidgets builds and works on macOS but must
build and run with X window and GTK instead of macOS's own GUI
framework, resulting unaligned styles and UX with surrounding desktop
environment.
WARNING This software is EXPERIMENTAL and UNSTABLE, can causes
lost of data you are working on with this.
For example, while I develop, once watched an abrupt termination of
this program that is not resolved.
Screenshot
Reviewing pandoc generated html in emacs xwidget webkit for mac os x,

How to build
On quite recent macOS X system with Xcode and WebKit2

Git clone this repo and checkout master

git clone https://github.com/veshboo/emacs.git
git checkout master

Notable dependencies

brew install texinfo
brew install gnutls

Environment variable for newly installed texinfo (makeinfo)

export PATH=/usr/local/opt/texinfo/bin:$PATH
export LDFLAGS=-L/usr/local/opt/texinfo/lib

then build Emacs

./autogen.sh
./configure --prefix=$HOME/works/emacs-devel --with-xwidgets
make install
For general build information, read INSTALL.REPO.
How to use
Your Emacs app built is located under prefix/nextstep/Emacs.app
... you can run it from command line
cd $HOME/works/emacs-devel/emacs
./nextstep/Emacs.app/Contents/MacOS/Emacs
or by double-clicking Emacs app icon under prefix/nextstep folder in
Finder.
Brief xwidget webkit commands and key mappings


Commands


M-x xwidget-webkit-browse-url, enter a URL you want visit
including ""https://"", ""http://"", ""files:///"" part


C-u M-x xwidget-webkit-browse-url, ... does same but using
new session of webkit




Key mappings (apply when keyboard focus is not in HTML input text or textarea element, in general)


space, shift-space, up/down, left/right, delete: Scrolling


b, r, +/-: backward, reload, zoom in/out


C-x 2, C-x 3: Duplicate browsing same page in new horizontal or vertical split window (also using a new session)


C-g: Give up focus held in HTML input text or textarea element to Emacs


C-s, C-r: isearch integration


C-x r m, C-x r l: bookmark integration




Example customization using xwidget webkit

Use xwidget-webkit-browse-url as the browse-url

;; In ~/.emacs or ~/.emacs.d/init.el
(setq browse-url-browser-function 'xwidget-webkit-browse-url)
* Then, many packages supporting `browse-url` will work with xwidget webkit

* For example, try `C-c C-c p` if you are using `markdown-preview`.


search-web with xwidget webkit

(require 'search-web)
(global-set-key (kbd ""C-c w"") 'search-web)
(defun browse-url-default-browser (url &rest args)
  ""Override `browse-url-default-browser' to use `xwidget-webkit' URL ARGS.""
  (xwidget-webkit-browse-url url args))

Browse to a URL bookmark from *Bookmark List*

(defvar xwidget-webkit-bookmark-jump-new-session) ;; xwidget.el
(defvar xwidget-webkit-last-session-buffer) ;; xwidget.el
(add-hook 'pre-command-hook
          (lambda ()
            (if (eq this-command #'bookmark-bmenu-list)
                (if (not (eq major-mode 'xwidget-webkit-mode))
                    (setq xwidget-webkit-bookmark-jump-new-session t)
                  (setq xwidget-webkit-bookmark-jump-new-session nil)
                  (setq xwidget-webkit-last-session-buffer (current-buffer))))))
* `RET` on a URL bookmark will show the page in the window with
  current `*Bookmark List*`

* It will create a new `xwidget-webkit-mode` buffer if the
  previous buffer in the selected window is not a
  `xwidget-webkit-mode`.  Otherwise, it will browse in the
  previous `xwidget-webkit-mode` buffer.


Write elisp using lisp/xwidget.el to your task

",batch1,16:59:00,Done
317,JetBrains/intellij-community,"IntelliJ IDEA Community Edition 
These instructions will help you build IntelliJ IDEA Community Edition from source code, which is the basis for IntelliJ Platform development.
The following conventions will be used to refer to directories on your machine:

<USER_HOME> is your home directory.
<IDEA_HOME> is the root directory for the IntelliJ source code.

Getting IntelliJ IDEA Community Edition Source Code
IntelliJ IDEA Community Edition source code is available from github.com/JetBrains/intellij-community by either cloning or
downloading a zip file (based on a branch) into <IDEA_HOME>. The default is the master branch.
The master branch contains the source code which will be used to create the next major version of IntelliJ IDEA. The branch names
and build numbers for older releases of IntelliJ IDEA can be found on the page of
Build Number Ranges.
Speed Tip: If the complete repository history isn't needed then using a shallow clone (git clone --depth 1) will save significant time.
These Git operations can also be done through the IntelliJ IDEA user interface.
IntelliJ IDEA Community Edition requires additional Android modules from separate Git repositories. To clone these repositories,
run one of the getPlugins scripts located in the <IDEA_HOME> directory. These scripts clone their respective master branches.

getPlugins.sh for Linux or macOS.
getPlugins.bat for Windows.

Note: Always git checkout the intellij-community and android Git repositories to the same branches/tags.
Building IntelliJ Community Edition
Version 2020.1 or newer of IntelliJ IDEA Community Edition or IntelliJ IDEA Ultimate Edition is required to build and develop
for the IntelliJ Platform.
Opening the IntelliJ Source Code for Build
Using IntelliJ IDEA File | Open, select the <IDEA_HOME> directory.

If IntelliJ IDEA displays an error about a missing or out of date required plugin (e.g. Kotlin),
enable, upgrade, or install that plugin and restart IntelliJ IDEA.
If IntelliJ IDEA displays an error about a Gradle configuration not found,
refresh the Gradle projects.

IntelliJ Build Configuration


Configure a JDK named ""corretto-11"", pointing to installation of JDK 11. It's recommended to use Amazon Corretto JDK, but other
distributions based on OpenJDK should work as well. You may download it directly
from Project Structure dialog.


If the Maven Integration plugin is disabled, add the path variable
""MAVEN_REPOSITORY"" pointing to <USER_HOME>/.m2/repository directory.


Speed Tip: If you have enough RAM on your computer,
configure the compiler settings
to enable the ""Compile independent modules in parallel"" option. Also, increase build process heap size:

if you use IntelliJ IDEA 2020.3 or newer, set ""User-local build process heap size"" to 2048.
if you use IntelliJ IDEA 2020.2 or older, copy value from ""Shared build process VM options"" to ""User-local build process VM options"" and add -Xmx2G to it.

These changes will greatly reduce compilation time.


Building the IntelliJ Application Source Code
To build IntelliJ IDEA Community Edition from source, choose Build | Build Project from the main menu.
To build installation packages, run the ant command in <IDEA_HOME> directory. See the build.xml file for details.
Running IntelliJ IDEA
To run the IntelliJ IDEA built from source, choose Run | Run from the main menu. This will use the preconfigured run configuration ""IDEA"".
To run tests on the build, apply these setting to the Run | Edit Configurations... | Templates | JUnit configuration tab:

Working dir: <IDEA_HOME>/bin
VM options:

-ea
-Didea.config.path=../test-config
-Didea.system.path=../test-system



You can find other helpful information at https://www.jetbrains.com/opensource/idea.
The ""Contribute Code"" section of that site describes how you can contribute to IntelliJ IDEA.
",batch1,16:57:57,Done
318,jjzhang166/LCUI,"The LCUI Project









Description
中文版说明文档
LCUI is a freely available software library to create GUI application.
It is written in C, support the use XML and CSS describe the graphical
interface, you can use it to make some simple effects, like in this example:

Author is from China, but his English is not very good, so, some
files will appear chinese character, please understand. thanks.
Please read the file docs/CHANGES.md, it contains IMPORTANT INFORMATION.
Read the file INSTALL for installation instructions.
See the  file docs/LICENSE.TXT  for the available licenses.
Documentation
Tutorial: https://lcui.lc-soft.io/guide/
API reference documentation has not yet been prepared, you can refer to the
header files, source code, and sample programs.
Building
Bootstrap
To bootstrap the build you need to run ./configure (in the root of the
source tree).
In the simplest case you would run:
git clone https://github.com/lc-soft/LCUI.git
cd LCUI
./autogen.sh
./configure

Prerequisites
If you want to build full-featured LCUI, we suggest you install the following
dependent libraries:

libpng — PNG image compression library
libjpeg — JPEG image compression library
libxml2 — The XML C parser and toolkit
libx11 — X11 client-side library
freetype — Font engine

If you system is Ubuntu, you can run following command to install dependencies:
apt-get install libpng-dev libjpeg-dev libxml2-dev libfreetype6-dev libx11-dev

Building On Windows
LCUI is mainly develop in the Windows environment, you can use VisualStudio
to open file build/windows/LCUI.sln and compile LCUI.
Bugs
Please report bugs  by e-mail to lc-soft@live.cn. Don't forget to send a
detailed explanation of  the problem --  there is nothing worse than receiving
a terse message that only says it doesn't work.
Contributing
You can send pull requests via GitHub.
Patches should:

Follow the style of the existing code.
One commit should do exactly one thing.
Commit messages should start with a summary line below 80 characters
followed by a blank line, and then the reasoning/analysis for why the
change was made (if appropriate).
Commits that fix a bug in a previous commit (which has already been
merged) should start with fixup! and then the summary line of the
commit it fixes. If you are writing your commit message in LCUI
then type fix⇥ to get the prefix and a menu allowing you to pick
the summary line from one of the last 15 commits.
Rebase your branch against the upstream’s master. We don’t want to
pull redundant merge commits.
Be clear about what license applies to your patch: The files with
in this repository are under the [GPL 2][] (or later) but (as the
original creator) we are still allowed to create non-free derivatives.
However, if patches are given to us under GPL then those cannot make
it into any non-free derivatives we may later wish to create. So to
make it easier for us (and avoid any legal issues) we prefer if
patches are released as public domain.

You can donate support the development of LCUI.
I'm currently an independent developer and your contributions are useful.
I have setup an LCUI Patreon page if you
want to donate and enable me to spend more time improving the library.
One-off donations are also greatly appreciated. Thanks!
GitHub Workflow
Developing patches should follow this workflow:
Initial Setup

Fork on GitHub (click Fork button)
Clone to computer: git clone git@github.com:«github account»/LCUI.git
cd into your repo: cd LCUI
Set up remote upstream: git remote add -f upstream git://github.com/lc-soft/LCUI.git

Adding a Feature

Create a branch for the new feature: git checkout -b my_new_feature
Work on your feature, add and commit as usual

Creating a branch is not strictly necessary, but it makes it easy to
delete your branch when the feature has been merged into upstream, diff
your branch with the version that actually ended in upstream, and to
submit pull requests for multiple features (branches).
Pushing to GitHub

Push branch to GitHub: git push origin my_new_feature
Issue pull request: Click Pull Request button on GitHub

Useful Commands
If a lot of changes has happened upstream you can replay your local changes
on top of these, this is done with rebase, e.g.:
git fetch upstream
git rebase upstream/master

This will fetch changes and re-apply your commits on top of these.
This is generally better than merge, as it will give a clear picture of which
commits are local to your branch. It will also “prune” any of your local
commits if the same changes have been applied upstream.
You can use -i with rebase for an “interactive” rebase. This allows
you to drop, re-arrange, merge, and reword commits, e.g.:
git rebase -i upstream/master

Legal
The LCUI Project is released under the GNU General Public License as published
by the Free Software Foundation, either version 2 of the License, or (at your
option) any later version.
Thanks to
Liu Chao lc-soft@live.cn
Special thanks to Liu Chao for his great work on the LCUI's development.
Backers
Support us with a monthly donation and help us continue our activities. [Become a backer]






























Sponsors
Become a sponsor and get your logo on our README on Github with a link to your site. [Become a sponsor]






























--- end of README.md ---
",batch2,8:10:17,Done
319,jeroen/autobrew-core,"Homebrew Core
Core formulae for the Homebrew package manager.
How do I install these formulae?
Just brew install <formula>. This is the default tap for Homebrew and is installed by default.
More Documentation, Troubleshooting, Contributing, Security, Community, Donations, License and Sponsors
See these sections in Homebrew/brew's README.
",batch1,16:59:00,Done
320,chaso137/wot-xvm,,batch2,8:10:18,Done
321,badcons2002/mycroft,"   
 


Mycroft
Mycroft is a hackable open source voice assistant.
Table of Contents

Table of Contents
Getting Started
Running Mycroft
Using Mycroft

Home Device and Account Manager
Skills


Behind the scenes

Pairing Information
Configuration
Using Mycroft Without Home
API Key Services
Using Mycroft behind a proxy

Using Mycroft behind a proxy without authentication
Using Mycroft behind an authenticated proxy




Getting Involved
Links

Getting Started
First, get the code on your system!  The simplest method is via git (git installation instructions):

cd ~/
git clone https://github.com/MycroftAI/mycroft-core.git
cd mycroft-core
bash dev_setup.sh

This script sets up dependencies and a virtualenv.  If running in an environment besides Ubuntu/Debian, Arch or Fedora you may need to manually install packages as instructed by dev_setup.sh.
NOTE: The default branch for this repository is 'dev', which should be considered a work-in-progress. If you want to clone a more stable version, switch over to the 'master' branch.
Running Mycroft
Mycroft provides start-mycroft.sh to perform common tasks. This script uses a virtualenv created by dev_setup.sh.  Assuming you installed mycroft-core in your home directory run:

cd ~/mycroft-core
./start-mycroft.sh debug

The ""debug"" command will start the background services (microphone listener, skill, messagebus, and audio subsystems) as well as bringing up a text-based Command Line Interface (CLI) you can use to interact with Mycroft and see the contents of the various logs. Alternatively you can run ./start-mycroft.sh all to begin the services without the command line interface.  Later you can bring up the CLI using ./start-mycroft.sh cli.
The background services can be stopped as a group with:

./stop-mycroft.sh

Using Mycroft
Home Device and Account Manager
Mycroft AI, Inc. maintains a device and account management system known as Mycroft Home. Developers may sign up at: https://home.mycroft.ai
By default, mycroft-core  is configured to use Home. By saying ""Hey Mycroft, pair my device"" (or any other request verbal request) you will be informed that your device needs to be paired. Mycroft will speak a 6-digit code which you can enter into the pairing page within the Mycroft Home site.
Once paired, your unit will use Mycroft API keys for services such as Speech-to-Text (STT), weather and various other skills.
Skills
Mycroft is nothing without skills.  There are a handful of default skills that are downloaded automatically to your /opt/mycroft/skills directory, but most need to be installed explicitly.  See the Skill Repo to discover skills made by others.  And please share your own interesting work!
Behind the scenes
Pairing Information
Pairing information generated by registering with Home is stored in:
~/.mycroft/identity/identity2.json <-- DO NOT SHARE THIS WITH OTHERS!
Configuration
Mycroft configuration consists of 4 possible locations:

mycroft-core/mycroft/configuration/mycroft.conf(Defaults)
Mycroft Home (Remote)
/etc/mycroft/mycroft.conf(Machine)
$HOME/.mycroft/mycroft.conf(User)

When the configuration loader starts, it looks in these locations in this order, and loads ALL configurations. Keys that exist in multiple configuration files will be overridden by the last file to contain the value. This process results in a minimal amount being written for a specific device and user, without modifying default distribution files.
Using Mycroft Without Home
If you do not wish to use the Mycroft Home service, before starting Mycroft for the first time, create $HOME/.mycroft/mycroft.conf with the following contents:
{
  ""skills"": {
    ""blacklisted_skills"": [
      ""mycroft-configuration.mycroftai"",
      ""mycroft-pairing.mycroftai""
    ]
  }
}

Mycroft will then be unable to perform speech-to-text conversion, so you'll need to set that up as well, using one of the STT engines Mycroft supports.
You may insert your own API keys into the configuration files listed above in Configuration.  For example, to insert the API key for the Weather skill, create a new JSON key in the configuration file like so:
{
  // other configuration settings...
  //
  ""WeatherSkill"": {
    ""api_key"": ""<insert your API key here>""
  }
}

API Key Services
These are the keys currently used in Mycroft Core:

STT API, Google STT, Google Cloud Speech
Weather Skill API, OpenWeatherMap
Wolfram-Alpha Skill

Using Mycroft behind a proxy
Many schools, universities and workplaces run a proxy on their network. If you need to type in a username and password to access the external internet, then you are likely behind a proxy.
If you plan to use Mycroft behind a proxy, then you will need to do an additional configuration step.
NOTE: In order to complete this step, you will need to know the hostname and port for the proxy server. Your network administrator will be able to provide these details. Your network administrator may want information on what type of traffic Mycroft will be using. We use https traffic on port 443, primarily for accessing ReST-based APIs.
Using Mycroft behind a proxy without authentication
If you are using Mycroft behind a proxy without authentication, add the following environment variables, changing the proxy_hostname.com and proxy_port for the values for your network. These commands are executed from the Linux command line interface (CLI).
$ export http_proxy=http://proxy_hostname.com:proxy_port
$ export https_port=http://proxy_hostname.com:proxy_port
$ export no_proxy=""localhost,127.0.0.1,localaddress,.localdomain.com,0.0.0.0,::1""
Using Mycroft behind an authenticated proxy
If  you are behind a proxy which requires authentication, add the following environment variables, changing the proxy_hostname.com and proxy_port for the values for your network. These commands are executed from the Linux command line interface (CLI).
$ export http_proxy=http://user:password@proxy_hostname.com:proxy_port
$ export https_port=http://user:password@proxy_hostname.com:proxy_port
$ export no_proxy=""localhost,127.0.0.1,localaddress,.localdomain.com,0.0.0.0,::1""
Getting Involved
This is an open source project and we would love your help. We have prepared a contributing guide to help you get started.
If this is your first PR or you're not sure where to get started,
say hi in Mycroft Chat and a team member would be happy to mentor you.
Join the Mycroft Forum for questions and answers.
Links

Creating a Skill
Documentation
Skill Writer API Docs
Release Notes
Mycroft Chat
Mycroft Forum
Mycroft Blog

",batch1,16:57:57,Done
322,aauer1/tinyusb,"TinyUSB

  
TinyUSB is an open-source cross-platform USB Host/Device stack for embedded system, designed to be memory-safe with no dynamic allocation and thread-safe with all interrupt events are deferred then handled in the non-ISR task function.

.
├── docs            # Documentation
├── examples        # Sample with Makefile and Segger Embedded build support
├── hw
│   ├── bsp         # Supported boards source files
│   └── mcu         # Low level mcu core & peripheral drivers
├── lib             # Sources from 3rd party such as freeRTOS, fatfs ...
├── src             # All sources files for TinyUSB stack itself.
├── test            # Unit tests for the stack
└── tools           # Files used internally

Contributors
Special thanks to all the people who spent their precious time and effort to help this project so far. Check out the
CONTRIBUTORS.md file for the list of all contributors and their awesome work for the stack.
Supported MCUs
The stack supports the following MCUs:

Espressif: ESP32-S2
MicroChip: SAMD21, SAMD51 (device only)
NordicSemi: nRF52833, nRF52840
Nuvoton: NUC120, NUC121/NUC125, NUC126, NUC505
NXP:

LPC Series: 11Uxx, 13xx, 175x_6x, 177x_8x, 18xx, 40xx, 43xx, 51Uxx, 54xxx, 55xx
iMX RT Series: RT1011, RT1015, RT1021, RT1052, RT1062, RT1064


Sony: CXD56
ST: STM32 series: L0, F0, F1, F2, F3, F4, F7, H7 (device only)
TI: MSP430
ValentyUSB eptri

Here is the list of supported Boards that can be used with provided examples.
Device Stack
Supports multiple device configurations by dynamically changing usb descriptors. Low power functions such like suspend, resume, and remote wakeup. Following device classes are supported:

Communication Class (CDC)
Human Interface Device (HID): Generic (In & Out), Keyboard, Mouse, Gamepad etc ...
Mass Storage Class (MSC): with multiple LUNs
Musical Instrument Digital Interface (MIDI)
Network with RNDIS, CDC-ECM (work in progress)
USB Test and Measurement Class (USBTMC)
Vendor-specific class support with generic In & Out endpoints. Can be used with MS OS 2.0 compatible descriptor to load winUSB driver without INF file.
WebUSB with vendor-specific class

Host Stack
Most active development is on the Device stack. The Host stack is under rework and largely untested.

Human Interface Device (HID): Keyboard, Mouse, Generic
Mass Storage Class (MSC)
Hub currently only supports 1 level of hub (due to my laziness)

OS Abstraction layer
TinyUSB is completely thread-safe by pushing all ISR events into a central queue, then process it later in the non-ISR context task function. It also uses semaphore/mutex to access shared resources such as CDC FIFO. Therefore the stack needs to use some of OS's basic APIs. Following OSes are already supported out of the box.

No OS : Disabling USB IRQ is used as way to provide mutex
FreeRTOS
Mynewt Due to the newt package build system, Mynewt examples are better to be on its own repo

Getting Started
Here are the details for getting started with the stack.
Porting
Want to help add TinyUSB support for a new MCU? Read here for an explanation on the low-level API needed by TinyUSB.
License
MIT license for all TinyUSB sources src folder, Full license is here. However, each file is individually licensed especially those in lib and hw/mcu folder. Please make sure you understand all the license term for files you use in your project.
Uses
TinyUSB is currently used by these other projects:

Adafruit nRF52 Arduino
Adafruit nRF52 Bootloader
Adafruit SAMD Arduino
CircuitPython
MicroPython
TinyUSB Arduino Library

Let me know if your project also uses TinyUSB and want to share.
",batch2,8:10:17,Done
323,thornjad/emacs,"Emacs for Aero
This project is a fork of GNU Emacs, used to build Aero Emacs. This repository contains changes to core functionality which are neither approved nor peer reviewed by GNU, and as such is NOT considered stable.
Major differences with GNU Emacs

Native compilation is enabled by default, providing significantly better performance over stock GNU Emacs
Pure GTK implementation is available but disabled by default, eliminating reliance on X and supporting Wayland out of the box. Enable with --with-pgtk, must have GTK3+ available.
Xwidgets support is enabled by default
Included Dockerfile for containerized builds

Installation
Manual install
The INSTALL file contains full installation information, but here’s the synopsis:
./autoconf.sh
./configure
make
make install
Before running make, it may be useful to check the output of configure to ensure the compiler will use all the features you want. You may be missing some optional dependencies.
Also note that make accepts a jobs argument, -j, which sets the number of threads the compiler can use. In theory, omitting this argument should give you a balance between fast-ish and not locking up the rest of your system. In practice, if you can give it the same number of threads as logical cores available, the process will go much quicker. On my 12-core system, make on its own takes around 2 minutes while make -j16 runs in around 15 seconds.
Docker
This is not considered stable, and not all features are currently working.
make build-docker
docker run -ti --name emacs --net=host\
 -v /tmp/.X11-unix:/tmp/.X11-unix:ro -e DISPLAY=""unix$DISPLAY""\
 -v <path_to_your_.emacs.d>:/home/aero/.config/emacs\
 -v <path_to_your_workspace>:/mnt/workspace\
 aero-emacs emacs
License
Copyright (C) 2018-2021 Jade Michael Thornton
  Copyright (C) 1985-2021 Free Software Foundation, Inc.
Aero Emacs is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation version 3.
NOTICE: While GNU Emacs may be licensed under either version 3 or any later version of the GPL, Aero Emacs is licensed under the terms of version 3 only. Version 3 is bad enough, I’m not letting it slip into some unknown future version automatically.
See the file COPYING for full license terms.
",batch1,16:58:59,Done
324,benhager/benhager.github.io,"Neo Cactus for Jekyll
Demo: https://mmarfil.com/
Screenshot

This Jekyll theme started as a port of Cactus to my own needs, but I ended up performing a lot more modifications than expected. Some people reached me out and asked if I could share it, so here we are.
Disclaimer: I'm only a designer, so please don't expect the code to be pretty.
Usage
To start your project, fork this respository, put in your content, and go!
",batch1,16:57:57,Done
325,rdeusser/emacs,"Emacs
Modern Emacs
",batch1,16:59:00,Done
326,arunsarat/odl-netvirt,,batch2,8:10:17,Done
327,joansmith/cucumber-jvm,"

Cucumber-JVM is a pure Java implementation of Cucumber that supports the most popular programming languages for the JVM.
You can run it with the tool of your choice.
Cucumber-JVM also integrates with all the popular Dependency Injection containers.
Documentation
Start Here.
Hello World
Check out the simple cucumber-java-skeleton starter project.
Downloading / Installation
Install
Bugs and Feature requests
You can register bugs and feature requests in the Github Issue Tracker.
You're most likely going to paste code and output, so familiarise yourself with
Github Flavored Markdown to make sure it remains readable.
At the very least - use triple backticks:
```java
// Why doesn't this work?
@Given(""I have (\\d+) cukes in my (.*)"")
public void some_cukes(int howMany, String what) {
    // HALP!
}
```

Please consider including the following information if you register a ticket:

What cucumber-jvm version you're using
What modules you're using (cucumber-java, cucumber-spring, cucumber-groovy etc)
What command you ran
What output you saw
How it can be reproduced

How soon will my ticket be fixed?
The best way to have a bug fixed or feature request implemented is to
fork the cucumber-jvm repo and send a
pull request.
If the pull request has good tests and follows the coding conventions (see below) it has a good chance of
making it into the next release.
If you don't fix the bug yourself (or pay someone to do it for you), the bug might never get fixed. If it is a serious
bug, other people than you might care enough to provide a fix.
In other words, there is no guarantee that a bug or feature request gets fixed. Tickets that are more than 6 months old
are likely to be closed to keep the backlog manageable.
Contributing fixes
See CONTRIBUTING.md
",batch2,8:09:16,Done
328,luisgerhorst/emacs,,batch1,16:59:00,Done
329,cristal-ise/cristal-ise,"CRISTAL-iSE 

Description-Driven Framework for No-Code/Low-Code Application Development
Main repository of the CRISTAL-iSE Description-Driven Framework.
CRISTAL-iSE is a description-driven software platform originally developed to track the construction of the CMS ECAL detector of the LHC at CERN. It consists of a core library, known as the kernel, which manages business objects called Items. Items are entirely configured from data, called descriptions, held in other Items. Every change of a state in an Item is a consequence of an execution of an activity in that Item's lifecycle, meaning that CRISTAL-iSE applications are completely traceable, even in their design. It also supports extensive versioning of Item description data, giving the system a high level of flexibility.
",batch2,8:09:16,Done
330,dilucious/gitfiti,"
gitfiti noun : Carefully crafted graffiti in a github commit history calendar.
An example of gitfiti in the wild:

gitfiti.py is a tool I wrote to decorate your github account's commit history calendar by (blatantly) abusing git's ability to accept commits in the past.
How?  gitfiti.py generates a bash script: gitfiti.sh that makes commits with the GIT_AUTHOR_DATE and GIT_COMMITTER_DATE environment variables set for each targeted pixel.
Since this is likely to clobber repo's history, I highly recommend that you create a new github repo when using gitfiti. Also, the generated bash script assumes you are using public-key authentication with git.
Pixel Art:

Included ""art"" from left to right: kitty, oneup, oneup2, hackerschool, octocat, octocat2
Usage:

Create a new github repo to store your handiwork.
Run gitfiti.py and follow the prompts for username, art selection, offset, and repo name.
Run the generated gitfiti.sh from your home directory (or any non-git tracked dir) and watch it go to work.
Wait... Seriously, you'll probably need to wait a day or two for the gitfiti to show in your commit graph.

User Templates
The file format for personal templates is the following:

Each template starts off with a "":"" and then a name (eg. "":foo"")
Each line after that is part of a json-recognizable array.
The array contain values 0-4, 0 being blank and 4 being dark green.
To add multiple templates, just add another name tag as described in 1.

For example:
:center-blank
[[1,1,1,1,1,1,1],
[1,1,1,1,1,1,1],
[1,1,1,1,1,1,1],
[1,1,1,0,1,1,1],
[1,1,1,1,1,1,1],
[1,1,1,1,1,1,1],
[1,1,1,1,1,1,1]]

This would output a 7 x 7 light green square with a single blank center square.
Once you have a file with templates, enter its name when prompted and the templates will be added to the list of options.
Removal:
Fortunately if you regret your gitfiti in the morning, removing it is fairly easy: delete the repo you created for your gitfiti (and wait).
License:
gitfiti is released under The MIT license (MIT)

Todo:

Remove 'requests' dependency thanks empathetic-alligator
Web interface See several web-based things below
Load ""art"" from a file thanks empathetic-alligator
Load commit content from a file
Text/alphabet option
...
Profit?

Notable derivatives or mentions:

Pikesley's Pokrovsky, which offers Github History Vandalism as a Service!
github-board commits gitfiti from easy templates
ghdecoy fills the contribution graph with random data (sneaky!)
Gitfiti Painter visual drawing tool for artists to easily create templates
git-draw a Chrome extension which will allow you to freely draw on your commit map(!)
github-jack a pure bash version with space invaders and shining creepypasta
Seen something else? Submit a pull request or open an issue!


",batch1,16:57:57,Done
331,robgolebiowski/laurynas-percona-server,,batch1,16:59:01,Done
332,SchollCenter/SchollCenter.github.io,"Grav Standard Administration Panel Plugin
This admin plugin for Grav is an HTML user interface that provides a convenient way to configure Grav and easily create and modify pages.  This will remain a totally optional plugin, and is not in any way required or needed to use Grav effectively.  In fact, the admin provides an intentionally limited view to ensure it remains easy to use and not overwhelming.  I'm sure power users will still prefer to work with the configuration files directly.

Features

User login with automatic password encryption
Forgot password functionality
Logged-in-user management
One click Grav core updates
Dashboard with maintenance status, site activity and latest page updates
Notifications system for latest news, blogs, and announcements
Ajax-powered backup capability
Ajax-powered clear-cache capability
System configuration management
Site configuration management
Normal and Expert modes which allow editing via forms or YAML
Page listing with filtering and search
Page creation, editing, moving, copying, and deleting
Powerful syntax highlighting code editor with instant Grav-powered preview
Editor features, hot keys, toolbar, and distraction-free fullscreen mode
Drag-n-drop upload of page media files including drag-n-drop placement in the editor
One click theme and plugin updates
Plugin manager that allows listing and configuration of installed plugins
Theme manager that allows listing and configuration of installed themes
GPM-powered installation of new plugins and themes

Support
Support
We have tested internally, but we hope to use this public beta phase to identify, isolate, and fix issues related to the plugin to ensure it is as solid and reliable as possible.
For live chatting, please use the dedicated Slack Chat Room for discussions directly related to Grav.
For bugs, features, improvements, please ensure you create issues in the admin plugin GitHub repository.
Installation
First ensure you are running the latest Grav 0.9.34 or later.  This is required for the admin plugin to run properly (-f forces a refresh of the GPM index).
$ bin/gpm selfupgrade -f

The admin plugin actually requires the help of 3 other plugins, so to get the admin plugin to work you first need to install admin, login, forms, and email plugins.  These are available via GPM, and because the plugin has dependencies you just need to proceed and install the admin plugin, and agree when prompted to install the others:
$ bin/gpm install admin

Manual Installation
Manual installation is not the recommended method of installation, however, it is still possible to install the admin plugin manually. Basically, you need to download each of the following plugins individually:

admin
login
form
email

Extract each archive file into your user/plugins folder, then ensure the folders are renamed to just admin/, login/, form/, and email/.  Then proceed with the Usage instructions below.
Usage
Create User with CLI
After this you need to create a user account with admin privileges:
$ bin/plugin login new-user

Create User Manually
Alternatively, you can create a user account manually, in a file called user/accounts/admin.yaml. This filename is actually the username that you will use to login. The contents will contain the other information for the user.
password: 'password'
email: 'youremail@mail.com'
fullname: 'Johnny Appleseed'
title: 'Site Administrator'
access:
  admin:
    login: true
    super: true

Of course you should edit your email, password, fullname, and title to suit your needs.

You can use any password when you manually put it in this .yaml file.  However, when you change your password in the admin, it must contain at least one number and one uppercase and lowercase letter, and at least 8 or more characters.

Accessing the Admin
By default, you can access the admin by pointing your browser to http://yoursite.com/admin. You can simply log in with the username and password set in the YAML file you configured earlier.

After logging in, your plaintext password will be removed and replaced by an encrypted one.

Standard Free & Paid Pro Versions
If you have been following the blog, Twitter, Slack chat, etc., you probably already know now that our intention is to provide two versions of this plugin.
The standard free version, is very powerful, and has more functionality than most commercial flat-file CMS systems.
We also intend to release in the near future a more feature-rich pro version that will include enhanced functionality, as well as some additional nice-to-have capabilities. This pro version will be a paid plugin the price of which is not yet 100% finalized.
Running Tests
First install the dev dependencies by running composer update from the Grav root.
Then composer test will run the Unit Tests, which should be always executed successfully on any site.
",batch2,8:10:17,Done
333,NixOS/nixpkgs,"






Nixpkgs is a collection of over
60,000 software packages that can be installed with the
Nix package manager. It also implements
NixOS, a purely-functional Linux distribution.
Manuals

NixOS Manual - how to install, configure, and maintain a purely-functional Linux distribution
Nixpkgs Manual - contributing to Nixpkgs and using programming-language-specific Nix expressions
Nix Package Manager Manual - how to write Nix expressions (programs), and how to use Nix command line tools

Community

Discourse Forum
Matrix Chat
NixOS Weekly
Community-maintained wiki
Community-maintained list of ways to get in touch (Discord, Telegram, IRC, etc.)

Other Project Repositories
The sources of all official Nix-related projects are in the NixOS
organization on GitHub. Here are some of
the main ones:

Nix - the purely functional package manager
NixOps - the tool to remotely deploy NixOS machines
nixos-hardware - NixOS profiles to optimize settings for different hardware
Nix RFCs - the formal process for making substantial changes to the community
NixOS homepage - the NixOS.org website
hydra - our continuous integration system
NixOS Artwork - NixOS artwork

Continuous Integration and Distribution
Nixpkgs and NixOS are built and tested by our continuous integration
system, Hydra.

Continuous package builds for unstable/master
Continuous package builds for the NixOS 20.09 release
Tests for unstable/master
Tests for the NixOS 20.09 release

Artifacts successfully built with Hydra are published to cache at
https://cache.nixos.org/. When successful build and test criteria are
met, the Nixpkgs expressions are distributed via Nix
channels.
Contributing
Nixpkgs is among the most active projects on GitHub. While thousands
of open issues and pull requests might seem a lot at first, it helps
consider it in the context of the scope of the project. Nixpkgs
describes how to build tens of thousands of pieces of software and implements a
Linux distribution. The GitHub Insights
page gives a sense of the project activity.
Community contributions are always welcome through GitHub Issues and
Pull Requests. When pull requests are made, our tooling automation bot,
OfBorg will perform various checks
to help ensure expression quality.
The Nixpkgs maintainers are people who have assigned themselves to
maintain specific individual packages. We encourage people who care
about a package to assign themselves as a maintainer. When a pull
request is made against a package, OfBorg will notify the appropriate
maintainer(s). The Nixpkgs committers are people who have been given
permission to merge.
Most contributions are based on and merged into these branches:

master is the main branch where all small contributions go
staging is branched from master, changes that have a big impact on
Hydra builds go to this branch
staging-next is branched from staging and only fixes to stabilize
and security fixes with a big impact on Hydra builds should be
contributed to this branch. This branch is merged into master when
deemed of sufficiently high quality

For more information about contributing to the project, please visit
the contributing page.
Donations
The infrastructure for NixOS and related projects is maintained by a
nonprofit organization, the NixOS
Foundation. To ensure the
continuity and expansion of the NixOS infrastructure, we are looking
for donations to our organization.
You can donate to the NixOS foundation by using Open Collective:

License
Nixpkgs is licensed under the MIT License.
Note: MIT license does not apply to the packages built by Nixpkgs,
merely to the files in this repository (the Nix expressions, build
scripts, NixOS modules, etc.). It also might not apply to patches
included in Nixpkgs, which may be derivative works of the packages to
which they apply. The aforementioned artifacts are all covered by the
licenses of the respective packages.
",batch1,16:57:57,Done
334,jared2501/test,"IntelliJ IDEA Community Edition
Building and Running from the IDE
To develop IntelliJ IDEA, you can use either IntelliJ IDEA Community Edition or IntelliJ IDEA Ultimate. To build and run the code:

Run getPlugins.sh / getPlugins.bat from the project root directory to check out additional modules.
Install the Kotlin plugin from the plugin manager (version 0.12.613). Parts of IntelliJ IDEA are written in Kotlin, and you need the plugin to compile the code.
Make sure you have the Groovy plugin enabled. Parts of IntelliJ IDEA are written in Groovy, and you will get compilation errors if you don't have the plugin enabled.
Make sure you have the UI Designer plugin enabled. Most of IntelliJ IDEA's UI is built using the UI Designer, and the version you build will not run correctly if you don't have the plugin enabled.
Open the project.
Configure a JSDK named ""IDEA jdk"" (case sensitive), pointing to an installation of JDK 1.6.
Unless you're running on a Mac with an Apple JDK, add <JDK_HOME>/lib/tools.jar to the set of ""IDEA jdk"" jars.
Configure a JSDK named ""1.8"", pointing to an installation of JDK 1.8.
Add <JDK_18_HOME>/lib/tools.jar to the set of ""1.8"" jars.
Use Build | Make Project to build the code.
To run the code, use the provided shared run configuration ""IDEA"".

You can find other useful information at http://www.jetbrains.org. Contribute section of that site describes how you can contribute to IntelliJ IDEA.
",batch1,16:58:59,Done
335,Rednickle/Redbrew,"Linuxbrew Core
Core formulae for the Homebrew package manager.

How do I install these formulae?
Just brew install <formula>. This is the default tap for Homebrew and is installed by default.
More Documentation, Troubleshooting, Contributing, Security, Community, Donations, License and Sponsors
See these sections in Homebrew/brew's README.
",batch1,16:57:58,Done
336,autobrew/homebrew-core,"Autobrew Core
System libraries for building R packages.
What is this
CRAN currently targets MacOS 10.11 (El-Capitain), however this version of MacOS is no longer supported by Apple, and the latest Homebrew no longer works there. Autobrew is a fork from upstream homebrew-core from the last day of MacOS 10.11 support. We selectively backport and adapt system libraries needed for building R packages.
This is not an officially supported project. All of this is a bit hacky and may not work for all libraries.
Contributing
If you send a pull request, the formula that has been changed will automatically be built on Travis CI. In addition some tests and reverse dependencies are checked.
At the end of the CI run the new binary bottle is uploaded to file.io and you see a download link in the Travis log.
",batch1,16:59:01,Done
337,rr-/dotfiles,"dotfiles
This repository contains configuration files and scripts tailored to my needs
or preferences. Some of these might prove useful to other people. The most
custom goodies are located in the bin/ directory.

What it roughly looks like when I'm not using a web browser
Repository structure

bin/: custom tools
cfg/: configuration and installers for third party programs
lib/: code shared between the scripts in bin/ and the modules in mod/
opt/: projects too big to fit in bin/
txt/: miscellaneous text resources

Installing a module
Every module can be installed with ./install like this:
./install cfg/zsh
./install cfg/vim
./install cfg/term/urxvt
Most things are installed using symbolic links.
The installation scripts also try to install relevant packages using various
package managers, e.g. zsh will try downloading only zsh, while bspwm
will download bspwm-git, PyQt4 and other dependencies required for the full
bspwm setup.
Caveats
Some modules will work only on GNU/Linux, but essential ones such as vim or
zsh will also work on Cygwin.
While the repository tries to be modular, some things (the ones having to do
with graphical environment) may not work. For example, I haven't tested panel
behavior if there is no mpd installed. Similarly, most of the AutoHotkey stuff
makes sense only if one has installed Cygwin and Firefox.
",batch2,8:10:17,Done
338,dankamongmen/dankcheese,,batch2,8:10:18,Done
339,asoltys/bitcoinjs-lib,"BitcoinJS (bitcoinjs-lib)



A javascript Bitcoin library for node.js and browsers. Written in TypeScript, but committing the JS files to verify.
Released under the terms of the MIT LICENSE.
Should I use this in production?
If you are thinking of using the master branch of this library in production, stop.
Master is not stable; it is our development branch, and only tagged releases may be classified as stable.
Can I trust this code?

Don't trust. Verify.

We recommend every user of this library and the bitcoinjs ecosystem audit and verify any underlying code for its validity and suitability,  including reviewing any and all of your project's dependencies.
Mistakes and bugs happen, but with your help in resolving and reporting issues, together we can produce open source software that is:

Easy to audit and verify,
Tested, with test coverage >95%,
Advanced and feature rich,
Standardized, using prettier and Node Buffer's throughout, and
Friendly, with a strong and helpful community, ready to answer questions.

Documentation
Presently,  we do not have any formal documentation other than our examples, please ask for help if our examples aren't enough to guide you.
Installation
npm install bitcoinjs-lib
Typically we support the Node Maintenance LTS version.
If in doubt, see the .travis.yml for what versions are used by our continuous integration tests.
WARNING: We presently don't provide any tooling to verify that the release on npm matches GitHub.  As such, you should verify anything downloaded by npm against your own verified copy.
Usage
Crypto is hard.
When working with private keys, the random number generator is fundamentally one of the most important parts of any software you write.
For random number generation, we default to the randombytes module, which uses window.crypto.getRandomValues in the browser, or Node js' crypto.randomBytes, depending on your build system.
Although this default is ~OK, there is no simple way to detect if the underlying RNG provided is good enough, or if it is catastrophically bad.
You should always verify this yourself to your own standards.
This library uses tiny-secp256k1, which uses RFC6979 to help prevent k re-use and exploitation.
Unfortunately, this isn't a silver bullet.
Often, Javascript itself is working against us by bypassing these counter-measures.
Problems in Buffer (UInt8Array), for example, can trivially result in catastrophic fund loss without any warning.
It can do this through undermining your random number generation, accidentally producing a duplicate k value, sending Bitcoin to a malformed output script, or any of a million different ways.
Running tests in your target environment is important and a recommended step to verify continuously.
Finally, adhere to best practice.
We are not an authorative source of best practice, but, at the very least:

Don't re-use addresses.
Don't share BIP32 extended public keys ('xpubs'). They are a liability, and it only takes 1 misplaced private key (or a buggy implementation!) and you are vulnerable to catastrophic fund loss.
Don't use Math.random - in any way - don't.
Enforce that users always verify (manually) a freshly-decoded human-readable version of their intended transaction before broadcast.
Don't ask users to generate mnemonics, or 'brain wallets',  humans are terrible random number generators.
Lastly, if you can, use Typescript or similar.

Browser
The recommended method of using bitcoinjs-lib in your browser is through Browserify.
If you're familiar with how to use browserify, ignore this and carry on, otherwise, it is recommended to read the tutorial at https://browserify.org/.
NOTE: We use Node Maintenance LTS features, if you need strict ES5, use --transform babelify in conjunction with your browserify step (using an es2015 preset).
WARNING: iOS devices have problems, use atleast buffer@5.0.5 or greater,  and enforce the test suites (for Buffer, and any other dependency) pass before use.
Typescript or VSCode users
Type declarations for Typescript are included in this library. Normal installation should include all the needed type information.
Examples
The below examples are implemented as integration tests, they should be very easy to understand.
Otherwise, pull requests are appreciated.
Some examples interact (via HTTPS) with a 3rd Party Blockchain Provider (3PBP).

Generate a random address
Import an address via WIF
Generate a 2-of-3 P2SH multisig address
Generate a SegWit address
Generate a SegWit P2SH address
Generate a SegWit 3-of-4 multisig address
Generate a SegWit 2-of-2 P2SH multisig address
Support the retrieval of transactions for an address (3rd party blockchain)
Generate a Testnet address
Generate a Litecoin address
Create a 1-to-1 Transaction
Create (and broadcast via 3PBP) a typical Transaction
Create (and broadcast via 3PBP) a Transaction with an OP_RETURN output
Create (and broadcast via 3PBP) a Transaction with a 2-of-4 P2SH(multisig) input
Create (and broadcast via 3PBP) a Transaction with a SegWit P2SH(P2WPKH) input
Create (and broadcast via 3PBP) a Transaction with a SegWit P2WPKH input
Create (and broadcast via 3PBP) a Transaction with a SegWit P2PK input
Create (and broadcast via 3PBP) a Transaction with a SegWit 3-of-4 P2SH(P2WSH(multisig)) input
Create (and broadcast via 3PBP) a Transaction and sign with an HDSigner interface (bip32)
Import a BIP32 testnet xpriv and export to WIF
Export a BIP32 xpriv, then import it
Export a BIP32 xpub
Create a BIP32, bitcoin, account 0, external address
Create a BIP44, bitcoin, account 0, external address
Create a BIP49, bitcoin testnet, account 0, external address
Use BIP39 to generate BIP32 addresses
Create (and broadcast via 3PBP) a Transaction where Alice can redeem the output after the expiry (in the past)
Create (and broadcast via 3PBP) a Transaction where Alice can redeem the output after the expiry (in the future)
Create (and broadcast via 3PBP) a Transaction where Alice and Bob can redeem the output at any time
Create (but fail to broadcast via 3PBP) a Transaction where Alice attempts to redeem before the expiry
Create (and broadcast via 3PBP) a Transaction where Alice can redeem the output after the expiry (in the future) (simple CHECKSEQUENCEVERIFY)
Create (but fail to broadcast via 3PBP) a Transaction where Alice attempts to redeem before the expiry (simple CHECKSEQUENCEVERIFY)
Create (and broadcast via 3PBP) a Transaction where Bob and Charles can send (complex CHECKSEQUENCEVERIFY)
Create (and broadcast via 3PBP) a Transaction where Alice (mediator) and Bob can send after 2 blocks (complex CHECKSEQUENCEVERIFY)
Create (and broadcast via 3PBP) a Transaction where Alice (mediator) can send after 5 blocks (complex CHECKSEQUENCEVERIFY)

If you have a use case that you feel could be listed here, please ask for it!
Contributing
See CONTRIBUTING.md.
Running the test suite
npm test
npm run-script coverage
Complementing Libraries

BIP21 - A BIP21 compatible URL encoding library
BIP38 - Passphrase-protected private keys
BIP39 - Mnemonic generation for deterministic keys
BIP32-Utils - A set of utilities for working with BIP32
BIP66 - Strict DER signature decoding
BIP68 - Relative lock-time encoding library
BIP69 - Lexicographical Indexing of Transaction Inputs and Outputs
Base58 - Base58 encoding/decoding
Base58 Check - Base58 check encoding/decoding
Bech32 - A BIP173 compliant Bech32 encoding library
coinselect - A fee-optimizing, transaction input selection module for bitcoinjs-lib.
merkle-lib - A performance conscious library for merkle root and tree calculations.
minimaldata - A module to check bitcoin policy: SCRIPT_VERIFY_MINIMALDATA

Alternatives

BCoin
Bitcore
Cryptocoin

LICENSE MIT
",batch2,8:09:16,Done
340,mjs973b/jukebox,,batch2,8:10:17,Done
341,tsingyixy77/emacs,,batch1,16:59:00,Done
342,Markionium/material-components-web,"Material Components for the web
Material Components for the web (MDC-Web) help developers execute Material Design.
Developed by a core team of engineers and UX designers at Google, these components enable a reliable development workflow to build beautiful and functional web projects.
Material Components for the web is the successor to Material Design Lite, and has 3 high-level goals:

Production-ready components consumable in an a-la-carte fashion
Best-in-class performance and adherence to the Material Design guidelines
Seamless integration with other JS frameworks and libraries

Quick start
Install the library
npm install --save material-components-web

Then simply include the correct files, write some HTML, and call mdc.autoInit() within a closing
<script> tag.
<!DOCTYPE html>
<html class=""mdc-typography"">
  <head>
    <title>Material Components for the web</title>
    <link rel=""stylesheet""
          href=""node_modules/material-components-web/dist/material-components-web.css"">
  </head>
  <body>
    <h2 class=""mdc-typography--display2"">Hello, Material Components!</h2>
    <div class=""mdc-textfield"" data-mdc-auto-init=""MDCTextfield"">
      <input type=""text"" class=""mdc-textfield__input"" id=""demo-input"">
      <label for=""demo-input"" class=""mdc-textfield__label"">Tell us how you feel!</label>
    </div>
    <script src=""node_modules/material-components-web/dist/material-components-web.js""></script>
    <script>mdc.autoInit()</script>
  </body>
</html>
That's all there is to it! This is the easiest way to get up and running with Material Components
for web. Check out our Getting Started guide for a more in-depth
introduction to the library.
Installing individual components
MDC-Web is modular by design. Each component lives within its own packages under the
@material npm scope.
npm install --save @material/button @material/card @material/textfield @material/typography

All our components can be found in the packages directory. Each component has a
README documenting installation and usage.
Including components
JavaScript
If you are using a module loader such as Webpack or SystemJS to load your JS modules, you can simply
import every component you need from material-components-web and use it as such.
import {checkbox as mdcCheckbox} from 'material-components-web';

const {MDCCheckbox, MDCCheckboxFoundation} = mdcCheckbox;
// Use MDCCheckbox and/or MDCCheckboxFoundation
You can do the same with individual components
import {MDCCheckbox, MDCCheckboxFoundation} from '@material/checkbox';
// Use MDCCheckbox and/or MDCCheckboxFoundation
We also provide UMD bundles for both material-components-web as
well as all individual components.
const {checkbox: mdcCheckbox} = require('material-components-web/dist/material-components-web');
// Use mdcCheckbox

const {MDCCheckbox, MDCCheckboxFoundation} = require('@material/checkbox/dist/mdc.checkbox');
// Use MDCCheckbox, MDCCheckboxFoundation
When no module system is used, every component is added under the global mdc namespace. This
occurs regardless of whether or not the entire library or the individual components are used.
Every component also ships with a minified version of its UMD bundle, which can be found at
dist/mdc.COMPONENT.min.js.
CSS
All components which include styles provide them at dist/mdc.COMPONENT.css, as well as a
complementary minified version at dist/mdc.COMPONENT.min.css. Note that CSS files for a
component's dependencies are not included within the component's CSS file, so if you are using
individual components you'll have to include each separately.
Each component also comes with a Sass source file that can be included in your application's Sass
// Using the whole library
@import 'material-components-web';

// Using individual components / mixins
@import '@material/checkbox';
@import '@material/typography';
@import '@material/elevation/mixins'; // Mixins for elevation.

NOTE: The components' Sass files expect that the node_modules directory containing the
@material scope folder is present on the Sass include path.

Running the demos
Setup the repo:
git clone https://github.com/material-components/material-components-web.git && cd material-components-web
npm i

Run the development server (served out of demos/):
cd /path/to/material-components-web
npm run dev
open http://localhost:8080

Useful Links

Getting Started Guide
Developer's Guide
All Components
Demos
Framework Integration Examples
Contributing
Material.io (external site)
Material Design Guidelines (external site)

Browser Support

Chrome
Safari 8+
Firefox
IE 11/Edge
Opera

",batch2,8:09:16,Done
343,supcoin/supcoin,"Supcoin Core integration/staging tree

https://www.supcoin.org
What is Supcoin?
Supcoin is an experimental new digital currency that enables instant payments to
anyone, anywhere in the world. Supcoin uses peer-to-peer technology to operate
with no central authority: managing transactions and issuing money are carried
out collectively by the network. Supcoin Core is the name of open source
software which enables the use of this currency.
For more information, as well as an immediately useable, binary version of
the Supcoin Core software, see https://www.supcoin.org/en/download.
License
Supcoin Core is released under the terms of the MIT license. See COPYING for more
information or see http://opensource.org/licenses/MIT.
Development process
Developers work in their own trees, then submit pull requests when they think
their feature or bug fix is ready.
If it is a simple/trivial/non-controversial change, then one of the Supcoin
development team members simply pulls it.
If it is a more complicated or potentially controversial change, then the patch
submitter will be asked to start a discussion (if they haven't already) on the
mailing list.
The patch will be accepted if there is broad consensus that it is a good thing.
Developers should expect to rework and resubmit patches if the code doesn't
match the project's coding conventions (see doc/coding.md) or are
controversial.
The master branch is regularly built and tested, but is not guaranteed to be
completely stable. Tags are created
regularly to indicate new official, stable release versions of Supcoin.
Testing
Testing and code review is the bottleneck for development; we get more pull
requests than we can review and test on short notice. Please be patient and help out by testing
other people's pull requests, and remember this is a security-critical project where any mistake might cost people
lots of money.
Automated Testing
Developers are strongly encouraged to write unit tests for new code, and to
submit new unit tests for old code. Unit tests can be compiled and run (assuming they weren't disabled in configure) with: make check
Every pull request is built for both Windows and Linux on a dedicated server,
and unit and sanity tests are automatically run. The binaries produced may be
used for manual QA testing — a link to them will appear in a comment on the
pull request posted by SupcoinPullTester. See https://github.com/TheBlueMatt/test-scripts
for the build/test scripts.
Manual Quality Assurance (QA) Testing
Large changes should have a test plan, and should be tested by somebody other
than the developer who wrote the code.
See https://github.com/supcoin/QA/ for how to create a test plan.
Translations
Changes to translations as well as new translations can be submitted to
Supcoin Core's Transifex page.
Translations are periodically pulled from Transifex and merged into the git repository. See the
translation process for details on how this works.
Important: We do not accept translation changes as GitHub pull requests because the next
pull from Transifex would automatically overwrite them again.
Translators should also subscribe to the mailing list.
Development tips and tricks
compiling for debugging
Run configure with the --enable-debug option, then make. Or run configure with
CXXFLAGS=""-g -ggdb -O0"" or whatever debug flags you need.
debug.log
If the code is behaving strangely, take a look in the debug.log file in the data directory;
error and debugging messages are written there.
The -debug=... command-line option controls debugging; running with just -debug will turn
on all categories (and give you a very large debug.log file).
The Qt code routes qDebug() output to debug.log under category ""qt"": run with -debug=qt
to see it.
testnet and regtest modes
Run with the -testnet option to run with ""play supcoins"" on the test network, if you
are testing multi-machine code that needs to operate across the internet.
If you are testing something that can run on one machine, run with the -regtest option.
In regression test mode, blocks can be created on-demand; see qa/rpc-tests/ for tests
that run in -regtest mode.
DEBUG_LOCKORDER
Supcoin Core is a multithreaded application, and deadlocks or other multithreading bugs
can be very difficult to track down. Compiling with -DDEBUG_LOCKORDER (configure
CXXFLAGS=""-DDEBUG_LOCKORDER -g"") inserts run-time checks to keep track of which locks
are held, and adds warnings to the debug.log file if inconsistencies are detected.
",batch2,8:09:15,Done
344,remacs/remacs,"Rust ❤️ Emacs
This project isn't maintained anymore. If you are looking for a rust based emacs fork, you can take a look at emacs-ng. However this fork is not about replacing the C code base, but to provide additional features using the rich ecosystem of rust.


A community-driven port of Emacs to Rust.
Table of Contents

Why Emacs?
Why Rust?
Why A Fork?
Getting Started

Requirements
Dockerized development environment
Building Remacs
Running Remacs


Design Goals
Progress
Porting Elisp Primitive Functions
Contributing

Why Emacs?
Emacs will change how you think about programming.
Emacs is totally introspectable. You can always find out 'what
code runs when I press this button?'.
Emacs is an incremental programming environment. There's no
edit-compile-run cycle. There isn't even an edit-run cycle. You can
execute snippets of code and gradually turn them into a finished
project. There's no distinction between your editor and your
interpreter.
Emacs is a mutable environment. You can set variables, tweak
functions with advice, or redefine entire functions. Nothing is
off-limits.
Emacs provides functionality without applications. Rather than
separate applications, functionality is all integrated into your Emacs
instance. Amazingly, this works. Ever wanted to use the same snippet
tool for writing C++ classes as well as emails?
Emacs is full of incredible software concepts that haven't hit the
mainstream yet. For example:

Many platforms have a single item clipboard. Emacs has an infinite
clipboard.
If you undo a change, and then continue editing, you can't redo the
original change. Emacs allows undoing to any historical state, even
allowing tree-based exploration of history.
Emacs supports a reverse variable search: you can find variables
with a given value.
You can perform structural editing of code, allowing you to make
changes without breaking syntax. This works for lisps (paredit) and
non-lisps (smartparens).
Many applications use a modal GUI: for example, you can't do other
edits during a find-and-replace operation. Emacs provides
recursive editing that allow you to suspend what you're
currently doing, perform other edits, then continue the original
task.

Emacs has a documentation culture. Emacs includes a usage manual,
a lisp programming manual, pervasive docstrings and even an
interactive tutorial.
Emacs has a broad ecosystem. If you want to edit code in a
niche language, there's probably an Emacs package for it.
Emacs doesn't have a monopoly on good ideas, and there are other great
tools out there. Nonetheless, we believe the Emacs learning curve pays
off.
Why Rust?
Rust is a great alternative to C.
Rust has a fantastic learning curve. The documentation is superb,
and the community is very helpful if you get stuck.
Rust has excellent tooling. The compiler makes great suggestions,
the unit test framework is good, and rustfmt helps ensure formatting
is beautiful and consistent.
The Rust packaging story is excellent. It's easy to reuse
the great libraries available, and just as easy to factor out code for
the benefit of others. We can replace entire C files in Emacs with
well-maintained Rust libraries.
Code written in Rust easily interoperates with C. This means we
can port to Rust incrementally, and having a working Emacs at each
step of the process.
Rust provides many compile-time checks, making it much easier to write
fast, correct code (even when using multithreading). This also makes
it much easier for newcomers to contribute.
Give it a try. We think you'll like it.
Why A Fork?
Emacs is a widely used tool with a long history, broad platform
support and strong backward compatibility requirements. The core team
is understandably cautious in making far-reaching changes.
Forking is a longstanding tradition in the Emacs community for trying
different approaches. Notable Emacs forks include XEmacs,
Guile Emacs,
and emacs-jit.
There have also been separate elisp implementations, such as
Deuce,
JEmacs and
El Compilador.
By forking, we can explore new development approaches. We can
use a pull request workflow with integrated CI.
We can drop legacy platforms and compilers. Remacs will never run
on MS-DOS, and that's OK.
There's a difference between the idea of Emacs and the current
implementation of Emacs. Forking allows us to explore being even
more Emacs-y.
Getting Started
Requirements


You will need
Rust installed.
The file rust-toolchain indicates the version that gets installed.
This happens automatically, so don't override the toolchain manually.
IMPORTANT: Whenever the toolchain updates, you have to reinstall
rustfmt manually.


You will need a C compiler and toolchain. On Linux, you can do
something like:
 apt install build-essential automake clang libclang-dev

On macOS, you'll need Xcode.


Linux:
 apt install texinfo libjpeg-dev libtiff-dev \
   libgif-dev libxpm-dev libgtk-3-dev gnutls-dev \
   libncurses5-dev libxml2-dev libxt-dev

macOS:
 brew install gnutls texinfo autoconf

To use the installed version of makeinfo instead of the built-in
(/usr/bin/makeinfo) one, you'll need to make sure /usr/local/opt/texinfo/bin
is before /usr/bin in PATH.
Mojave install libxml2 headers with: open /Library/Developer/CommandLineTools/Packages/macOS_SDK_headers_for_macOS_10.14.pkg


Dockerized development environment
If you don't want to bother with the above setup you can use the
provided Docker environment. Make sure you have
docker 1.12+ and
docker-compose 1.8+ available.
To spin up the environment run
docker-compose up -d
The first time you run this command, Docker will build the image. After
that any subsequent startups will happen in less than a second. If
this command fails because of needing absolute paths, make sure to set
the PWD environment variable before calling the command like so:
PWD=$(pwd) docker-compose up -d
The working directory with remacs will be mounted under the same path
in the container so editing the files on your host machine will
automatically be reflected inside the container. To build remacs use
the steps from Building Remacs prefixed with
docker-compose exec remacs, this will ensure the commands are
executed inside the container.
Building Remacs
$ ./autogen.sh
$ ./configure --enable-rust-debug
$ make

For a release build, don't pass --enable-rust-debug.
The Makefile obeys cargo's RUSTFLAGS variable and additional options
can be passed to cargo with CARGO_FLAGS.
For example:
$ make CARGO_FLAGS=""-vv"" RUSTFLAGS=""-Zunstable-options --cfg MARKER_DEBUG""
Running Remacs
You can now run your shiny new Remacs build!
# Using -q to ignore your .emacs.d, so Remacs starts up quickly.
# RUST_BACKTRACE is optional, but useful if your instance crashes.
$ RUST_BACKTRACE=1 src/remacs -q
Design Goals
Compatibility: Remacs should not break existing elisp code, and
ideally provide the same FFI too.
Leverage Rust itself: Remacs should make best use of Rust to
ensure code is robust and performant.
Leverage the Rust ecosystem: Remacs should use existing Rust
crates wherever possible, and create new, separate crates where our
code could benefit others.
Great docs: Emacs has excellent documentation, Remacs should be no
different.
Progress
At this point we focus on porting lisp functions from C to Rust.
Currently there are 642 functions in Rust and 823 in C (May 2019).
We have a progress section in our wiki
and there's also a list of long-term goals
under projects.
Porting Elisp Primitive Functions
The first thing to look at is the C implementation for the atan
function. It takes an optional second argument, which makes it
interesting. The complicated mathematical bits, on the other hand, are
handled by the standard library. This allows us to focus on the
porting process without getting distracted by the math.
The Lisp values we are given as arguments are tagged pointers; in this
case they are pointers to doubles. The code has to check the tag and
follow the pointer to retrieve the real values. Note that this code
invokes a C macro (called DEFUN) that reduces some of the
boilerplate. The macro declares a static variable called Satan that
holds the metadata the Lisp compiler will need in order to
successfully call this function, such as the docstring and the pointer
to the Fatan function, which is what the C implementation is named:
DEFUN (""atan"", Fatan, Satan, 1, 2, 0,
       doc: /* Return the inverse tangent of the arguments.
If only one argument Y is given, return the inverse tangent of Y.
If two arguments Y and X are given, return the inverse tangent of Y
divided by X, i.e. the angle in radians between the vector (X, Y)
and the x-axis.  */)
  (Lisp_Object y, Lisp_Object x)
{
  double d = extract_float (y);
  if (NILP (x))
    d = atan (d);
  else
    {
      double d2 = extract_float (x);
      d = atan2 (d, d2);
    }
  return make_float (d);
}
extract_float checks the tag (signalling an ""invalid argument"" error
if it's not the tag for a double), and returns the actual
value. NILP checks to see if the tag indicates that this is a null
value, indicating that the user didn't supply a second argument at
all.
Next take a look at the current Rust implementation. It must also take
an optional argument, and it also invokes a (Rust) macro to reduce the
boilerplate of declaring the static data for the function. However, it
also takes care of all of the type conversions and checks that we need
to do in order to handle the arguments and return value:
/// Return the inverse tangent of the arguments.
/// If only one argument Y is given, return the inverse tangent of Y.
/// If two arguments Y and X are given, return the inverse tangent of Y
/// divided by X, i.e. the angle in radians between the vector (X, Y)
/// and the x-axis
#[lisp_fn(min = ""1"")]
pub fn atan(y: EmacsDouble, x: Option<EmacsDouble>) -> EmacsDouble {
    match x {
        None => y.atan(),
        Some(x) => y.atan2(x)
    }
}
You can see that we don't have to check to see if our arguments are of
the correct type, the code generated by the lisp_fn macro does this
for us. We also asked for the second argument to be an
Option<EmacsDouble>. This is the Rust type for a value which is
either a valid double or isn't specified at all. We use a match
statement to handle both cases.
This code is so much better that it's hard to believe just how simple
the implementation of the macro is. It just calls .into() on the
arguments and the return value; the compiler does the rest when it
dispatches this method call to the correct implementation.
Contributing
Pull requests welcome, no copyright assignment required. This project is under the
Rust code of conduct.
There's lots to do! We keep a list of
low hanging fruit
here so you can easily choose one. You can find information in the
Porting cookbook
or ask for help in our Gitter channel.
",batch1,16:58:59,Done
345,miRkwood-RNA/miRkwood,,batch2,8:10:17,Done
346,magnetophon/nixpkgs.newer,"






Nixpkgs is a collection of over
40,000 software packages that can be installed with the
Nix package manager. It also implements
NixOS, a purely-functional Linux distribution.
Manuals

NixOS Manual - how to install, configure, and maintain a purely-functional Linux distribution
Nixpkgs Manual - contributing to Nixpkgs and using programming-language-specific Nix expressions
Nix Package Manager Manual - how to write Nix expressions (programs), and how to use Nix command line tools

Community

Discourse Forum
IRC - #nixos on freenode.net
NixOS Weekly
Community-maintained wiki
Community-maintained list of ways to get in touch (Discord, Matrix, Telegram, other IRC channels, etc.)

Other Project Repositories
The sources of all official Nix-related projects are in the NixOS
organization on GitHub. Here are some of
the main ones:

Nix - the purely functional package manager
NixOps - the tool to remotely deploy NixOS machines
nixos-hardware - NixOS profiles to optimize settings for different hardware
Nix RFCs - the formal process for making substantial changes to the community
NixOS homepage - the NixOS.org website
hydra - our continuous integration system
NixOS Artwork - NixOS artwork

Continuous Integration and Distribution
Nixpkgs and NixOS are built and tested by our continuous integration
system, Hydra.

Continuous package builds for unstable/master
Continuous package builds for the NixOS 20.03 release
Tests for unstable/master
Tests for the NixOS 20.03 release

Artifacts successfully built with Hydra are published to cache at
https://cache.nixos.org/. When successful build and test criteria are
met, the Nixpkgs expressions are distributed via Nix
channels.
Contributing
Nixpkgs is among the most active projects on GitHub. While thousands
of open issues and pull requests might seem a lot at first, it helps
consider it in the context of the scope of the project. Nixpkgs
describes how to build over 40,000 pieces of software and implements a
Linux distribution. The GitHub Insights
page gives a sense of the project activity.
Community contributions are always welcome through GitHub Issues and
Pull Requests. When pull requests are made, our tooling automation bot,
OfBorg will perform various checks
to help ensure expression quality.
The Nixpkgs maintainers are people who have assigned themselves to
maintain specific individual packages. We encourage people who care
about a package to assign themselves as a maintainer. When a pull
request is made against a package, OfBorg will notify the appropriate
maintainer(s). The Nixpkgs committers are people who have been given
permission to merge.
Most contributions are based on and merged into these branches:

master is the main branch where all small contributions go
staging is branched from master, changes that have a big impact on
Hydra builds go to this branch
staging-next is branched from staging and only fixes to stabilize
and security fixes with a big impact on Hydra builds should be
contributed to this branch. This branch is merged into master when
deemed of sufficiently high quality

For more information about contributing to the project, please visit
the contributing page.
Donations
The infrastructure for NixOS and related projects is maintained by a
nonprofit organization, the NixOS
Foundation. To ensure the
continuity and expansion of the NixOS infrastructure, we are looking
for donations to our organization.
You can donate to the NixOS foundation by using Open Collective:

License
Nixpkgs is licensed under the MIT License.
Note: MIT license does not apply to the packages built by Nixpkgs,
merely to the files in this repository (the Nix expressions, build
scripts, NixOS modules, etc.). It also might not apply to patches
included in Nixpkgs, which may be derivative works of the packages to
which they apply. The aforementioned artifacts are all covered by the
licenses of the respective packages.
",batch1,16:57:58,Done
347,AuScope/portal-core,"Portal-Core Developer's Guide
https://confluence.csiro.au/display/AusGRID/Portal+Core+-+Setting+up+your+development+environment
",batch2,8:10:17,Done
348,lewis-revill/agc-llvm,,batch1,16:57:59,Done
349,eteran/edb-debugger,"




edb is a cross platform AArch32/x86/x86-64 debugger. It was inspired by Ollydbg,
but aims to function on AArch32, x86, and x86-64 as well as multiple OS's. Linux is the
only officially supported platform at the moment, but FreeBSD, OpenBSD, OSX and
Windows ports are underway with varying degrees of functionality.

edb is available under the GPL 2 license, see the COPYING for details.
NOTE: This README now only covers the most essential documentation, for more
complete documentation see the wiki
Cloning
When cloning the repo, please use git's --recursive flag to ensure that the
sub-modules will be properly cloned and updated to the correct versions.
Here is an example:
git clone --recursive https://github.com/eteran/edb-debugger.git
Compiling
Compiling edb is generally quite simple. The latest release of edb currently
depends on the following packages:



Dependency
Version Required




GCC/Clang
Supporting C++14


Qt
>= 5.2


Boost (Headers Only)
>= 1.35


Capstone
>= 3.0


Graphviz
>= 2.38.0 (Optional)



The development master branch will be increasing the minimum requirements to:



Dependency
Version Required




GCC/Clang
Supporting C++17


Qt
>= 5.9


Capstone
>= 3.0


Graphviz
>= 2.38.0 (Optional)



Many distributions already have packages that satisify these. The wiki contains
examples for some popular distributions:

https://github.com/eteran/edb-debugger/wiki/Compiling-(Fedora)
https://github.com/eteran/edb-debugger/wiki/Compiling-(Ubuntu)
https://github.com/eteran/edb-debugger/wiki/Compiling-(Debian)

Once you have the necessary dependencies installed, compilation is done with
cmake:
CMake
If you plan to just run edb out of the build directory, it's as simple as this:
$ mkdir build
$ cd build
$ cmake ..
$ make
$ ./edb

If you would like to properly install edb on the system for all users, it's
only a little different:
$ mkdir build
$ cd build
$ cmake -DCMAKE_INSTALL_PREFIX=/usr/local/ ..
$ make
$ make install
$ edb

Installing
Basic installation is simple, you may run
$ make install

In which case the plugins will be installed in /usr/local/lib/edb and the
binaries will be installed in /usr/local/bin/.

",batch2,8:09:16,Done
350,Ergus/Emacs,,batch1,16:59:00,Done
351,flyingcircusio/nixpkgs,"






Nixpkgs is a collection of over
40,000 software packages that can be installed with the
Nix package manager. It also implements
NixOS, a purely-functional Linux distribution.
Manuals

NixOS Manual - how to install, configure, and maintain a purely-functional Linux distribution
Nixpkgs Manual - contributing to Nixpkgs and using programming-language-specific Nix expressions
Nix Package Manager Manual - how to write Nix expressions (programs), and how to use Nix command line tools

Community

Discourse Forum
IRC - #nixos on freenode.net
NixOS Weekly
Community-maintained wiki
Community-maintained list of ways to get in touch (Discord, Matrix, Telegram, other IRC channels, etc.)

Other Project Repositories
The sources of all official Nix-related projects are in the NixOS
organization on GitHub. Here are some of
the main ones:

Nix - the purely functional package manager
NixOps - the tool to remotely deploy NixOS machines
nixos-hardware - NixOS profiles to optimize settings for different hardware
Nix RFCs - the formal process for making substantial changes to the community
NixOS homepage - the NixOS.org website
hydra - our continuous integration system
NixOS Artwork - NixOS artwork

Continuous Integration and Distribution
Nixpkgs and NixOS are built and tested by our continuous integration
system, Hydra.

Continuous package builds for unstable/master
Continuous package builds for the NixOS 20.09 release
Tests for unstable/master
Tests for the NixOS 20.09 release

Artifacts successfully built with Hydra are published to cache at
https://cache.nixos.org/. When successful build and test criteria are
met, the Nixpkgs expressions are distributed via Nix
channels.
Contributing
Nixpkgs is among the most active projects on GitHub. While thousands
of open issues and pull requests might seem a lot at first, it helps
consider it in the context of the scope of the project. Nixpkgs
describes how to build over 40,000 pieces of software and implements a
Linux distribution. The GitHub Insights
page gives a sense of the project activity.
Community contributions are always welcome through GitHub Issues and
Pull Requests. When pull requests are made, our tooling automation bot,
OfBorg will perform various checks
to help ensure expression quality.
The Nixpkgs maintainers are people who have assigned themselves to
maintain specific individual packages. We encourage people who care
about a package to assign themselves as a maintainer. When a pull
request is made against a package, OfBorg will notify the appropriate
maintainer(s). The Nixpkgs committers are people who have been given
permission to merge.
Most contributions are based on and merged into these branches:

master is the main branch where all small contributions go
staging is branched from master, changes that have a big impact on
Hydra builds go to this branch
staging-next is branched from staging and only fixes to stabilize
and security fixes with a big impact on Hydra builds should be
contributed to this branch. This branch is merged into master when
deemed of sufficiently high quality

For more information about contributing to the project, please visit
the contributing page.
Donations
The infrastructure for NixOS and related projects is maintained by a
nonprofit organization, the NixOS
Foundation. To ensure the
continuity and expansion of the NixOS infrastructure, we are looking
for donations to our organization.
You can donate to the NixOS foundation by using Open Collective:

License
Nixpkgs is licensed under the MIT License.
Note: MIT license does not apply to the packages built by Nixpkgs,
merely to the files in this repository (the Nix expressions, build
scripts, NixOS modules, etc.). It also might not apply to patches
included in Nixpkgs, which may be derivative works of the packages to
which they apply. The aforementioned artifacts are all covered by the
licenses of the respective packages.
",batch1,16:57:59,Done
352,pk-codebox-evo/intellij-community-jan-2017,"IntelliJ IDEA Community Edition
Building and Running from the IDE
To develop IntelliJ IDEA, you can use either IntelliJ IDEA Community Edition or IntelliJ IDEA Ultimate not older than 15.0. To build and run the code:

Run getPlugins.sh / getPlugins.bat from the project root directory to check out additional modules.
If this git repository is not on 'master' branch you need to checkout the same branches/tags in android and android/tools-base git repositories.
Open the project.
If an error notification about a missing required plugin (e.g. Kotlin) is shown enable or install that plugin.
Configure a JSDK named ""IDEA jdk"" (case sensitive), pointing to an installation of JDK 1.6.
Unless you're running on a Mac with an Apple JDK, add <JDK_HOME>/lib/tools.jar to the set of ""IDEA jdk"" jars.
Configure a JSDK named ""1.8"", pointing to an installation of JDK 1.8.
Add <JDK_18_HOME>/lib/tools.jar to the set of ""1.8"" jars.
Use Build | Build Project to build the code.

To run the IDE from the built code

use the provided shared run configuration ""IDEA"".

To run tests apply these setting to the default JUnit run configuration type:

Working dir:
<IDEA_HOME>\bin
VM parameters:
-ea -Xbootclasspath/p:../out/classes/production/boot -Xmx128M -Djava.system.class.loader=com.intellij.util.lang.UrlClassLoader -Didea.config.path=../test-config -Didea.system.path=../test-system

You can find other useful information at http://www.jetbrains.org. Contribute section of that site describes how you can contribute to IntelliJ IDEA.
",batch1,16:57:58,Done
353,g0v-data/mirror-minutely,,batch1,16:57:57,Done
354,joLapss/volumio,"







Volumio is an headless audiophile music player, designed to play music with the highest possible fidelity. Volumio runs on most embedded devices (Raspberry Pi, UDOO, Odroid, Cubieboard, Beaglebone, Pine64, Allo Sparky...) and on any ordinary PC (x86).
Volumio 2 System Images
LATEST

Link : https://volumio.org/get-started/

Logins

user : volumio
password : volumio

Root login via ssh has been disabled by default, however user volumio can become root.
Resources
Developers are welcome! Check out the resources:

Main documentation
Setup a dev environment
Forum thread for feedbacks and suggestions
Wiki (there might be some overlap between the documentation and the wiki; in general, consider documentation as the primary information source)

Volumio 2 Virtual Machines
Some VM images are available, although they're not always kept up-to-date. It can be an alternative to developing directly on target (Raspberry Pi, or whatever).
VMWARE Image is suggested, as Network configuration is tricky with Virtual Box

VMWare Virtual Machine - Beta1
Virtual Box Virtual Machine - Alpha5

System Images built with Volumio Builder
",batch2,8:09:16,Done
355,kybkqkybkq/llvm,,batch1,16:57:59,Done
356,norbusan/mupdf-debian,,batch2,8:09:16,Done
357,koktf23/libsndfile,"libsndfile


libsndfile is a C library for reading and writing files containing sampled audio
data.
Hacking
The canonical source code repository for libsndfile is at
https://github.com/erikd/libsndfile/.
You can grab the source code using:
git clone git://github.com/erikd/libsndfile.git

For building for Android see BuildingForAndroid.
There are currently two build systems; the official GNU autotool based one and
a more limited and experimental CMake based build system. Use of the CMake build
system is documented below.
Setting up a build environment for libsndfile on Debian or Ubuntu is as simple as:
sudo apt install autoconf autogen automake build-essential libasound2-dev \
    libflac-dev libogg-dev libtool libvorbis-dev pkg-config python

For other Linux distributions or any of the *BSDs, the setup should be similar
although the package install tools and package names may be slightly different.
Similarly on Mac OS X, assuming brew is already installed:
brew install autoconf autogen automake flac libogg libtool libvorbis pkg-config

Once the build environment has been set up, building and testing libsndfile is
as simple as:
./autogen.sh
./configure --enable-werror
make
make check

The CMake build system
Although Autotools is the primary and recommended build toolchain, experimental
CMake meta build generator is also available. The build process with CMake takes
place in two stages. First, standard build files are created from configuration
scripts. Then the platform's native build tools are used for the actual
building. CMake can produce Microsoft Visual Studio project and solution files,
Unix Makefiles, Xcode projects and many more.
Some IDE support CMake natively or with plugins, check you IDE documentation
for details.
Requirements

C99-compliant compiler toolchain (tested with GCC, Clang and Visual
Studio 2015)
CMake 3.1.3 or newer

There are some recommended packages to enable all features of libsndfile:

Ogg, Vorbis and FLAC libraries and headers to enable these formats support
ALSA development package under Linux to build sndfile-play utility
Sndio development package under BSD to build sndfile-play utility

Building from command line
CMake can handle out-of-place builds, enabling several builds from
the same source tree, and cross-compilation. The ability to build a directory
tree outside the source tree is a key feature, ensuring that if a build
directory is removed, the source files remain unaffected.
mkdir CMakeBuild
cd CMakeBuild

Then run cmake command with directory where CMakeLists.txt script is located
as argument (relative paths are supported):
cmake ..

This command will configure and write build script or solution to CMakeBuild
directory. CMake is smart enough to create Unix makefiles under Linux or Visual
Studio solution if you have Visual Studio installed, but you can configure
generator
with -G command line parameter:
cmake .. -G""Unix Makefiles""

The build procedure depends on the selected generator. With ""Unix Makefiles"" you
can type:
make & make install

With ""Visual Studio"" and some other generators you can open solution or project
from CMakeBuild directory and build using IDE.
Finally, you can use unified command:
cmake --build .

CMake also provides Qt-based cross platform GUI, cmake-gui. Using it is trivial
and does not require detailed explanations.
Configuring CMake
You can pass additional options with /D<parameter>=<value> when you run
cmake command. Some useful system options:

CMAKE_C_FLAGS - additional C compiler flags
CMAKE_BUILD_TYPE - configuration type, DEBUG, RELEASE, RELWITHDEBINFO
or MINSIZEREL. DEBUG is default
CMAKE_INSTALL_PREFIX - build install location, the same as --prefix option
of configure script

Useful libsndfile options:

BUILD_SHARED_LIBS - build shared library (DLL under Windows) when ON,
build static library othervise. This option is ON by default.
BUILD_PROGRAMS - build libsndfile's utilities from programs/ directory,
ON by default.
BUILD_EXAMPLES - build examples, ON by default.
BUILD_TESTING - build tests. Then you can run tests with ctest command,
ON by default. Setting BUILD_SHARED_LIBS to ON disables this option.
ENABLE_EXTERNAL_LIBS - enable Ogg, Vorbis and FLAC support. This option is
available and set to ON if all dependency libraries were found.
ENABLE_CPU_CLIP - enable tricky cpu specific clipper. Enabled and set to
ON when CPU clips negative\positive. Don't touch it if you are not sure
ENABLE_BOW_DOCS - enable black-on-white documentation theme, OFF by
default.
ENABLE_EXPERIMENTAL - enable experimental code. Don't use it if you are
not sure. This option is OFF by default.
ENABLE_CPACK - enable CPack support.
This option is ON by default.
ENABLE_PACKAGE_CONFIG - generate and install package config file.
This option is ON by default.
ENABLE_STATIC_RUNTIME - enable static runtime on Windows platform, OFF by
default.
ENABLE_COMPATIBLE_LIBSNDFILE_NAME - set DLL name to libsndfile-1.dll
(canonical name) on Windows platform, sndfile.dll otherwise, OFF by
default. Library name can be different depending on platform. The well known
DLL name on Windows platform is libsndfile-1.dll, because the only way to
build Windows library before was MinGW toolchain with Autotools. This name
is native for MinGW ecosystem, Autotools constructs it using MinGW platform
rules from sndfile target. But when you build with CMake using native
Windows compiler, the name is sndfile.dll. This is name for native Windows
platform, because Windows has no library naming rules. It is preffered
because you can search library using package manager or CMake's
find_library command on any platform using the same sndfile name.

Deprecated options:

DISABLE_EXTERNAL_LIBS - disable Ogg, Vorbis and FLAC support. Replaced by
ENABLE_EXTERNAL_LIBS
DISABLE_CPU_CLIP - disable tricky cpu specific clipper. Replaced by
ENABLE_CPU_CLIP
BUILD_STATIC_LIBS - build static library. Use BUILD_SHARED_LIBS instead

Linking from CMake projects
When libsndfile built and installed with ENABLE_PACKAGE_CONFIG option set
to ON, you can find library from your CMakeLists.txt with this command:
find_package(SndFile)

SndFile_FOUND is set to ON when library is found.
If libsndfile dependency is critical, you can add REQUIRED to
find_package:
find_package(SndFile REQUIRED)

With with option find_package will terminate configuration process
if libsndfile is not found.
You can also add version check:
find_package(SndFile 1.0.29)

find_package will report error, if libsndfile version is < 1.0.29.
You can combine REQUIRED and version if you need.
To link libsndfile library use:
target_link_libraries(my_application PRIVATE SndFile::sndfile)

Notes for Windows users
First advice - set ENABLE_STATIC_RUNTIME to ON. This will remove dependencies
on runtime DLLs.
Second advice is about Ogg, Vorbis and FLAC support. Searching external
libraries under Windows is a little bit tricky. The best way is to use
Vcpkg. You need to install static libogg,
libvorbis and libflac libraries:
vcpkg install libogg:x64-windows-static libvorbis:x64-windows-static
libflac:x64-windows-static libogg:x86-windows-static
libvorbis:x86-windows-static libflac:x86-windows-static

Then and add this parameter to cmake command line:
-DCMAKE_TOOLCHAIN_FILE=<path-to-vcpkg>/scripts/buildsystems/vcpkg.cmake

You also need to set VCPKG_TARGET_TRIPLET because you use static libraries:
-DVCPKG_TARGET_TRIPLET=x64-windows-static

Submitting Patches
See CONTRIBUTING.md for details.
",batch2,8:10:17,Done
358,edquist/autopyfactory,,batch2,8:09:16,Done
359,ideacatalyst/Huginn,"

What is Huginn?
Huginn is a system for building agents that perform automated tasks for you online.  They can read the web, watch for events, and take actions on your behalf.  Huginn's Agents create and consume events, propagating them along a directed graph.  Think of it as a hackable version of IFTTT or Zapier on your own server.  You always know who has your data.  You do.

Here are some of the things that you can do with Huginn:

Track the weather and get an email when it's going to rain (or snow) tomorrow (""Don't forget your umbrella!"")
List terms that you care about and receive emails when their occurrence on Twitter changes.  (For example, want to know when something interesting has happened in the world of Machine Learning?  Huginn will watch the term ""machine learning"" on Twitter and tell you when there is a spike in discussion.)
Watch for air travel or shopping deals
Follow your project names on Twitter and get updates when people mention them
Scrape websites and receive emails when they change
Connect to Adioso, HipChat, Basecamp, Growl, FTP, IMAP, Jabber, JIRA, MQTT, nextbus, Pushbullet, Pushover, RSS, Bash, Slack, StubHub, translation APIs, Twilio, Twitter, Wunderground, and Weibo, to name a few.
Send digest emails with things that you care about at specific times during the day
Track counts of high frequency events and send an SMS within moments when they spike, such as the term ""san francisco emergency""
Send and receive WebHooks
Run custom JavaScript or CoffeeScript functions
Track your location over time
Create Amazon Mechanical Turk workflows as the inputs, or outputs, of agents (the Amazon Turk Agent is called the ""HumanTaskAgent""). For example: ""Once a day, ask 5 people for a funny cat photo; send the results to 5 more people to be rated; send the top-rated photo to 5 people for a funny caption; send to 5 final people to rate for funniest caption; finally, post the best captioned photo on my blog.""

 
Join us in our Gitter room to discuss the project.
Join us!
Want to help with Huginn?  All contributions are encouraged!  You could make UI improvements, add new Agents, write documentation and tutorials, or try tackling issues tagged with #help-wanted.  Please fork, add specs, and send pull requests!
Really want a fix or feature? Want to solve some community issues and earn some extra coffee money? Take a look at the current bounties on Bountysource.
Have an awesome idea but not feeling quite up to contributing yet? Head over to our Official 'suggest an agent' thread  and tell us!
Examples
Please checkout the Huginn Introductory Screencast!
And now, some example screenshots.  Below them are instructions to get you started.





Getting Started
Docker
The quickest and easiest way to check out Huginn is to use the official Docker image. Have a look at the documentation.
Local Installation
If you just want to play around, you can simply fork this repository, then perform the following steps:

Run git remote add upstream https://github.com/huginn/huginn.git to add the main repository as a remote for your fork.
Copy .env.example to .env (cp .env.example .env) and edit .env, at least updating the APP_SECRET_TOKEN variable.
Make sure that you have MySQL or PostgreSQL installed. (On a Mac, the easiest way is with Homebrew. If you're going to use PostgreSQL, you'll need to prepend all commands below with DATABASE_ADAPTER=postgresql.)
Run bundle to install dependencies
Run bundle exec rake db:create, bundle exec rake db:migrate, and then bundle exec rake db:seed to create a development database with some example Agents.
Run bundle exec foreman start, visit http://localhost:3000/, and login with the username of admin and the password of password.
Setup some Agents!
Read the wiki for usage examples and to get started making new Agents.
Periodically run git fetch upstream and then git checkout master && git merge upstream/master to merge in the newest version of Huginn.

Note: By default, emails are intercepted in the development Rails environment, which is what you just setup.  You can view
them at http://localhost:3000/letter_opener. If you'd like to send real emails via SMTP when playing
with Huginn locally, set SEND_EMAIL_IN_DEVELOPMENT to true in your .env file.
If you need more detailed instructions, see the Novice setup guide.
Develop
All agents have specs! And there's also acceptance tests that simulate running Huginn in a headless browser.

Install PhantomJS 2.1.1 or greater:

Using Node Package Manager: npm install phantomjs
Using Homebrew on OSX brew install phantomjs


Run all specs with bundle exec rspec
Run a specific spec with bundle exec rspec path/to/specific/test_spec.rb.
Read more about rspec for rails here.

Using Huginn Agent gems
Huginn Agents can now be written as external gems and be added to your Huginn installation with the ADDITIONAL_GEMS environment variable. See the Additional Agent gems section of .env.example for more information.
If you'd like to write your own Huginn Agent Gem, please see huginn_agent.
Our general intention is to encourage complex and specific Agents to be written as Gems, while continuing to add new general-purpose Agents to the core Huginn repository.
Deployment
Heroku
Try Huginn on Heroku:  (Takes a few minutes to setup. Read the documentation while you are waiting and be sure to click 'View it' after launch!)
Huginn launches on the free version of Heroku with significant limitations. For non-experimental use, we strongly recommend Heroku's 1GB paid plan or our Docker container.
Please see the Huginn Wiki for detailed deployment strategies for different providers.
Manual installation on any server
Have a look at the installation guide.
Optional Setup
Setup for private development
See private development instructions on the wiki.
Enable the WeatherAgent
In order to use the WeatherAgent you need an API key with Wunderground. Signup for one and then change the value of api_key: your-key in your seeded WeatherAgent.
Disable SSL
We assume your deployment will run over SSL. This is a very good idea! However, if you wish to turn this off, you'll probably need to edit config/initializers/devise.rb and modify the line containing config.rememberable_options = { :secure => true }.  You will also need to edit config/environments/production.rb and modify the value of config.force_ssl.
License
Huginn is provided under the MIT License.
Huginn was originally created by @cantino in 2013. Since then, many people's dedicated contributions have made it what it is today.
   
",batch2,8:10:18,Done
360,KnothHe/redis-5.0,"Redis-5.0 Source Code
Note
Original README.md is README-origin.md
What is this repository for

This repository just saved Redis 5.0 release source code

How to get Redis 5.0 source code

You can just clone from offical repository use following command

git clone https://github.com/antirez/redis.git --branch 5.0 --single-branch

Surely, you can just clone this repository

Tools that can be used to read source code

Source Trail

This is what I used to read Redis 5.0 source code
Steps

You need to generate a compile_commands.json file. The tool bear may help. Then go to the project root directory, run following command.

bear make

Use sourcetrail Create a new project and Select C/C++ from compilation Database as Source Group and select previous compile_commands.json.


CLion

Amazing!
",batch2,8:09:15,Done
361,lqwdev/cs188-emacs,,batch1,16:59:00,Done
362,zsiki/GeoEasy,"GeoEasy
surveying calculation, network adjustment, digital terrain models, regression calculation
Volunteers are wellcome! Send us bug reports, feature requests through the
issue tracker.  Clone the repository, change the code and send us back a pull
request to include your enchacement in the core system.
History
The beginning of GeoEasy goes back to the late nineties (1997). Before the 3.0
version it was a proprietary software marketed in Hungary. After twenty
year long development (with active and less active periods) in 2017 the license
has been changed to open source.
Documentation
See the doc folder for various reStructuredText files, the wiki and a paper in Geoinformatics FCE CTU.
Presentation at FOSS4G 2019 Bucharest and video
Developer documentation
OpenHub page
Installation
Installation binaries for latest stabil version (3.1.3):

Debian/Ubuntu
Windows
Windows portable (zip)

Users can select source or beta binary or older releases. Linux and Windows operating
systems are supported. See the installation guide.
Localization
GeoEasy available in different languages. You can localize GeoEasy to your language, see wiki page



Language
Translator(s)




Czech
Bc. Tomáš Bouček, Bc. Jana Špererová


Russian
звездочёт (zvezdochiot)


German
Gergely Szabó, Csaba Égető


Hungarian
Zoltán Siki


English
Andras Gabriel, Zoltán Siki


Spanish
Andres Herrera



Open source software/packages used

Tcl/Tk (https://www.tcl.tk/) the programming language for the project
GNU Gama (https://www.gnu.org/software/gama/) for network adjustment
Triangle (https://github.com/MrPhil/Triangle) for DTM
Proj cs2cs (http://proj.org/) for KML and GPX export
Nullsoft Scriptable Install System (http://nsis.sourceforge.net/Main\_Page) to create the Windows installer
Freewrap (http://freewrap.sourceforge.net/) to convert Tcl/Tk scripts to Linux/Windows executables
bash-deb-build (https://github.com/BASH-Auto-Tools/bash-deb-build) to build debian package
rst2pdf (https://rst2pdf.org) to generate pdf docs from rst
pngquant (https://pngquant.org) to reduce the size of png images in the docs
TclDoc (https://wiki.tcl-lang.org/page/tcldoc) to generate developer documentation from sources

Download statistics of binary releases
Last 12 months

Yearly

",batch2,8:09:16,Done
363,rfsfrn/emacs-mac-borderless,"#Emacs-Mac Borderless Mod
Emacs-Mac without a title bar.
If you use my homebrew formula, it has support for changing the icon to the spacemacs icon with --with-spacemacs-icon
brew install https://raw.githubusercontent.com/Gwydir8/emacs-mac-borderless/master/emacs-mac-borderless.rb
Screenshot

",batch1,16:59:01,Done
364,eugene-matvejev/ultimate-commit-machine,"ULTIMATE COMMIT MACHINE REPOSITORY
aka 1kkk repository
how to use
$ ./ultimate-commit-machine.sh
purposes of repository

I am very curious, if there is any list top commiters of the day, in github](https://github.com)?
I discovered some limitations of github

10k commits preview per pull request preview [28.02.2017]
seems like you can't push more than 500k commits per single push [28.02.2017]


find out if possible to reproduce SHA1-attack

",batch1,16:57:57,Done
365,gemian/connman,,batch2,8:09:15,Done
366,keyteki/keyteki,"Keyteki
Web based implementation of Keyforge: the Unique Deck Game
FAQ
What is it?
This is the respository for the code internally known as keyteki which is running on thecrucible.online allowing people to play KeyForge online using only their browser
Does't this look a lot like Jinteki/Throneteki? The Android netrunner/AGOT online experience?
Glad you noticed! Yes, jinteki was a huge inspiration for this project, as the interface is clean and user friendly, so I've tried to make this similar in a lot of ways
Keyteki is a fork of the ringteki sourcecode
Can I contribute?
Sure! The code is written in node.js(server) and react.js(client). Feel free to make suggestions, implement new cards, refactor bits of the code that are a bit clunky(there's a few of those atm), raise pull requests or submit bug reports
If you are going to contribute code, try and follow the style of the existing code as much as possible and talk to me before engaging in any big refactors. Also bear in mind there is an .eslintrc file in the project so try to follow those rules.
Documentation for implementing cards
There is also a list of events raised by the code here. If you're writing abilities which listen for these events, it tells you what parameters the event has and whether it has a handler. If you're writing code which calls any of these events, please make sure you pass the same parameters.
The biggest help at the moment would be in terms of CSS, as that's a bit of a weakness of mine, feel free to pick up any of the issues tagged 'CSS' in the issue list.
If you're not coding inclined, then just playing games on the site, and reporting bugs and issues that you find is a big help
X Y Z doesn't work
That's not a question, but that still sucks, sorry :( First, bear in mind the site is in its infancy so a lot of things aren't implemented yet, but you should be able to do most things with a bit of manual input. If there's anything you can't do that you need to be able to do, let me know by raising an issue.
See this document for features I have planned and a link to the currently implemented cards:
How do I do X Y Z?
Check out the About page of Keyteki live deployment.
Development
Docker
If you have docker installed, you can use the containerised version of the site.
Clone the repository, then run the following commands:
git submodule init
git submodule update
npm install
docker-compose up

In another terminal, run the following command:
docker-compose exec lobby node server/scripts/fetchdata

Non Docker
Required Software

Git
Node.js 8
PostgreSQL
Redis

Clone the repository, then run the following commands:
git submodule init
git submodule update
npm install
mkdir server/logs

Create config/local.json5 and put the following in it:
{
    dbHost: 'localhost',
    mqHost: 'localhost',

    lobby: {
        port: 4000
    },

    gameNode: {
        hostname: 'localhost'
    }
}

Run the following commands:
node server/scripts/fetchdata.js
node .
node server/gamenode

There are two exectuable components and you'll need to configure/run both to run a local server. First is the lobby server and then there are game nodes. The default configurations assume you are running postgres locally on the default port. If you need to change any configurations, edit config/default.json5 or create a config/local.json5 configuration that overrides any desired settings.
To download all supported languages (not needed if you're running just a test / dev server):
node server/scripts/fetchdata.js --language=en
node server/scripts/fetchdata.js --language=es
node server/scripts/fetchdata.js --language=de
node server/scripts/fetchdata.js --language=fr
node server/scripts/fetchdata.js --language=it
node server/scripts/fetchdata.js --language=pl
node server/scripts/fetchdata.js --language=pt
node server/scripts/fetchdata.js --language=zh-hans
node server/scripts/fetchdata.js --language=zh-hant

For production:
npm run build-vendor
npm run build
NODE_ENV=production PORT=4000 node .

Then for each game node (typically one per CPU/core):
PORT={port} SERVER={node-name} node server/gamenode

Running and Testing
The game server should be accessible by browsing to localhost:4000.
The docker setup creates a default 'admin' user with the password of 'password'.
You can register 2 or more users, to play against yourself.
They can have fake email addresses.
You can login as both users either from 2 different browsers, or by
using an incognito window.
These users will be normal (non-admin) users. To escalate a user to
the admin role requires manual edits to the database, but that
is not required for testing in-game functionality.
If you implement or make changes to a card, you can use manual mode
to add it to a deck from within a game. Use manual mode, and the command:
/add-card <card name>

Before you run the unit tests, be sure all the necessary dependencies are installed
npm install

Then, to run the tests:
npm test

Coding Guidelines
All JavaScript code included in Keyteki should pass (no errors, no warnings)
linting by ESLint, according to the rules defined in
.eslintrc at the root of this repo. To manually check that that is indeed the
case install ESLint and run
npm run lint

from repository's root.
All tests should also pass. To run these manually do:
npm test

If you are making any game engine changes, these will not be accepted without unit tests to cover them.
Discord Discusson
Keyteki Discord Server
",batch2,8:09:15,Done
367,tsuu32/emacs-w32con-vt,"Emacs with 256/true color support in Windows Console
News

2020/09/26: Update the patch for Emacs 27.1

Background

Current GNU Emacs in Windows Console (emacs -nw at cmd.exe) supports only 16 colors.
Windows Console recently supports true color by Virtual Terminal sequences.
Windows Terminal also supports Virtual Terminal sequences.
Vim already supports truecolor in Windows Console. (See https://github.com/vim/vim/commit/cafafb381a04e33f3ce9cd15dd9f94b73226831f)

Build
Note: This is experimental.
Current implementation is based on Emacs 27.1.
Prerequisites:

Windows 10
MSYS2 and MINGW-w64
Dependancies are installed (See https://github.com/tsuu32/emacs-w32con-vt/blob/master/nt/INSTALL.W64)

Steps:

Clone this repo and ./autogen.sh to generate configure script in MSYS2 bash:

git clone https://github.com/tsuu32/emacs-w32con-vt.git
cd emacs-w32con-vt
./autogen.sh

Run configure with --with-w32-vt-color=NUMBER option:

./configure --without-dbus --with-w32-vt-color=24bit
You can set 16 or 256, 24bit for NUMBER.

Run Make:

make
Usage
Run Emacs in cmd.exe or powershell.exe:
rem for cmd.exe
set PATH=C:\msys64\mingw64\bin;%PATH%
path\to\emacs-w32con-vt\src\emacs.exe -nw
You can also run via Windows Terminal.
Screenshot
With zenburn theme.

Bug

Starting emacs with no theme cause buggy at 24bit color.

Future work

 Autoconf support.
 Use escape sequences more (not only color).

",batch1,16:58:59,Done
368,AjayBrahmakshatriya/llvm-bb-emitter,,batch1,16:57:59,Done
369,vechain/thor,"VeChain Thor
A general purpose blockchain highly compatible with Ethereum's ecosystem.
This is the first implementation written in golang.




   
Table of contents

Installation

Requirements
Getting the source
Dependency management
Building


Running Thor

Sub-commands


Docker
Explorers
Faucet
RESTful API
Acknowledgement
Contributing

Installation
Requirements
Thor requires Go 1.13+ and C compiler to build. To install Go, follow this link.
Getting the source
Clone the Thor repo:
git clone https://github.com/vechain/thor.git
cd thor
Dependency management
Simply run:
make dep
If you keep getting network errors, it is suggested to use Go Module Proxy. https://proxy.golang.org/ is one option.
Building
To build the main app thor, just run
make
or build the full suite:
make all
If no errors are reported, all built executable binaries will appear in folder bin.
Running Thor
Connect to VeChain's mainnet:
bin/thor --network main
Connect to VeChain's testnet:
bin/thor --network test
or startup a custom network
bin/thor --network <custom-net-genesis.json>
An example genesis config file can be found at genesis/example.json.
To show usages of all command line options:
bin/thor -h

--network value             the network to join (main|test) or path to genesis file
--data-dir value            directory for block-chain databases
--cache value               megabytes of ram allocated to internal caching (default: 2048)
--beneficiary value         address for block rewards
--target-gas-limit value    target block gas limit (adaptive if set to 0) (default: 0)
--api-addr value            API service listening address (default: ""localhost:8669"")
--api-cors value            comma separated list of domains from which to accept cross origin requests to API
--api-timeout value         API request timeout value in milliseconds (default: 10000)
--api-call-gas-limit value  limit contract call gas (default: 50000000)
--api-backtrace-limit value limit the distance between 'position' and best block for subscriptions APIs (default: 1000)
--verbosity value           log verbosity (0-9) (default: 3)
--max-peers value           maximum number of P2P network peers (P2P network disabled if set to 0) (default: 25)
--p2p-port value            P2P network listening port (default: 11235)
--nat value                 port mapping mechanism (any|none|upnp|pmp|extip:<IP>) (default: ""none"")
--bootnode value            comma separated list of bootnode IDs
--skip-logs                 skip writing event|transfer logs (/logs API will be disabled)
--pprof                     turn on go-pprof
--disable-pruner            disable state pruner to keep all history
--help, -h                  show help
--version, -v               print the version

Sub-commands

solo                client runs in solo mode for test & dev

# create new block when there is pending transaction
bin/thor solo --on-demand

# save blockchain data to disk(default to memory)
bin/thor solo --persist

# two options can work together
bin/thor solo --persist --on-demand

master-key          master key management

# print the master address
bin/thor master-key

# export master key to keystore
bin/thor master-key --export > keystore.json


# import master key from keystore
cat keystore.json | bin/thor master-key --import
Docker
Docker is one quick way for running a vechain node:
docker run -d\
  -v {path-to-your-data-directory}/.org.vechain.thor:/root/.org.vechain.thor\
  -p 127.0.0.1:8669:8669 -p 11235:11235 -p 11235:11235/udp\
  --name thor-node vechain/thor --network test
Do not forget to add the --api-addr 0.0.0.0:8669 flag if you want other containers and/or hosts to have access to the RESTful API. Thor binds to localhost by default and it will not accept requests outside the container itself without the flag.
The Dockerfile is designed to build the last release of the source code and will publish docker images to dockerhub by release, feel free to fork and build Dockerfile for your own purpose.
Explorers

VeChain Explorer (Official)
VeChainStats
VechainThorScan
Vexplorer
Insight

Testnet faucet

faucet.vecha.in by VeChain Foundation

API
Once thor has started, the online OpenAPI doc can be accessed in your browser. e.g. http://localhost:8669/ by default.

Acknowledgement
A special shout out to following projects:

Ethereum
Swagger

Contributing
Thank you so much for considering to help out with the source code! We welcome contributions from anyone on the internet, and are grateful for even the smallest of fixes!
Please fork, fix, commit and send a pull request for the maintainers to review and merge into the main code base.
Forking Thor
When you ""Fork"" the project, GitHub will make a copy of the project that is entirely yours; it lives in your namespace, and you can push to it.
Getting ready for a pull request
Please check the following:

Code must be adhere to the official Go Formatting guidelines.
Get the branch up to date, by merging in any recent changes from the master branch.

Making the pull request

On the GitHub site, go to ""Code"". Then click the green ""Compare and Review"" button. Your branch is probably in the ""Example Comparisons"" list, so click on it. If not, select it for the ""compare"" branch.
Make sure you are comparing your new branch to master. It probably won't be, since the front page is the latest release branch, rather than master now. So click the base branch and change it to master.
Press Create Pull Request button.
Provide a brief title.
Explain the major changes you are asking to be code reviewed. Often it is useful to open a second tab in your browser where you can look through the diff yourself to remind yourself of all the changes you have made.

License
VeChain Thor is licensed under the
GNU Lesser General Public License v3.0, also included
in LICENSE file in repository.
",batch2,8:10:17,Done
