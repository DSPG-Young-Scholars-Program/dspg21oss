{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:30px\" align=\"center\"> <b> Scaping GitHub READMEs </b> </div>\n",
    "\n",
    "<div style=\"font-size:18px\" align=\"center\"> <b> Brandon Kramer, UVA Biocomplexity Institute, OSS DSPG 2021 </b> </div>\n",
    "\n",
    "<br>\n",
    "\n",
    "### Overview  \n",
    "\n",
    "In this notebook, we have developed a function for scraping GitHub READMEs in order to classify repositories into different types of software projects. \n",
    "\n",
    "The pipeline is setup with the following steps: \n",
    "\n",
    "1. Loading all of the packages \n",
    "\n",
    "2. Calling a function that scrapes the READMEs \n",
    "\n",
    "3. Loading the repositories from PostgreSQL as a DataFrame \n",
    "\n",
    "4. Cross-referencing the repos against the scraped data \n",
    "\n",
    "5. Scraping the repos using multiprocessing \n",
    "\n",
    "6. Checking the data that was scraped \n",
    "\n",
    "### Load Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready\n"
     ]
    }
   ],
   "source": [
    "# for pulling/manipulating data \n",
    "import os \n",
    "import glob\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import psycopg2 as pg\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# for web scraping \n",
    "import json \n",
    "import lxml\n",
    "import requests \n",
    "from requests.auth import HTTPBasicAuth\n",
    "from bs4 import BeautifulSoup\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "import multiprocessing\n",
    "from multiprocessing.pool import ThreadPool as Pool\n",
    "from datetime import datetime\n",
    "import sys\n",
    "print(\"ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call Functions\n",
    "\n",
    "To use the `scrape_readmes()` function, you first need to set the username and personal acccess token (PAT) that you create on GitHub. While you can run it without this information but having no PAT means you can only make 50 calls an hour compared to about 5,000. It helps if you have several PATs to help in this process, especially since this function takes only a few moments to make 5,000 calls after the multiprocessing is incorporated. In this pipeline, we are calling the usernames and PATs from a table in PostgreSQL that looks like this: \n",
    "\n",
    "|    login    |   token  | \n",
    "|    :---:    |  :----:  |     \n",
    "|  username1  |   PAT1   | \n",
    "|  username2  |   PAT2   | \n",
    "|  etc.  |   etc.   | \n",
    "\n",
    "Once the username and PAT are passed into the authetication fields, `requests` will connect to the GitHub repository of all the slugs you feed it. We have designed the function to throw errors for all issues it encounters unless it gets a 404 error (i.e. no README available), as this usually means that the repo or README has been deleted, never existed, and/or is no longer available for some reason. Unfortunately, the way that GitHub's API is setup seems to require two calls to get the README: one to get the JSON with the README location and another to decode the content. We have a `@sleep_and_retry` decorator to deal with this if we setup a `slurm` for each PAT, but using it here in the notebook means that you will just want to wait for the threshold to be hit and then move onto the next PAT. While we plan to continue working on this function to minimize the number of calls, this process will allow us to get some preliminary data for classification in the short-term. \n",
    "\n",
    "At the end of this chunk, you will also see the `filter_scraped_readmes()` function that filters scraped READMEs. Basically, you just feed this function your original data and then it filters out slugs that have already been scraped based on the local CSV that has that information. \n",
    "\n",
    "**NOTE: Before running this cell, you need to set the** `github_pat_index`. **Changing this parameter provides access to 36 different PATs.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = pg.connect(host = 'postgis1', database = 'sdad', \n",
    "                        user = os.environ.get('db_user'), \n",
    "                        password = os.environ.get('db_pwd'))\n",
    "github_pats = '''SELECT * FROM gh_2007_2020.pats_update'''\n",
    "github_pats = pd.read_sql_query(github_pats, con=connection)\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started scraping\n",
      "404 error on cjbd/src\n",
      "Finished scraping\n"
     ]
    }
   ],
   "source": [
    "# set this parameter to a number between 0 and 35  \n",
    "github_pat_index = 24\n",
    "\n",
    "# can only make 2500 calls per hour \n",
    "# because the function calls twice each time \n",
    "@sleep_and_retry\n",
    "@limits(calls=2500, period=3600)\n",
    "def scrape_readmes(slug):\n",
    "    \n",
    "    github_username = github_pats.login[github_pat_index]\n",
    "    github_token = github_pats.token[github_pat_index]\n",
    "    \n",
    "    while True:\n",
    "        try: \n",
    "            # define url based on the slug \n",
    "            url = f'https://api.github.com/repos/{slug}/readme'\n",
    "            response = requests.get(url, auth=(github_username, github_token))\n",
    "            response_code = response.status_code\n",
    "            \n",
    "            if response_code == 404: \n",
    "                print(f\"404 error on {slug}\")\n",
    "                readme_string = \"404 ERROR - NO README\"\n",
    "                now = datetime.now()\n",
    "                current_time = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                return slug, readme_string, current_time, \"Done\"\n",
    "            \n",
    "        except KeyError:\n",
    "            print(\"Key error for: \" + slug, flush=True)\n",
    "            break\n",
    "        \n",
    "        except requests.exceptions.HTTPError as http_error:\n",
    "            print (\"HTTP Error:\", http_error)\n",
    "            raise SystemExit(http_error)\n",
    "            break\n",
    "            \n",
    "        except requests.exceptions.ConnectionError as connection_error:\n",
    "            print (\"Error Connecting:\", connection_error)\n",
    "            raise SystemExit(connection_error)\n",
    "            break \n",
    "        \n",
    "        except requests.exceptions.TooManyRedirects as toomany_requests:\n",
    "            print (\"Too Many Requests:\", toomany_requests)\n",
    "            raise SystemExit(toomany_requests)\n",
    "            break\n",
    "                \n",
    "        except requests.exceptions.Timeout as timeout_error:\n",
    "            print (\"Timeout Error:\", timeout_error)\n",
    "            raise SystemExit(timeout_error)\n",
    "            break\n",
    "        \n",
    "        except requests.exceptions.RequestException as request_exception_error:\n",
    "            print (\"Oops, Some Other Error:\", request_exception_error)\n",
    "            raise SystemExit(request_exception_error)\n",
    "            break \n",
    "            \n",
    "        html_content = response.content\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        site_json=json.loads(soup.text)\n",
    "        readme_link = site_json['download_url']\n",
    "        \n",
    "        #try:\n",
    "        #    readme_link = site_json['download_url']\n",
    "        \n",
    "        #except KeyError as error_403:\n",
    "        #    now = datetime.now()\n",
    "        #    print(f\"Rate limited exceeded (403 error) on {slug} at\", \n",
    "        #          now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "            #github_pat_index = github_pat_index - 1\n",
    "        #    raise SystemExit(error_403) \n",
    "        #    break \n",
    "            \n",
    "        while True:\n",
    "            try: \n",
    "                readme_response = requests.get(readme_link, auth=(github_username, github_token))\n",
    "                readme_response_code = readme_response.status_code\n",
    "                \n",
    "            except requests.exceptions.HTTPError as http_error:\n",
    "                print (\"HTTP Error:\", http_error)\n",
    "                raise SystemExit(http_error)\n",
    "                break\n",
    "            \n",
    "            except requests.exceptions.ConnectionError as connection_error:\n",
    "                print (\"Error Connecting:\", connection_error)\n",
    "                raise SystemExit(connection_error)\n",
    "                break\n",
    "            \n",
    "            except requests.exceptions.TooManyRedirects as toomany_requests:\n",
    "                print (\"Too Many Requests:\", toomany_requests)\n",
    "                raise SystemExit(toomany_requests)\n",
    "                break \n",
    "        \n",
    "            except requests.exceptions.Timeout as timeout_error:\n",
    "                print (\"Timeout Error:\", timeout_error)\n",
    "                raise SystemExit(timeout_error)\n",
    "                break\n",
    "        \n",
    "            except requests.exceptions.RequestException as request_exception_error:\n",
    "                print (\"Oops, Some Other Error:\", request_exception_error)\n",
    "                raise SystemExit(request_exception_error)\n",
    "                break  \n",
    "    \n",
    "            # pull the content out of the readme \n",
    "            readme_content = readme_response.content\n",
    "            readme_soup = BeautifulSoup(readme_content, 'html.parser')\n",
    "            readme_string = str(readme_soup)\n",
    "    \n",
    "            #give us the the timing and status \n",
    "            now = datetime.now()\n",
    "            current_time = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            #print(readme_string)\n",
    "            return slug, readme_string, current_time, \"Done\"\n",
    "\n",
    "def filter_scraped_readmes(original_data): \n",
    "    ''' \n",
    "    Function ingests repos data and filters out already scraped data from local csv \n",
    "    '''\n",
    "    \n",
    "    # ingests local csv data and converts it to a list \n",
    "    #os.chdir('/project/class/bii_sdad_dspg/ncses_oss_2021/requests_scrape/')\n",
    "    os.chdir('/project/biocomplexity/sdad/projects_data/ncses/oss/dspg_2021/')\n",
    "    all_filenames = [i for i in glob.glob('*.csv')]\n",
    "    combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
    "    combined_csv = combined_csv[combined_csv['status'] == 'Done']\n",
    "    scraped_slugs_list = combined_csv['slug'].tolist()\n",
    "    \n",
    "    # filters out all of the scraped slugs from the original_data \n",
    "    filtered_slugs = ~raw_slug_data.slug.isin(scraped_slugs_list)\n",
    "    filtered_slugs = raw_slug_data[filtered_slugs]\n",
    "    \n",
    "    # provides the output of current slug count and number of slugs filtered \n",
    "    new_slug_count = filtered_slugs['slug'].count()\n",
    "    slug_count_diff = raw_slug_data['slug'].count() - filtered_slugs['slug'].count()\n",
    "    print(\"Current data has\", new_slug_count, \"entries left to scrape (filtered\", slug_count_diff, \"from input data)\")\n",
    "    return filtered_slugs\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Started scraping\")\n",
    "    scrape_readmes(slug = 'cjbd/src')\n",
    "    print(\"Finished scraping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingesting the Repo Slugs from the Database \n",
    "\n",
    "Here, we ingested the repository data from PostgreSQL. To pull from different subsets of the data, you can change the range of the commits clause in the `SQL` code. Even if you pull from the same range as someone else already has, the next step of the pipeline is cross-referencing which READMEs have already been scraped and then removing from slugs from the dataset. I have also added a clause to remove all of the repos with an 'Init' status, as their commits data have not yet been scraped and they seem to missing in some systematic way. For now, we are just ignoring them to deal with the majority of valid repos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>spdx</th>\n",
       "      <th>slug</th>\n",
       "      <th>createdat</th>\n",
       "      <th>description</th>\n",
       "      <th>primarylanguage</th>\n",
       "      <th>branch</th>\n",
       "      <th>commits</th>\n",
       "      <th>asof</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MDEwOlJlcG9zaXRvcnkyNzUwNzAwMzc=</td>\n",
       "      <td>BSD-3-Clause</td>\n",
       "      <td>dbuskariol-org/chromium</td>\n",
       "      <td>2020-06-26 04:04:29</td>\n",
       "      <td>The official GitHub mirror of the Chromium source</td>\n",
       "      <td>None</td>\n",
       "      <td>MDM6UmVmMjc1MDcwMDM3OnJlZnMvaGVhZHMvbWFzdGVy</td>\n",
       "      <td>849191</td>\n",
       "      <td>2021-01-03 16:55:57</td>\n",
       "      <td>Init</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MDEwOlJlcG9zaXRvcnkxOTM0MzEyNTI=</td>\n",
       "      <td>BSD-3-Clause</td>\n",
       "      <td>cjbd/src</td>\n",
       "      <td>2019-06-24 04:03:03</td>\n",
       "      <td>src</td>\n",
       "      <td>None</td>\n",
       "      <td>MDM6UmVmMTkzNDMxMjUyOnJlZnMvaGVhZHMvbWFzdGVy</td>\n",
       "      <td>795211</td>\n",
       "      <td>2021-01-03 22:57:50</td>\n",
       "      <td>Init</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MDEwOlJlcG9zaXRvcnkyODUxOTgyOTQ=</td>\n",
       "      <td>GPL-2.0</td>\n",
       "      <td>firemax13/android_kernel_sm6150_unified</td>\n",
       "      <td>2020-08-05 06:17:00</td>\n",
       "      <td>Samsung Galaxy A71 &amp; A80 Unified Kernel Source...</td>\n",
       "      <td>C</td>\n",
       "      <td>MDM6UmVmMjg1MTk4Mjk0OnJlZnMvaGVhZHMvYnRmNy1maXJl</td>\n",
       "      <td>745131</td>\n",
       "      <td>2021-01-03 19:04:27</td>\n",
       "      <td>Init</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MDEwOlJlcG9zaXRvcnkzMDU2MTkzMjA=</td>\n",
       "      <td>GPL-2.0</td>\n",
       "      <td>firemax13/a80kernel</td>\n",
       "      <td>2020-10-20 07:04:11</td>\n",
       "      <td>FireKernel Custom Extreme Kernel For Galaxy A80</td>\n",
       "      <td>C</td>\n",
       "      <td>MDM6UmVmMzA1NjE5MzIwOnJlZnMvaGVhZHMvbWFpbg==</td>\n",
       "      <td>745131</td>\n",
       "      <td>2021-01-03 23:51:13</td>\n",
       "      <td>Init</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MDEwOlJlcG9zaXRvcnk4Mjk0MDUzOA==</td>\n",
       "      <td>MIT</td>\n",
       "      <td>eugene-matvejev/ultimate-commit-machine</td>\n",
       "      <td>2017-02-23 15:24:09</td>\n",
       "      <td>explore github.com limits and \"same-hash\" attack</td>\n",
       "      <td>Shell</td>\n",
       "      <td>MDM6UmVmODI5NDA1Mzg6cmVmcy9oZWFkcy9tYXN0ZXI=</td>\n",
       "      <td>716089</td>\n",
       "      <td>2021-01-03 22:19:35</td>\n",
       "      <td>Init</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id          spdx  \\\n",
       "0  MDEwOlJlcG9zaXRvcnkyNzUwNzAwMzc=  BSD-3-Clause   \n",
       "1  MDEwOlJlcG9zaXRvcnkxOTM0MzEyNTI=  BSD-3-Clause   \n",
       "2  MDEwOlJlcG9zaXRvcnkyODUxOTgyOTQ=       GPL-2.0   \n",
       "3  MDEwOlJlcG9zaXRvcnkzMDU2MTkzMjA=       GPL-2.0   \n",
       "4  MDEwOlJlcG9zaXRvcnk4Mjk0MDUzOA==           MIT   \n",
       "\n",
       "                                      slug           createdat  \\\n",
       "0                  dbuskariol-org/chromium 2020-06-26 04:04:29   \n",
       "1                                 cjbd/src 2019-06-24 04:03:03   \n",
       "2  firemax13/android_kernel_sm6150_unified 2020-08-05 06:17:00   \n",
       "3                      firemax13/a80kernel 2020-10-20 07:04:11   \n",
       "4  eugene-matvejev/ultimate-commit-machine 2017-02-23 15:24:09   \n",
       "\n",
       "                                         description primarylanguage  \\\n",
       "0  The official GitHub mirror of the Chromium source            None   \n",
       "1                                                src            None   \n",
       "2  Samsung Galaxy A71 & A80 Unified Kernel Source...               C   \n",
       "3    FireKernel Custom Extreme Kernel For Galaxy A80               C   \n",
       "4   explore github.com limits and \"same-hash\" attack           Shell   \n",
       "\n",
       "                                             branch  commits  \\\n",
       "0      MDM6UmVmMjc1MDcwMDM3OnJlZnMvaGVhZHMvbWFzdGVy   849191   \n",
       "1      MDM6UmVmMTkzNDMxMjUyOnJlZnMvaGVhZHMvbWFzdGVy   795211   \n",
       "2  MDM6UmVmMjg1MTk4Mjk0OnJlZnMvaGVhZHMvYnRmNy1maXJl   745131   \n",
       "3      MDM6UmVmMzA1NjE5MzIwOnJlZnMvaGVhZHMvbWFpbg==   745131   \n",
       "4      MDM6UmVmODI5NDA1Mzg6cmVmcy9oZWFkcy9tYXN0ZXI=   716089   \n",
       "\n",
       "                 asof status  \n",
       "0 2021-01-03 16:55:57   Init  \n",
       "1 2021-01-03 22:57:50   Init  \n",
       "2 2021-01-03 19:04:27   Init  \n",
       "3 2021-01-03 23:51:13   Init  \n",
       "4 2021-01-03 22:19:35   Init  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connect to the database, download data \n",
    "connection = pg.connect(host = 'postgis1', database = 'sdad', \n",
    "                        user = os.environ.get('db_user'), \n",
    "                        password = os.environ.get('db_pwd'))\n",
    "# commits < 99 AND commits > 95 AND status != 'Init'\n",
    "raw_slug_data = '''SELECT * FROM gh_2007_2020.repos_ranked where commits > 1000 '''\n",
    "\n",
    "# convert to a dataframe, show how many missing we have (none)\n",
    "raw_slug_data = pd.read_sql_query(raw_slug_data, con=connection)\n",
    "raw_slug_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the Counts \n",
    "\n",
    "Before running the function, you want to compare the original table that you just pulled from the database and the local CSV files that are keeping track of the already downloaded data. The first cell below gives you the count for the original table and the next cell pulls in all of the CSVs, concatenates them, and then gives the count of how many more slugs need to be scraped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slug</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <td>ITHIM/ithim-r-interface</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100001</th>\n",
       "      <td>wmde/jahresbericht2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100002</th>\n",
       "      <td>asskek/VARIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100003</th>\n",
       "      <td>konnectors/caf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100004</th>\n",
       "      <td>tuleyman/discord</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           slug\n",
       "100000  ITHIM/ithim-r-interface\n",
       "100001   wmde/jahresbericht2016\n",
       "100002             asskek/VARIS\n",
       "100003           konnectors/caf\n",
       "100004         tuleyman/discord"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('/project/class/bii_sdad_dspg/uva_2021/dspg21oss/')\n",
    "raw_slug_data = pd.read_csv('brandon_to_scrape_0712.csv')\n",
    "raw_slug_data = raw_slug_data.iloc[100000:]\n",
    "raw_slug_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44121"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_slug_data['slug'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current data has 37369 entries left to scrape (filtered 6752 from input data)\n"
     ]
    }
   ],
   "source": [
    "new_slugs = filter_scraped_readmes(original_data=raw_slug_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping the READMEs \n",
    "\n",
    "This chunk of code does a few things. First, it sets the `batch_name` to both keep track of which batch of data we are collecting and to name the output files with the appropriate name. This MUST be done before you run the code chunk. Second, the code sets up multiprocessing to draw from multiple cores. Third, the code converts the DataFrame into a list to feed the slugs into the for loop. Lastly, we feed all the slugs to the function as a for loop and it downloads the data into our project folder. \n",
    "\n",
    "**NOTE: Before running this cell, set the** `batch_name` **variable. Changing this variable will tell us which download batch the data was collected during and then save the CSV with that batch name.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404 error on ctessum/geom\n",
      "404 error on KDAB/perfparser\n",
      "404 error on zeatul/poc\n",
      "404 error on Azure/azure-mobile-apps-js-client\n",
      "404 error on gstoner/gpudb\n",
      "404 error on tttamaki/libdai\n",
      "404 error on irfu/Lapdog_GIT\n",
      "404 error on romanodesouza/dotfiles\n",
      "404 error on Yechengyang/FOD\n",
      "404 error on james5deutschland/nsjail\n",
      "404 error on openSUSE/susefirewall2\n",
      "404 error on CyanogenMod/android_external_libphonenumbergoogle\n",
      "404 error on mqudsi/nvim-config\n",
      "404 error on code-hunger/opengl3-sandbox\n",
      "404 error on BaroboRobotics/BaroboLink2\n",
      "404 error on atefganm/openpli-oe-core\n",
      "404 error on humanfactors/blog\n",
      "404 error on natheon/rustlings\n",
      "404 error on knl/prezto\n",
      "404 error on cilt-uct/uct-quartz\n",
      "404 error on cilt-uct/profilewow\n",
      "404 error on akihiro-terasaki/sakai_master\n",
      "404 error on stevemar/collections-master\n",
      "404 error on tianocore/tianocore.github.io\n",
      "404 error on arizvisa/syringe\n",
      "404 error on logasja/pbrt-v3\n",
      "404 error on rockon9sky/custom-kcptun\n",
      "404 error on aunali1/xtideuniversalbios\n",
      "404 error on GerkinDev/deployer.js-dev\n",
      "404 error on JongHyeonSong/JongHyeonSong.github.io\n",
      "404 error on axlm/clinit\n",
      "404 error on Keruspe/blog\n",
      "404 error on Averrhoes/lfs\n",
      "404 error on HolyBlackCat/unfinished-2\n",
      "404 error on tingfeng-key/lumen-modules404 error on hsuan1117/AIRpollution\n",
      "\n",
      "404 error on top319VIP/LINCOLN\n",
      "404 error on cuclaoliu/git-repo\n",
      "404 error on magnush/mhwaveedit\n",
      "404 error on phanindra1212/gaevfs\n",
      "404 error on tekul/spring-security\n",
      "404 error on lukas-krecan/spring-ws-test\n",
      "404 error on jaceklaskowski/mastering-kafka-streams-book\n",
      "404 error on AmitKumarDas/curated-tech\n",
      "404 error on th-koeln/amcgala\n",
      "404 error on akihiro-terasaki/sakai_stable\n",
      "404 error on joshuapouliot27/Land-Drone\n",
      "404 error on naturing-machine/amandarine\n",
      "404 error on netshowers/NEWO\n",
      "404 error on yandex/mediastorage-proxy\n",
      "404 error on phil65/script.maps.browser\n",
      "404 error on ogiba/FamilyTree\n",
      "404 error on ros2/demos\n",
      "404 error on FIRST-Team-339/2016\n",
      "404 error on InsightLab/data-science-cookbook\n",
      "404 error on ultilix/catawampus\n",
      "404 error on mkurdej/cmaketools\n",
      "404 error on 99degree/minigbm\n",
      "404 error on tekknolagi/tekknolagi.github.com\n",
      "404 error on petrachor/blockscout_node\n",
      "404 error on inkch/surf\n",
      "404 error on plusww/plusww.github.io\n",
      "404 error on ajpmaclean/VTKEx\n",
      "404 error on WhisperChi/owt-client-native-windows\n",
      "404 error on BasnetSagar0/sagar1\n",
      "404 error on google/fest\n",
      "404 error on Natal76588/repo\n",
      "404 error on ryanbhayward/games-puzzles-algorithms\n",
      "404 error on laravel/lumen-docs\n",
      "404 error on simonlynen/yaml-cpp.emitter-refactor\n",
      "404 error on certik/yaml-cpp\n",
      "404 error on bradparks/visual\n",
      "404 error on LineageOS/android_external_libphonenumbergoogle404 error on fzyitt/fzyitt.github.io\n",
      "\n",
      "404 error on LykkeCity/EthereumApiDotNetCore\n",
      "404 error on pulp-platform/gvsoc\n",
      "404 error on streamsets/datacollector-edge\n",
      "404 error on eaciit/hdc\n",
      "404 error on kennyric/namebench\n",
      "404 error on tyward/item\n",
      "404 error on Pasadi/axis2-transports\n",
      "404 error on KDE/ksystemlog\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'download_url'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-f7988c549feb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0masof_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mstatus_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscrape_readmes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslugs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mslug_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mreadme_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/brandon_env/lib/python3.9/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    868\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m     \u001b[0m__next__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m                    \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/brandon_env/lib/python3.9/multiprocessing/pool.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/brandon_env/lib/python3.9/site-packages/ratelimit/decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRateLimitException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_remaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/brandon_env/lib/python3.9/site-packages/ratelimit/decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m                     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-8111913f8d6e>\u001b[0m in \u001b[0;36mscrape_readmes\u001b[0;34m(slug)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml_content\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0msite_json\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mreadme_link\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msite_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'download_url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m#try:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'download_url'"
     ]
    }
   ],
   "source": [
    "# need to change this for each batch or you will save over what you had  \n",
    "batch_name = 'oss_readme_batch_06_34' \n",
    "\n",
    "# sets the number of cores so that it can draw from multiprocessors \n",
    "# there must be 1 core subtracted so that the notebook can run too \n",
    "cores_available = multiprocessing.cpu_count() - 1\n",
    "pool = Pool(cores_available)\n",
    "\n",
    "# convert the dataframe into a list for the subsequent for loop \n",
    "raw_slugs = new_slugs[\"slug\"].tolist()\n",
    "slugs = []\n",
    "for s in raw_slugs:\n",
    "    slugs.append(s.strip())\n",
    "    \n",
    "# now we will feed in all of the remaining slugs \n",
    "slug_log = []\n",
    "readme_log = []\n",
    "asof_log = []\n",
    "status_log = []\n",
    "for result in pool.imap_unordered(scrape_readmes, slugs):\n",
    "    slug_log.append(result[0])\n",
    "    readme_log.append(result[1])\n",
    "    asof_log.append(result[2])\n",
    "    status_log.append(result[3])\n",
    "    final_log = pd.DataFrame({'slug': slug_log, \"readme_text\": readme_log, 'batch': batch_name, 'as_of': asof_log, 'status': status_log}, \n",
    "                              columns=[\"slug\", \"readme_text\", \"batch\", \"as_of\", \"status\"])\n",
    "    #final_log.to_csv('/project/class/bii_sdad_dspg/ncses_oss_2021/requests_scrape/'+batch_name+'.csv', sep=',', encoding='utf-8', index=False)\n",
    "    final_log.to_csv('/project/biocomplexity/sdad/projects_data/ncses/oss/dspg_2021/'+batch_name+'.csv', sep=',', encoding='utf-8', index=False)\n",
    "print(\"Finished scraping\", len(final_log), \"of\", len(slugs), \"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common/Known Errors \n",
    "\n",
    "Here are some common/known errors that I have identified: \n",
    "\n",
    "1. `\"MarkupResemblesLocatorWarning: \"{slug}\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.` \n",
    "\n",
    "    Solution: This will just throw a warning and does not seemt to affect subsequent runs of the dataset. You can ignore this. \n",
    "    \n",
    "2. `KeyError: 'download_url'`\n",
    "\n",
    "    Solution: It seems like this error can mean one of two things. The first is insignificant and \n",
    "\n",
    "### Examining the Dataset \n",
    "\n",
    "This code just pulls in all of the downloaded data as a DataFrame and sees how many entries we have downloaded already. You can use the commented line to output all the data together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#print(sys.getrecursionlimit())\n",
    "sys.setrecursionlimit(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slug</th>\n",
       "      <th>readme_text</th>\n",
       "      <th>batch</th>\n",
       "      <th>as_of</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>mumblepins/debian-samba</td>\n",
       "      <td>This is the release version of Samba, the free...</td>\n",
       "      <td>oss_readme_batch1_1</td>\n",
       "      <td>6/11/21 17:21</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>fdvarela/odoo8</td>\n",
       "      <td>[![Build Status](http://runbot.odoo.com/runbot...</td>\n",
       "      <td>oss_readme_batch1_1</td>\n",
       "      <td>6/11/21 17:21</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>Nkosi-tshawe/moodle</td>\n",
       "      <td>.-..-.\\n   __...</td>\n",
       "      <td>oss_readme_batch1_1</td>\n",
       "      <td>6/11/21 17:21</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>dllsf/odootest</td>\n",
       "      <td>[![Build Status](http://runbot.odoo.com/runbot...</td>\n",
       "      <td>oss_readme_batch1_1</td>\n",
       "      <td>6/11/21 17:21</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>dkavadia/stupefy</td>\n",
       "      <td>.-..-.\\n   __...</td>\n",
       "      <td>oss_readme_batch1_1</td>\n",
       "      <td>6/11/21 17:21</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>zuokun2013/site1</td>\n",
       "      <td># Initial page\\n\\nthis is a hello world page\\n...</td>\n",
       "      <td>oss_readme_batch_05_72</td>\n",
       "      <td>2021-07-12 23:16:10</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>almcalle/gatsby-starter-netlify-cms</td>\n",
       "      <td># Gatsby + Netlify CMS Starter\\n\\n[![Netlify S...</td>\n",
       "      <td>oss_readme_batch_05_72</td>\n",
       "      <td>2021-07-12 23:16:10</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>biwers/aj-test</td>\n",
       "      <td>&lt;!-- AUTO-GENERATED-CONTENT:START (STARTER) --...</td>\n",
       "      <td>oss_readme_batch_05_72</td>\n",
       "      <td>2021-07-12 23:16:10</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>taraldga/mossekarusellen-netlify-cms</td>\n",
       "      <td>404 ERROR - NO README</td>\n",
       "      <td>oss_readme_batch_05_72</td>\n",
       "      <td>2021-07-12 23:16:10</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>EbenezerSqueem/gatsby-starter-netlify-cms</td>\n",
       "      <td># Gatsby + Netlify CMS Starter\\n\\n[![Netlify S...</td>\n",
       "      <td>oss_readme_batch_05_72</td>\n",
       "      <td>2021-07-12 23:16:11</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>444527 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          slug  \\\n",
       "415                    mumblepins/debian-samba   \n",
       "238                             fdvarela/odoo8   \n",
       "237                        Nkosi-tshawe/moodle   \n",
       "236                             dllsf/odootest   \n",
       "235                           dkavadia/stupefy   \n",
       "..                                         ...   \n",
       "535                           zuokun2013/site1   \n",
       "534        almcalle/gatsby-starter-netlify-cms   \n",
       "533                             biwers/aj-test   \n",
       "539       taraldga/mossekarusellen-netlify-cms   \n",
       "550  EbenezerSqueem/gatsby-starter-netlify-cms   \n",
       "\n",
       "                                           readme_text  \\\n",
       "415  This is the release version of Samba, the free...   \n",
       "238  [![Build Status](http://runbot.odoo.com/runbot...   \n",
       "237                                   .-..-.\\n   __...   \n",
       "236  [![Build Status](http://runbot.odoo.com/runbot...   \n",
       "235                                   .-..-.\\n   __...   \n",
       "..                                                 ...   \n",
       "535  # Initial page\\n\\nthis is a hello world page\\n...   \n",
       "534  # Gatsby + Netlify CMS Starter\\n\\n[![Netlify S...   \n",
       "533  <!-- AUTO-GENERATED-CONTENT:START (STARTER) --...   \n",
       "539                              404 ERROR - NO README   \n",
       "550  # Gatsby + Netlify CMS Starter\\n\\n[![Netlify S...   \n",
       "\n",
       "                      batch                as_of status  \n",
       "415     oss_readme_batch1_1        6/11/21 17:21   Done  \n",
       "238     oss_readme_batch1_1        6/11/21 17:21   Done  \n",
       "237     oss_readme_batch1_1        6/11/21 17:21   Done  \n",
       "236     oss_readme_batch1_1        6/11/21 17:21   Done  \n",
       "235     oss_readme_batch1_1        6/11/21 17:21   Done  \n",
       "..                      ...                  ...    ...  \n",
       "535  oss_readme_batch_05_72  2021-07-12 23:16:10   Done  \n",
       "534  oss_readme_batch_05_72  2021-07-12 23:16:10   Done  \n",
       "533  oss_readme_batch_05_72  2021-07-12 23:16:10   Done  \n",
       "539  oss_readme_batch_05_72  2021-07-12 23:16:10   Done  \n",
       "550  oss_readme_batch_05_72  2021-07-12 23:16:11   Done  \n",
       "\n",
       "[444527 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('/project/biocomplexity/sdad/projects_data/ncses/oss/dspg_2021/')\n",
    "extension = 'csv'\n",
    "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
    "combined_csv = combined_csv[combined_csv['status'] == 'Done']\n",
    "combined_csv = combined_csv.sort_values(\"batch\")\n",
    "combined_csv\n",
    "# 21220 > 55872 now \n",
    "combined_csv.to_csv('/project/class/bii_sdad_dspg/uva_2021/dspg21oss/oss_readme_data_071221.csv', sep=',', encoding='utf-8', index=False)\n",
    "combined_csv.to_csv('/project/biocomplexity/sdad/projects_data/ncses/oss/dspg_2021/oss_readme_data_071221.csv', sep=',', encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slug</th>\n",
       "      <th>readme_text</th>\n",
       "      <th>batch</th>\n",
       "      <th>as_of</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mumblepins/debian-samba</td>\n",
       "      <td>This is the release version of Samba, the free...</td>\n",
       "      <td>oss_readme_batch1_1</td>\n",
       "      <td>6/11/21 17:21</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fdvarela/odoo8</td>\n",
       "      <td>[![Build Status](http://runbot.odoo.com/runbot...</td>\n",
       "      <td>oss_readme_batch1_1</td>\n",
       "      <td>6/11/21 17:21</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nkosi-tshawe/moodle</td>\n",
       "      <td>.-..-.\\n   __...</td>\n",
       "      <td>oss_readme_batch1_1</td>\n",
       "      <td>6/11/21 17:21</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dllsf/odootest</td>\n",
       "      <td>[![Build Status](http://runbot.odoo.com/runbot...</td>\n",
       "      <td>oss_readme_batch1_1</td>\n",
       "      <td>6/11/21 17:21</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dkavadia/stupefy</td>\n",
       "      <td>.-..-.\\n   __...</td>\n",
       "      <td>oss_readme_batch1_1</td>\n",
       "      <td>6/11/21 17:21</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444522</th>\n",
       "      <td>zuokun2013/site1</td>\n",
       "      <td># Initial page\\n\\nthis is a hello world page\\n...</td>\n",
       "      <td>oss_readme_batch_05_72</td>\n",
       "      <td>2021-07-12 23:16:10</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444523</th>\n",
       "      <td>almcalle/gatsby-starter-netlify-cms</td>\n",
       "      <td># Gatsby + Netlify CMS Starter\\n\\n[![Netlify S...</td>\n",
       "      <td>oss_readme_batch_05_72</td>\n",
       "      <td>2021-07-12 23:16:10</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444524</th>\n",
       "      <td>biwers/aj-test</td>\n",
       "      <td>&lt;!-- AUTO-GENERATED-CONTENT:START (STARTER) --...</td>\n",
       "      <td>oss_readme_batch_05_72</td>\n",
       "      <td>2021-07-12 23:16:10</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444525</th>\n",
       "      <td>taraldga/mossekarusellen-netlify-cms</td>\n",
       "      <td>404 ERROR - NO README</td>\n",
       "      <td>oss_readme_batch_05_72</td>\n",
       "      <td>2021-07-12 23:16:10</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444526</th>\n",
       "      <td>EbenezerSqueem/gatsby-starter-netlify-cms</td>\n",
       "      <td># Gatsby + Netlify CMS Starter\\n\\n[![Netlify S...</td>\n",
       "      <td>oss_readme_batch_05_72</td>\n",
       "      <td>2021-07-12 23:16:11</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>444527 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             slug  \\\n",
       "0                         mumblepins/debian-samba   \n",
       "1                                  fdvarela/odoo8   \n",
       "2                             Nkosi-tshawe/moodle   \n",
       "3                                  dllsf/odootest   \n",
       "4                                dkavadia/stupefy   \n",
       "...                                           ...   \n",
       "444522                           zuokun2013/site1   \n",
       "444523        almcalle/gatsby-starter-netlify-cms   \n",
       "444524                             biwers/aj-test   \n",
       "444525       taraldga/mossekarusellen-netlify-cms   \n",
       "444526  EbenezerSqueem/gatsby-starter-netlify-cms   \n",
       "\n",
       "                                              readme_text  \\\n",
       "0       This is the release version of Samba, the free...   \n",
       "1       [![Build Status](http://runbot.odoo.com/runbot...   \n",
       "2                                        .-..-.\\n   __...   \n",
       "3       [![Build Status](http://runbot.odoo.com/runbot...   \n",
       "4                                        .-..-.\\n   __...   \n",
       "...                                                   ...   \n",
       "444522  # Initial page\\n\\nthis is a hello world page\\n...   \n",
       "444523  # Gatsby + Netlify CMS Starter\\n\\n[![Netlify S...   \n",
       "444524  <!-- AUTO-GENERATED-CONTENT:START (STARTER) --...   \n",
       "444525                              404 ERROR - NO README   \n",
       "444526  # Gatsby + Netlify CMS Starter\\n\\n[![Netlify S...   \n",
       "\n",
       "                         batch                as_of status  \n",
       "0          oss_readme_batch1_1        6/11/21 17:21   Done  \n",
       "1          oss_readme_batch1_1        6/11/21 17:21   Done  \n",
       "2          oss_readme_batch1_1        6/11/21 17:21   Done  \n",
       "3          oss_readme_batch1_1        6/11/21 17:21   Done  \n",
       "4          oss_readme_batch1_1        6/11/21 17:21   Done  \n",
       "...                        ...                  ...    ...  \n",
       "444522  oss_readme_batch_05_72  2021-07-12 23:16:10   Done  \n",
       "444523  oss_readme_batch_05_72  2021-07-12 23:16:10   Done  \n",
       "444524  oss_readme_batch_05_72  2021-07-12 23:16:10   Done  \n",
       "444525  oss_readme_batch_05_72  2021-07-12 23:16:10   Done  \n",
       "444526  oss_readme_batch_05_72  2021-07-12 23:16:11   Done  \n",
       "\n",
       "[444527 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('/project/biocomplexity/sdad/projects_data/ncses/oss/dspg_2021/')\n",
    "check = pd.read_csv('oss_readme_data_071221.csv')\n",
    "check\n",
    "#check.to_csv('/project/class/bii_sdad_dspg/uva_2021/dspg21oss/oss_readme_data_071121.csv', sep=',', encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ That tells you the number of entries we have downloaded. \n",
    "\n",
    "### Development Space \n",
    "\n",
    "Below, are just some snippets of code that might be useful when tweaking the existing code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slug</th>\n",
       "      <th>readme_text</th>\n",
       "      <th>batch</th>\n",
       "      <th>as_of</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>h2oota/emacs-win64-msvc</td>\n",
       "      <td>Copyright (C) 2001-2016 Free Software Foundati...</td>\n",
       "      <td>oss_readme_batch1_1</td>\n",
       "      <td>6/11/21 17:21</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Distrotech/mysql-server</td>\n",
       "      <td>MySQL Server 5.6\\r\\n\\r\\nThis is a release of M...</td>\n",
       "      <td>oss_readme_batch1_1</td>\n",
       "      <td>6/11/21 17:21</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DaichiUeura/Emacs-for-Windows--xj-</td>\n",
       "      <td>Copyright (C) 2001, 2002, 2003, 2004, 2005, 20...</td>\n",
       "      <td>oss_readme_batch1_1</td>\n",
       "      <td>6/11/21 17:21</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>httpgit12/jb4evea-16</td>\n",
       "      <td>&lt;img alt=\"Swift logo\" height=\"70\" src=\"https:/...</td>\n",
       "      <td>oss_readme_batch1_1</td>\n",
       "      <td>6/11/21 17:21</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tehsurfer/hugo-contrarian</td>\n",
       "      <td># Contrarian website\\nThis website is the face...</td>\n",
       "      <td>oss_readme_batch1_1</td>\n",
       "      <td>6/11/21 17:21</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55867</th>\n",
       "      <td>jongbinjung/undi</td>\n",
       "      <td>\\n&lt;!-- README.md is generated from README.Rmd....</td>\n",
       "      <td>oss_readme_batch1_9</td>\n",
       "      <td>2021-06-14 15:06:27</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55868</th>\n",
       "      <td>angelsenra/orphans</td>\n",
       "      <td># orphans\\nCollection of small projects or gis...</td>\n",
       "      <td>oss_readme_batch1_9</td>\n",
       "      <td>2021-06-14 15:06:26</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55869</th>\n",
       "      <td>polsani/gatsby-starter-netlify-cms</td>\n",
       "      <td>**Note:** Gatsby v2 beta support is here! Chec...</td>\n",
       "      <td>oss_readme_batch1_9</td>\n",
       "      <td>2021-06-14 15:06:26</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55870</th>\n",
       "      <td>Arya-NK/Arya-NK.github.io</td>\n",
       "      <td>\\n</td>\n",
       "      <td>oss_readme_batch1_9</td>\n",
       "      <td>2021-06-14 15:06:27</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55871</th>\n",
       "      <td>pghant/big-theta</td>\n",
       "      <td>### The EC2 instance running the Neo4j databas...</td>\n",
       "      <td>oss_readme_batch1_9</td>\n",
       "      <td>2021-06-14 15:06:25</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55872 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     slug  \\\n",
       "0                 h2oota/emacs-win64-msvc   \n",
       "1                 Distrotech/mysql-server   \n",
       "2      DaichiUeura/Emacs-for-Windows--xj-   \n",
       "3                    httpgit12/jb4evea-16   \n",
       "4               Tehsurfer/hugo-contrarian   \n",
       "...                                   ...   \n",
       "55867                    jongbinjung/undi   \n",
       "55868                  angelsenra/orphans   \n",
       "55869  polsani/gatsby-starter-netlify-cms   \n",
       "55870           Arya-NK/Arya-NK.github.io   \n",
       "55871                    pghant/big-theta   \n",
       "\n",
       "                                             readme_text                batch  \\\n",
       "0      Copyright (C) 2001-2016 Free Software Foundati...  oss_readme_batch1_1   \n",
       "1      MySQL Server 5.6\\r\\n\\r\\nThis is a release of M...  oss_readme_batch1_1   \n",
       "2      Copyright (C) 2001, 2002, 2003, 2004, 2005, 20...  oss_readme_batch1_1   \n",
       "3      <img alt=\"Swift logo\" height=\"70\" src=\"https:/...  oss_readme_batch1_1   \n",
       "4      # Contrarian website\\nThis website is the face...  oss_readme_batch1_1   \n",
       "...                                                  ...                  ...   \n",
       "55867  \\n<!-- README.md is generated from README.Rmd....  oss_readme_batch1_9   \n",
       "55868  # orphans\\nCollection of small projects or gis...  oss_readme_batch1_9   \n",
       "55869  **Note:** Gatsby v2 beta support is here! Chec...  oss_readme_batch1_9   \n",
       "55870                                                 \\n  oss_readme_batch1_9   \n",
       "55871  ### The EC2 instance running the Neo4j databas...  oss_readme_batch1_9   \n",
       "\n",
       "                     as_of status  \n",
       "0            6/11/21 17:21   Done  \n",
       "1            6/11/21 17:21   Done  \n",
       "2            6/11/21 17:21   Done  \n",
       "3            6/11/21 17:21   Done  \n",
       "4            6/11/21 17:21   Done  \n",
       "...                    ...    ...  \n",
       "55867  2021-06-14 15:06:27   Done  \n",
       "55868  2021-06-14 15:06:26   Done  \n",
       "55869  2021-06-14 15:06:26   Done  \n",
       "55870  2021-06-14 15:06:27   Done  \n",
       "55871  2021-06-14 15:06:25   Done  \n",
       "\n",
       "[55872 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('/project/class/bii_sdad_dspg/ncses_oss_2021/requests_scrape/')\n",
    "check = pd.read_csv('oss_readme_batch_01_all.csv')\n",
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slug</th>\n",
       "      <th>readme_text</th>\n",
       "      <th>batch</th>\n",
       "      <th>as_of</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>zvini/website</td>\n",
       "      <td>404 ERROR - NO README</td>\n",
       "      <td>oss_readme_batch1_1</td>\n",
       "      <td>6/11/21 16:40</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>paleobiodb/data_service</td>\n",
       "      <td>404 ERROR - NO README</td>\n",
       "      <td>oss_readme_batch1_1</td>\n",
       "      <td>6/11/21 16:40</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>jandockx/ppwcode-recovered-from-google-code</td>\n",
       "      <td>404 ERROR - NO README</td>\n",
       "      <td>oss_readme_batch1_1</td>\n",
       "      <td>6/11/21 16:40</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>Liujingfang1/kprune</td>\n",
       "      <td>404 ERROR - NO README</td>\n",
       "      <td>oss_readme_batch1_1</td>\n",
       "      <td>6/11/21 17:21</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>tiagoanatar/ninjagame</td>\n",
       "      <td>404 ERROR - NO README</td>\n",
       "      <td>oss_readme_batch1_1</td>\n",
       "      <td>6/11/21 16:40</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>a14chrve/Examensarbetet</td>\n",
       "      <td>404 ERROR - NO README</td>\n",
       "      <td>oss_readme_batch_02_27</td>\n",
       "      <td>2021-06-23 12:44:33</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1321</th>\n",
       "      <td>albinsjolin/esporthub-website</td>\n",
       "      <td>404 ERROR - NO README</td>\n",
       "      <td>oss_readme_batch_02_27</td>\n",
       "      <td>2021-06-23 12:44:32</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1317</th>\n",
       "      <td>edurekavivekh/pistream</td>\n",
       "      <td>404 ERROR - NO README</td>\n",
       "      <td>oss_readme_batch_02_27</td>\n",
       "      <td>2021-06-23 12:44:32</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344</th>\n",
       "      <td>mouseM/learningMouse</td>\n",
       "      <td>404 ERROR - NO README</td>\n",
       "      <td>oss_readme_batch_02_27</td>\n",
       "      <td>2021-06-23 12:44:32</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342</th>\n",
       "      <td>netrunner-debian-kde-extras/kde-telepathy-ktp-...</td>\n",
       "      <td>404 ERROR - NO README</td>\n",
       "      <td>oss_readme_batch_02_27</td>\n",
       "      <td>2021-06-23 12:44:32</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4277 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   slug  \\\n",
       "460                                       zvini/website   \n",
       "448                             paleobiodb/data_service   \n",
       "451         jandockx/ppwcode-recovered-from-google-code   \n",
       "493                                 Liujingfang1/kprune   \n",
       "439                               tiagoanatar/ninjagame   \n",
       "...                                                 ...   \n",
       "1386                            a14chrve/Examensarbetet   \n",
       "1321                      albinsjolin/esporthub-website   \n",
       "1317                             edurekavivekh/pistream   \n",
       "1344                               mouseM/learningMouse   \n",
       "1342  netrunner-debian-kde-extras/kde-telepathy-ktp-...   \n",
       "\n",
       "                readme_text                   batch                as_of  \\\n",
       "460   404 ERROR - NO README     oss_readme_batch1_1        6/11/21 16:40   \n",
       "448   404 ERROR - NO README     oss_readme_batch1_1        6/11/21 16:40   \n",
       "451   404 ERROR - NO README     oss_readme_batch1_1        6/11/21 16:40   \n",
       "493   404 ERROR - NO README     oss_readme_batch1_1        6/11/21 17:21   \n",
       "439   404 ERROR - NO README     oss_readme_batch1_1        6/11/21 16:40   \n",
       "...                     ...                     ...                  ...   \n",
       "1386  404 ERROR - NO README  oss_readme_batch_02_27  2021-06-23 12:44:33   \n",
       "1321  404 ERROR - NO README  oss_readme_batch_02_27  2021-06-23 12:44:32   \n",
       "1317  404 ERROR - NO README  oss_readme_batch_02_27  2021-06-23 12:44:32   \n",
       "1344  404 ERROR - NO README  oss_readme_batch_02_27  2021-06-23 12:44:32   \n",
       "1342  404 ERROR - NO README  oss_readme_batch_02_27  2021-06-23 12:44:32   \n",
       "\n",
       "     status  \n",
       "460    Done  \n",
       "448    Done  \n",
       "451    Done  \n",
       "493    Done  \n",
       "439    Done  \n",
       "...     ...  \n",
       "1386   Done  \n",
       "1321   Done  \n",
       "1317   Done  \n",
       "1344   Done  \n",
       "1342   Done  \n",
       "\n",
       "[4277 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy the function from above and make tweaks in this code chunk\n",
    "combined_csv[combined_csv['readme_text'] == \"404 ERROR - NO README\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slug</th>\n",
       "      <th>readme_text</th>\n",
       "      <th>batch</th>\n",
       "      <th>as_of</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>flowchain/flowchain-ledger</td>\n",
       "      <td># flowchain-ledger\\n&amp;gt; Flowchain distributed...</td>\n",
       "      <td>oss_readme_batch1_10</td>\n",
       "      <td>2021-06-14 15:15:29</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>fabiocolacio/Mercury</td>\n",
       "      <td># Mercury Chat\\n\\nMercury is my end-to-end enc...</td>\n",
       "      <td>oss_readme_batch1_10</td>\n",
       "      <td>2021-06-14 15:15:38</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>fkbenjamin/pc-firebase-starter</td>\n",
       "      <td># PassChain\\n\\nAuthors: Rob-Jago Flötgen, Flor...</td>\n",
       "      <td>oss_readme_batch1_11</td>\n",
       "      <td>2021-06-14 15:19:18</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>peacedudegregoryks/Old-SBC-SocialBenefitCoin</td>\n",
       "      <td># The Social Benefit Coin Smart contract\\n \\nW...</td>\n",
       "      <td>oss_readme_batch1_11</td>\n",
       "      <td>2021-06-14 15:19:33</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>mukira/fukoblockchainexplorer</td>\n",
       "      <td># Fuko blockchain Explorer\\n\\n![GitHub Logo](h...</td>\n",
       "      <td>oss_readme_batch1_11</td>\n",
       "      <td>2021-06-14 15:19:28</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>agadzinski/vehicle-manufacture-20180607173737101</td>\n",
       "      <td># Blockchain - Tutorial\\n\\nThis is the tutoria...</td>\n",
       "      <td>oss_readme_batch1_8</td>\n",
       "      <td>2021-06-14 12:35:40</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1689</th>\n",
       "      <td>shubhamp1p/vehicle-manufacture-20180606135842485</td>\n",
       "      <td># Blockchain - Tutorial\\n\\nThis is the tutoria...</td>\n",
       "      <td>oss_readme_batch1_8</td>\n",
       "      <td>2021-06-14 12:35:40</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>energywebfoundation/ew-did-registry</td>\n",
       "      <td># EW DID Library v0.1\\n## Disclaimer\\n&amp;gt; The...</td>\n",
       "      <td>oss_readme_batch1_8</td>\n",
       "      <td>2021-06-14 12:20:53</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>jeffet/vehicle-manufacture-20180318192523657</td>\n",
       "      <td># Blockchain - Tutorial\\n\\nThis is the tutoria...</td>\n",
       "      <td>oss_readme_batch1_9</td>\n",
       "      <td>2021-06-14 15:06:23</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>jeffet/vehicle-manufacture-20180318202618486</td>\n",
       "      <td># Blockchain - Tutorial\\n\\nThis is the tutoria...</td>\n",
       "      <td>oss_readme_batch1_9</td>\n",
       "      <td>2021-06-14 15:06:23</td>\n",
       "      <td>Done</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  slug  \\\n",
       "449                         flowchain/flowchain-ledger   \n",
       "948                               fabiocolacio/Mercury   \n",
       "254                     fkbenjamin/pc-firebase-starter   \n",
       "1041      peacedudegregoryks/Old-SBC-SocialBenefitCoin   \n",
       "758                      mukira/fukoblockchainexplorer   \n",
       "...                                                ...   \n",
       "1724  agadzinski/vehicle-manufacture-20180607173737101   \n",
       "1689  shubhamp1p/vehicle-manufacture-20180606135842485   \n",
       "33                 energywebfoundation/ew-did-registry   \n",
       "236       jeffet/vehicle-manufacture-20180318192523657   \n",
       "224       jeffet/vehicle-manufacture-20180318202618486   \n",
       "\n",
       "                                            readme_text                 batch  \\\n",
       "449   # flowchain-ledger\\n&gt; Flowchain distributed...  oss_readme_batch1_10   \n",
       "948   # Mercury Chat\\n\\nMercury is my end-to-end enc...  oss_readme_batch1_10   \n",
       "254   # PassChain\\n\\nAuthors: Rob-Jago Flötgen, Flor...  oss_readme_batch1_11   \n",
       "1041  # The Social Benefit Coin Smart contract\\n \\nW...  oss_readme_batch1_11   \n",
       "758   # Fuko blockchain Explorer\\n\\n![GitHub Logo](h...  oss_readme_batch1_11   \n",
       "...                                                 ...                   ...   \n",
       "1724  # Blockchain - Tutorial\\n\\nThis is the tutoria...   oss_readme_batch1_8   \n",
       "1689  # Blockchain - Tutorial\\n\\nThis is the tutoria...   oss_readme_batch1_8   \n",
       "33    # EW DID Library v0.1\\n## Disclaimer\\n&gt; The...   oss_readme_batch1_8   \n",
       "236   # Blockchain - Tutorial\\n\\nThis is the tutoria...   oss_readme_batch1_9   \n",
       "224   # Blockchain - Tutorial\\n\\nThis is the tutoria...   oss_readme_batch1_9   \n",
       "\n",
       "                    as_of status  \n",
       "449   2021-06-14 15:15:29   Done  \n",
       "948   2021-06-14 15:15:38   Done  \n",
       "254   2021-06-14 15:19:18   Done  \n",
       "1041  2021-06-14 15:19:33   Done  \n",
       "758   2021-06-14 15:19:28   Done  \n",
       "...                   ...    ...  \n",
       "1724  2021-06-14 12:35:40   Done  \n",
       "1689  2021-06-14 12:35:40   Done  \n",
       "33    2021-06-14 12:20:53   Done  \n",
       "236   2021-06-14 15:06:23   Done  \n",
       "224   2021-06-14 15:06:23   Done  \n",
       "\n",
       "[124 rows x 5 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_data = combined_csv[combined_csv['readme_text'].notna()]\n",
    "#select_data['readme_clean'] = select_data['readme_text'].str.lower()\n",
    "select_data[select_data['readme_text'].str.contains(\"Blockchain\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-16-ae7a4300d788>, line 158)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-ae7a4300d788>\"\u001b[0;36m, line \u001b[0;32m158\u001b[0m\n\u001b[0;31m    results = [r.get() for r in results]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# set the environment variables with your username and github personal access token here \n",
    "# connect to the database, download data \n",
    "#connection = pg.connect(host = 'postgis1', database = 'sdad', \n",
    "#                        user = os.environ.get('db_user'), \n",
    "#                        password = os.environ.get('db_pwd'))\n",
    "#github_pats = '''SELECT * FROM gh_2007_2020.pats_update'''\n",
    "#github_pats = pd.read_sql_query(github_pats, con=connection)\n",
    "   \n",
    "        \n",
    "# can only make 2500 calls per hour \n",
    "# because the function calls twice each time \n",
    "@sleep_and_retry\n",
    "@limits(calls=2500, period=3600)\n",
    "def scrape_readmes(slug, github_pat_index):\n",
    "    \n",
    "    github_username = github_pats.login[github_pat_index]\n",
    "    github_token = github_pats.token[github_pat_index]\n",
    "    \n",
    "    while True: \n",
    "        try: \n",
    "            # define url based on the slug \n",
    "            url = f'https://api.github.com/repos/{slug}/readme'\n",
    "            response = requests.get(url, auth=(github_username, github_token))\n",
    "            response_code = response.status_code\n",
    "            \n",
    "            if response_code == 404: \n",
    "                print(f\"404 error on {slug}\")\n",
    "                readme_string = \"404 ERROR - NO README\"\n",
    "                now = datetime.now()\n",
    "                current_time = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                return slug, readme_string, current_time, \"Done\"\n",
    "            \n",
    "            elif response_code == 403:\n",
    "                print(f\"Rate limit exceeded (403 error) on {slug} at \", datetime.datetime.now())\n",
    "                github_pat_index+=1\n",
    "                print(\"***Exit current PAT, proceed to next PAT.\")\n",
    "                break  \n",
    "            \n",
    "        except KeyError:\n",
    "            print(\"Key error for: \" + slug, flush=True)\n",
    "            break\n",
    "        \n",
    "        except requests.exceptions.HTTPError as http_error:\n",
    "            print (\"HTTP Error:\", http_error)\n",
    "            raise SystemExit(http_error)\n",
    "            break\n",
    "            \n",
    "        except requests.exceptions.ConnectionError as connection_error:\n",
    "            print (\"Error Connecting:\", connection_error)\n",
    "            raise SystemExit(connection_error)\n",
    "            break \n",
    "        \n",
    "        except requests.exceptions.TooManyRedirects as toomany_requests:\n",
    "            print (\"Too Many Requests:\", toomany_requests)\n",
    "            raise SystemExit(toomany_requests)\n",
    "            break\n",
    "                \n",
    "        except requests.exceptions.Timeout as timeout_error:\n",
    "            print (\"Timeout Error:\", timeout_error)\n",
    "            raise SystemExit(timeout_error)\n",
    "            break\n",
    "        \n",
    "        except requests.exceptions.RequestException as request_exception_error:\n",
    "            print (\"Oops, Some Other Error:\", request_exception_error)\n",
    "            raise SystemExit(request_exception_error)\n",
    "            break \n",
    "            \n",
    "        html_content = response.content\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        site_json=json.loads(soup.text)\n",
    "        readme_link = site_json['download_url']\n",
    "        \n",
    "        while True:\n",
    "            try: \n",
    "                readme_response = requests.get(readme_link, auth=(github_username, github_token))\n",
    "                readme_response_code = readme_response.status_code\n",
    "                \n",
    "            except requests.exceptions.HTTPError as http_error:\n",
    "                print (\"HTTP Error:\", http_error)\n",
    "                raise SystemExit(http_error)\n",
    "                break\n",
    "            \n",
    "            except requests.exceptions.ConnectionError as connection_error:\n",
    "                print (\"Error Connecting:\", connection_error)\n",
    "                raise SystemExit(connection_error)\n",
    "                break\n",
    "            \n",
    "            except requests.exceptions.TooManyRedirects as toomany_requests:\n",
    "                print (\"Too Many Requests:\", toomany_requests)\n",
    "                raise SystemExit(toomany_requests)\n",
    "                break \n",
    "        \n",
    "            except requests.exceptions.Timeout as timeout_error:\n",
    "                print (\"Timeout Error:\", timeout_error)\n",
    "                raise SystemExit(timeout_error)\n",
    "                break\n",
    "        \n",
    "            except requests.exceptions.RequestException as request_exception_error:\n",
    "                print (\"Oops, Some Other Error:\", request_exception_error)\n",
    "                raise SystemExit(request_exception_error)\n",
    "                break  \n",
    "    \n",
    "            # pull the content out of the readme \n",
    "            readme_content = readme_response.content\n",
    "            readme_soup = BeautifulSoup(readme_content, 'html.parser')\n",
    "            readme_string = str(readme_soup)\n",
    "    \n",
    "            #give us the the timing and status \n",
    "            now = datetime.now()\n",
    "            current_time = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            #print(readme_string)\n",
    "            return slug, readme_string, current_time, \"Done\"\n",
    "        \n",
    "def filter_scraped_readmes(original_data): \n",
    "    ''' \n",
    "    Function ingests repos data and filters out already scraped data from local csv \n",
    "    '''\n",
    "    \n",
    "    # ingests local csv data and converts it to a list \n",
    "    os.chdir('/project/class/bii_sdad_dspg/ncses_oss_2021/requests_scrape/')\n",
    "    all_filenames = [i for i in glob.glob('*.csv')]\n",
    "    combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
    "    combined_csv = combined_csv[combined_csv['status'] == 'Done']\n",
    "    scraped_slugs_list = combined_csv['slug'].tolist()\n",
    "    \n",
    "    # filters out all of the scraped slugs from the original_data \n",
    "    filtered_slugs = ~raw_slug_data.slug.isin(scraped_slugs_list)\n",
    "    filtered_slugs = raw_slug_data[filtered_slugs]\n",
    "    \n",
    "    # provides the output of current slug count and number of slugs filtered \n",
    "    new_slug_count = filtered_slugs['slug'].count()\n",
    "    slug_count_diff = raw_slug_data['slug'].count() - filtered_slugs['slug'].count()\n",
    "    print(\"Current data has\", new_slug_count, \"entries (filtered\", slug_count_diff, \"from input data)\")\n",
    "    return filtered_slugs    \n",
    "\n",
    "test_slugs = [\"brandonleekramer/diversity\", \n",
    "              'cjbd/src'\n",
    "              \"uva-bi-sdad/oss-2020\", \n",
    "              \"facebook/react\", \n",
    "              \"RichardLitt/standard-readme\",            \n",
    "]\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Started scraping\")\n",
    "    \n",
    "    cores_available = multiprocessing.cpu_count() - 1\n",
    "    print(f'There are {cores_available} CPUs available.')\n",
    "    pool = multiprocessing.Pool(cores_available)\n",
    "    \n",
    "    # now we will feed in all of the remaining slugs \n",
    "    slug_log = []\n",
    "    readme_log = []\n",
    "    asof_log = []\n",
    "    status_log = []\n",
    "    \n",
    "    for slug in test_slugs:\n",
    "        result = pool.apply_async(scrape_readmes, args=(test_slugs, 15)\n",
    "        results = [r.get() for r in results]\n",
    "            #slug_log.append(result[0])\n",
    "            #readme_log.append(result[1])\n",
    "            #asof_log.append(result[2])\n",
    "            #status_log.append(result[3])\n",
    "            #final_log = pd.DataFrame({'slug': slug_log, \"readme_text\": readme_log, 'batch': batch_name, 'as_of': asof_log, 'status': status_log}, \n",
    "            #                  columns=[\"slug\", \"readme_text\", \"batch\", \"as_of\", \"status\"])\n",
    "            #final_log.to_csv('/project/class/bii_sdad_dspg/ncses_oss_2021/requests_scrape/'+batch_name+'.csv', sep=',', encoding='utf-8', index=False)\n",
    "        #print(\"Finished scraping\", len(final_log), \"of\", len(slugs), \"records\")\n",
    "        #print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = pg.connect(host = 'postgis1', database = 'sdad', \n",
    "                        user = os.environ.get('db_user'), \n",
    "                        password = os.environ.get('db_pwd'))\n",
    "#PATs access token, saved as a dataframe\n",
    "github_pats = '''SELECT * FROM gh_2007_2020.pats_update'''\n",
    "github_pats = pd.read_sql_query(github_pats, con=connection)\n",
    "\n",
    "#PATs access token, saved as a list\n",
    "access_tokens = github_pats[\"token\"]\n",
    "\n",
    "#number of tokens available for use, a numeric value\n",
    "num_token = '''SELECT COUNT(*) FROM gh_2007_2020.pats_update'''\n",
    "num_token = pd.read_sql_query(num_token, con=connection)\n",
    "num_token=num_token.iloc[0]['count']\n",
    "\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index ranges from 0 to maximum number of PATs available\n",
    "def get_access_token(github_pat_index):\n",
    "    if github_pat_index < num_token:\n",
    "       # print(\"Extracting access token #\", github_pat_index+1,\", total\", num_token, \"tokens are available.\")\n",
    "        return github_pats.token[github_pat_index]\n",
    "    else:\n",
    "        print(\"token exceed limit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current data has 24729 entries (filtered 3247 from input data)\n",
      "404 error on id\n",
      "404 error on slug\n",
      "404 error on branch\n",
      "404 error on status\n",
      "404 error on primarylanguage\n",
      "404 error on commits\n",
      "404 error on createdat\n",
      "404 error on description\n",
      "404 error on spdx\n",
      "404 error on asof\n",
      "Finished scraping 10 of 24729 records using akindlon977\n",
      "Current data has 24729 entries (filtered 3247 from input data)\n",
      "404 error on description\n",
      "404 error on slug\n",
      "404 error on primarylanguage\n",
      "404 error on status\n",
      "404 error on id\n",
      "404 error on createdat\n",
      "404 error on commits\n",
      "404 error on asof\n",
      "404 error on branch\n",
      "404 error on spdx\n",
      "Finished scraping 20 of 24729 records using Azraab\n"
     ]
    }
   ],
   "source": [
    "test_slugs = [\"brandonleekramer/diversity\", \n",
    "              'cjbd/src'\n",
    "              \"uva-bi-sdad/oss-2020\", \n",
    "              \"facebook/react\", \n",
    "              \"RichardLitt/standard-readme\",            \n",
    "]\n",
    "\n",
    "tmp_username = os.environ['GITHUB_USERNAME'] = 'brandonleekramer'\n",
    "tmp_token = os.environ['GITHUB_TOKEN'] = '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'\n",
    "\n",
    "# need to change this for each batch or you will save over what you had  \n",
    "batch_name_test = \"nothing\"\n",
    "cores_available = multiprocessing.cpu_count() - 1\n",
    "pool = Pool(cores_available)\n",
    "\n",
    "slug_log = []\n",
    "readme_log = []\n",
    "asof_log = []\n",
    "status_log = []\n",
    "\n",
    "for github_username, github_token in zip(login_list, token_list): \n",
    "    \n",
    "    test_slugs = filter_scraped_readmes(original_data = test_slugs)\n",
    "    \n",
    "    for result in pool.imap_unordered(scrape_readmes, test_slugs):\n",
    "        slug_log.append(result[0])\n",
    "        readme_log.append(result[1])\n",
    "        asof_log.append(result[2])\n",
    "        status_log.append(result[3])\n",
    "        final_log = pd.DataFrame({'slug': slug_log, \"readme_text\": readme_log, \n",
    "                                  'batch': batch_name_test, 'as_of': asof_log, 'status': status_log}, \n",
    "                                  columns=[\"slug\", \"readme_text\", \"batch\", \"as_of\", \"status\"])\n",
    "    print(\"Finished scraping\", len(final_log), \"of\", len(test_slugs), \"records\", \"using\", github_username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current data has 24729 entries (filtered 3247 from input data)\n",
      "Current data has 24729 entries (filtered 3247 from input data)\n",
      "27976 24729\n"
     ]
    }
   ],
   "source": [
    "for github_username, github_token in zip(login_list, token_list): \n",
    "    check_this = filter_scraped_readmes(original_data = raw_slug_data)\n",
    "    # now you need to add +=1 to the batch name when sending the output \n",
    "\n",
    "print(raw_slug_data['slug'].count(), check_this['slug'].count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to insert updates into sql: https://stackoverflow.com/questions/23103962/how-to-write-dataframe-to-postgres-table\n",
    "multiprocessing: https://stackoverflow.com/questions/45718546/with-clause-for-multiprocessing-in-python/45734483\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brandon_env",
   "language": "python",
   "name": "brandon_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
