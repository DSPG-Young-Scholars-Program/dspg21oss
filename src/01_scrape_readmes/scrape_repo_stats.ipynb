{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape repo stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['db_user']='zz3hs'\n",
    "os.environ['db_pwd']='zz3hs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages \n",
    "import os\n",
    "import psycopg2 as pg\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import requests as r\n",
    "import string \n",
    "import json\n",
    "import base64\n",
    "import urllib.request\n",
    "import itertools \n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stars_count(url):\n",
    "    html = r.get(url).text\n",
    "    #soup = BeautifulSoup(html, 'lxml') \n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    stars_class = \"social-count js-social-count\"\n",
    "    stars = soup.find('a', class_ = stars_class).text.strip()\n",
    "    return stars\n",
    "\n",
    "url1 = \"https://github.com/tidyverse/ggplot2\"\n",
    "print(stars_count(url1))\n",
    "\n",
    "\n",
    "html = r.get(url1).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url1 = \"https://github.com/tidyverse/ggplot2\"\n",
    "\n",
    "html = r.get(url1).text\n",
    "\n",
    "#print(html)\n",
    "soup = BeautifulSoup(html, 'lxml') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myPath = os.getcwd()\n",
    "\n",
    "print(myPath + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the database, download data, limit to top 5000 repos that have the highest number of commits\n",
    "connection = pg.connect(host = 'postgis1', database = 'sdad', \n",
    "                        user = os.environ.get('db_user'), \n",
    "                        password = os.environ.get('db_pwd'))\n",
    "\n",
    "raw_slug_data = '''SELECT * FROM gh_2007_2020.repos ORDER BY commits DESC LIMIT 100'''\n",
    "\n",
    "# convert to a dataframe, show how many missing we have (none)\n",
    "raw_slug_data = pd.read_sql_query(raw_slug_data, con=connection)\n",
    "raw_slug_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slugs = [\"brandonleekramer/diversity\", \"uva-bi-sdad/oss-2020\", \"facebook/react\"] #test data \n",
    "slugs = raw_slug_data.slug.tolist()\n",
    "print(*slugs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slug1 = slugs[10]\n",
    "print(slug1)\n",
    "url1 =f'https://github.com/{slug1}/'\n",
    "print(url1)\n",
    "\n",
    "print(stars_count(url1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for slug in slugs:\n",
    "    url = f'https://github.com/{slug}/'\n",
    "    split_slugs = slug.split(\"/\")\n",
    "    login = split_slugs[0]\n",
    "    repo = split_slugs[1]\n",
    "    fullfilename = os.path.join(myPath, f'readme_{login}_{repo}.txt')\n",
    "    urllib.request.urlretrieve(url, fullfilename)\n",
    "    print(f'Finished scraping: {login}/{repo}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = []\n",
    "readme_text = [] \n",
    "for filename in os.listdir(myPath):\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(os.path.join(myPath, filename)) as f:\n",
    "            content = f.read()\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            clean_html = ''.join(soup.article.findAll(text=True))\n",
    "            repo_name.append(filename)\n",
    "            readme_text.append(clean_html)\n",
    "            df = pd.DataFrame({'slug': repo_name, 'readme_text': readme_text}, columns=[\"slug\", \"readme_text\"])\n",
    "            df['slug'] = df['slug'].str.replace('readme_','')\n",
    "            df['slug'] = df['slug'].str.replace('.txt','')\n",
    "            # this works because slugs can't have underscores\n",
    "            df['slug'] = df['slug'].str.replace('_','/') \n",
    "df "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-crystal]",
   "language": "python",
   "name": "conda-env-.conda-crystal-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
